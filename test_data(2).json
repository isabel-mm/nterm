[
    {
        "text": "Introduction 1.The use of tree-based methods in current (corpus) linguistics\nOver the last 20 or so years, multifactorial modeling has taken much of corpus linguistics by storm.\nCompared to, say, the 1980s or even the 1990s, there is now a huge and constantly growing number of studies in corpus linguistics that tackle phenomena of interest with methods that allow researchers to study the effect of multiple independent variables, or predictors, on the dependent variable, or response, of interest.\nThe by far most widely used statistical tool in this respect is probably that of generalized linear (regression) modeling, either directly or, as in the case of sociolinguistic Varbrul applications, indirectly.\nHowever, over the last few years, many researchers have also realized that generalized linear models or their extension to generalized linear mixed-effects models can run into problems especially when applied to observational data such as corpus data. This is because, unlike nicely balanced experimental data, observational corpus data exhibit a variety of characteristics which make regression modeling hard: -The data are often (extremely) Zipfian distributed\nGiven these issues, a growing number of researchers are exploring data with such characteristics using methods other than regression modeling; a particularly popular alternative showing up in more and more studies is the family of treebased methods, including in particular classification and regression trees (CARTs), conditional inference trees, and random or conditional inference forests (see -CONJ: which conjunction heads the subordinate clause: weil/because versus bevor/before versus als/when versus nachdem/after; -SUBORDTYPE: causal or temporal (as in (1)); and -LENGTHDIFF: the length difference between the main and the subordinate clause (in words).\n(1) a. I was shocked when the cat killed the mouse.\nb. When the cat killed the mouse, I was shocked.\nThe classification tree resulting from the data can be interpreted as follows: Starting from the top, if the subordinate clause type is causal, go left and \"predict\" mc-sc; if the subordinate clause type is temporal, then go right and check the length difference of the main and the subordinate clause\n: if that difference is < -2.5, go left and also \"predict\" mc-sc, otherwise go right and check whether the conjunction is before.\nIf it is, \"predict\" mc-sc again, but if it is not, \"predict\" sc-mc.\nThis particular classification tree can actually be summarized more concisely, given its structure: One could just say \"always 'predict' mc-sc, unless the length difference between main and subordinate clause is greater than -2.5 and the conjunctions are after or when.\"\nHowever, this tree actually already also suggests that summarizing such a tree in prose can actually be extremely nonintuitive/difficult.\nThis is for several reasons: First, one needs to realize that every split in the tree potentially requires another if-clause -\"if X is this, go left, if it isn't, go right\"which means that, with increasing depths of such trees, their prose summaries become quite painful to process.\nFigure Second, one also needs to realize that the binary-split nature of these methods can make the interpretation of the effects of numeric predictors quite cumbersome: Where a regression model would just return a significant slope for the effect of a numeric predictor, which can be easily summarized with a \"the more x, the more/less y\" sentence, a classification tree will often \"represent\" such a slope with multiple binary splits on the same variable.\nConsider the verb-particle construction alternation exemplified in (2), which is known to be quite strongly influenced by the length of the direct object (DO): (2) a. The cat bit off the bird's head.\nb. The cat bit the bird's head off.\nConsider how the effect of this numeric predictor is represented in a classification tree such as the one shown in Figure -MODE: whether the example is from spoken or written data; -COMPLEXITY: how complex the DO is: simple versus phrasally modified versus clausally modified; -DO_LENGTHSYLL: how long the DO is (in syllables); -PP: whether the verb-particle construction is followed by a directional PP or not; -DO_ANIMACY: whether the referent of the DO is animate or inanimate; -DO_CONCRETENESS: whether the referent of the DO is concrete or abstract.\nThe tree reveals the effect of DO_LENGTHSYLL in two splits: First, there is a split at the top depending on whether the DO is shorter than 4.5 syllables.\nBut if it is and one goes down the left part of the tree, later one needs to also consider whether the DO is shorter than 1.5 syllables (i.e. whether the DO is one syllable long).\nAnd if the DO is not shorter than 4.5 syllables and you go down the right part of the tree, then later one needs to consider whether the DO is less than 9.5 syllables long.\nIn other words, it can happen that that single number in a regression modela slope of a numeric predictoris recoverable from a tree only by piecing together three or even more splits in different locations in a classification tree.\nFinally, it is worth mentioning that classification trees are not always particularly stable or robust: (1) even small changes in predictor values can produce big changes in the predictions, (2) even small changes to a data set can lead to drastic changes in the structure these kinds of algorithms find\nOne recent attempt to improve tree-based approaches that will also be included in this discussion is that of conditional inference trees, a version of trees that uses a regression-and p value-based approach to identify the best splits in predictors (rather than the abovementioned criteria used in many regular tree-based approaches), which reduces the need for pruning and crossvalidation Another recent development is that studies are now also more often using an extension of tree-based methods called random forests randomness on the level of the data points because random forests involve fitting many decision trees on many different randomly sampled (with or without replacement) subsets of the original data; randomness on the level of predictors because at every split in every tree, only a randomly selected subset of the predictors is eligible to be chosen.\nRandom forests usually tend to overfit much less than individual CARTs or training/test validation approaches and can be straightforwardly evaluated given that random forest accuracies are prediction, not classification, accuracies.\nIn other words, while classification trees, strictly speaking, only return classification accuracies, i.e. accuracies of classifying cases on which the tree was trained, random forests return predictions for cases that did not feature in the training process of each tree, but cases that were \"out of the bag\" of a tree that was fit, thus reducing overfitting and obviating the need for crossvalidation.\nHowever, compared to regression models and classification/conditional inference trees, random forests have the distinct disadvantage that interpreting them is hard: There is no single model from a single data set, or no single tree from a single data set, that can be straightforwardly plottedthere are, say, 500 different trees fit on 500 differently sampled data sets, with thousands of different splits in different locations in trees with differently sampled predictors available for splits ….\nThus, the problem is that there is a method (random/ conditional inference forests) that seems statistically superior to another (classification/conditional inference trees), but that is much harder to interpret/visualize.\nTwo solutions are pursued in the literature:\n-The random forest implementations that seem to be most widely used in (corpus) linguistics offer the functionality of computing variable importance scores, which quantify the size of the effect that a predictor has on the response; some version of thesepermutation-based scores, conditional importance scores, and scaled or unscaled onesare reported frequently.\nIn addition, they offer the computation of partial dependence scores, which represent the direction of the effect that levels/values of the predictor have on a response; for reasons not entirely clear to me, these are reported much less often (not even by scholars using Rs randomForest package, where these scores can be generated very easily).\n-Since the publications of The goals of the present paper\nIn this paper, I want to discuss critically the field's increasing reliance on treebased methods as well as the field's currently predominant ways of using and interpreting random forests.\nTo that end, I will first showcase a kind of data set that multiple tree-based approaches turn out to be very bad at handling in the sense that they fail to identify the correct predictors-response relation(s) in the data, which can lead to (1) suboptimal classification accuracies and (2) non-parsimonious trees (i.e. suboptimal variable importance scores); this data set and its analysis using the following R functions: tree::tree and rpart::rpart for classification trees and party::ctree as well as partykit::ctree for conditional inference trees (although the latter is merely a \"reimplementation\" of the former, see <http\nSecond and based on that first part, I will discuss how the practice of summarizing random forests on the basis of single trees can be, minimally, risky or, maximally, even flawed and how the kinds of data that are problematic for tree-based approaches can be studied more safely; these issues will be the focus of Section 3.\nCode for all the analyses in Sections 2 and 3 will be made available on my website.\nSection 4 concludes.\n2 Data sets that trees cannot handle (very well)\nA small artificial data set\nLet me first introduce a data set that has a structure that we will find is problematic for the tree-based approaches; this data set is represented in a summary frequency table in Table\nThere are two crucial things to notice about this data set.\nThe first is that the three predictors differ in monofactorial predictive power:\n-P1s (for Predictor 1 from the small data set) leads to 70% accuracy: when P1s is a, the Response is x 7 out of 10 times, when P1s is b, the Response is y 7 out of 10 times\n.\n-P2s (for Predictor 2 from the small data set) leads to 60% accuracy: when P2s is e, the Response is x 6 out of 10 times, when P2s is f, the Response is y 6 out of 10 times.\n-P3s (for Predictor 3 from the small data set) leads to only 50% accuracy: when P3s is m, the Response is x 6 out of 12 times, when P3s is n, the Response is y 4 out of 8 times.\nThe second thing to notice is that the two weaker predictors, P2s and P3s, together yield perfect 100% accuracy: when P2s is e and P3s is m, the Response is x all 6 times; when P2s is e and P3s is n, the Response is y all 4 times; when P2s is f and P3s is m, the Response is y all 6 times; when P2s is f and P3s is n, the Response is x all 4 times.\nThis data set exemplifies something that is at least relatable to the so-called XOR problem, a situation where two variables show no main effect [not true of P2s. which has a main effect] but a perfect interaction.\nIn this case, because of the lack of a marginally detectable main effect, none of the variables may be selected in the first split of a classification tree, and the interaction may never be discovered.\nOne final comment regarding these data: The use of the above data set is not to imply that a data set like this is typical for corpus-linguistic data in general or for tree-based analyses of such data in particular.\nOf course, corpus data are usuallyhopefullya bit larger than the above data (and see Section 2.2 below for a larger sample size) and they do not usually exhibit the kind of complete separation shown above.\nHowever, the Zipfian distribution that corpus data often exhibit makes it quite likely that some categorical predictors have highly frequent levels whose association to the response variable may overpower other (combinations of) predictors.\nAlso and as even one reviewer commented, this data set \"seems to be the best (because simplest) illustration of the issue at hand\" because it allows me to show that tree-based approaches may even fail at detecting the perfectly predictive effect of P2s and P3s in the data; the tree-based methods discussed below would not miraculously fare better if the data were not perfectly predictive (as that reviewer actually showed on the basis of more probabilistic/less deterministic data).\n2.1.1 How tree::tree handles this small data set\nThe first tree is based on tree::tree in R, i.e. the function tree from the package tree How rpart::rpart handles this small data set\nThe second tree uses the function rpart from the package rpart (Therneau and Atkinson 2018) and is shown in Figure Interim summary\nIn sum, in terms of accuracy, none of the approaches scores a value higher than 75% although there is a very simple interaction-like structure in the data that should result in 100% accuracy.\nIn terms of variable importance, no tree recognizes that P2s and P3s are important when combined, whereas P1s is not: Two approaches exaggerate the role of P1s and the third doesn't recognize any predictor as important; with these results, no approach would produce proper partial dependence scores for P2s and P3s: Even tree::tree, the only method that attributes at least some importance to P3s, does not see that P3s interacts with, so to speak, P2s.\nIn other words, none of the trees succeeds at finding the right structure in even as simple a data set as this and this is because, as The large version of the artificial data set Given these results, a first obvious objection to the above would be that part of the problem might be the ridiculously small sample size, which should affect especially the conditional inference tree, which uses p-values as a splitting criterion and could be expected to suffer from the small sample size.\nLet us therefore increase data set 1 small by a factor of 10 (and rename the predictors to P1l [for large], etc.) and see whether that improves the results.\nAs we will see, the new sample size leads to considerable changes.\nHow tree::tree handles this larger data set\nThe algorithm from tree::tree now returns a classification accuracy of 100%: every single case is identified correctly.\nHowever, as Figure A somewhat tree-savvy reader might now inquire whether the result of tree::tree will improve in terms of parsimony if the tree is pruned, a procedure that is routinely employed to avoid overfitting.\nHowever, as the companion file on my website at <http How rpart::rpart handles this larger data set\nThe increase of the sample size also changes the results of rpart::rpart, and the resulting tree shown in Figure How party::ctree handles this larger data set What about the conditional inference tree, whose performance should benefit considerably?\nIndeed, this conditional inference tree (and the corresponding tree generated with partykit::ctree) now leads to the same results as the other two methods, as is shown in Figure Interim summary\nThe more realistic sample size of data set 1 large has changed the picture considerably: All approaches now achieve 100% accuracy, but still no approach recognizes that it is the interaction of P2l:P3l alone that would be sufficient; none of the trees is parsimonious.\nThis is already an interesting finding given how widespread the consensus among corpus linguists is that tree-based approaches are good at detecting interactions: in this case, all approaches return a three-way interaction when a two-way interaction is all that would be required/desired.\nThe issue of variable effects is worthy of specific mention: If we were to compute variable importance scores on the trees, then these variable importance scores for P1l, P2l, and P3l do not alert the analyst to the fact that it is the \"interaction\" of P2l and P3lnot P2l or P3l in isolationthat does all the work.\nIn other words, partial dependence scores would not provide the desired results in any of the above applicationswhat we would ideally like to see is (1) a variable importance score that is very small for P1l and (2) some indications that P2l and P3l on their own do not do much, but that, together, they do a lot.\nRandom/Conditional inference forests to the rescue?\nOn the whole, the picture that has emerged so far is somewhat sobering because trees on both the small and the large data sets never returned an optimal tree, optimal in terms of accuracy, parsimony, and effect interpretations simply because one strong predictor chosen for the first split may overpower everything else, something which can of course very easily happen in Zipfian-distributed corpus-linguistic data.\nIs there any way in which this result can be improved?\nOne approach would seem to be to try and tweak hyperparameters of fitting trees, such as (1) the minimum decreases in deviance that define when trees stop splitting, (2) the minimum sample sizes per node, or (3) the tree depth.\nIn the current scenario, however, they will not change that the tree-building algorithms will still go with local importance and split on P1s/P1l.\nThus, a more powerful improvement might be the use of the extension of classification/conditional inference trees already discussed in Section 1.1, namely, random forests.\nSpecifically, one might expect that the two layers of randomness introduced in random forests would help in the present case because, among the, say, 500 trees, some could contain random samples of the data points in which P1s/P1l is not as dominant as it is in the data set as a whole; some could be trees where P1s/P1l was not available for the first split, which means the algorithm could only choose either P2s/P2l or P3s/P3l, which in turn means that if, at the next splits, P3s/P3l or P2s/P2l were available, respectively, those trees would result in 100% accuracy and parsimoniously so it also means these trees would assess P1s/P1l's importance like a human analyst might prefer it (see\nIn other words, the fact that not all predictors are always available for splits addresses the problem that \"a classification tree makes its splits based on local best performance\"\nWhat happens if we apply randomForest::randomForest (Liaw and Wiener 2002) to both data set 1 small and data set 1 large (setting the hyperparamater of mtry [how many predictors are considered at each split?] to 2 so that there is at least some choice at each split and using the default of ntree [how many trees are grown?]), first, the accuracies are improving over the trees: The random forest from data set 1 small scores an accuracy of 90%, whereas the forest from data set 1 large scores 100% (i.e. just like the trees).\nHowever, as Table What about the interpretation of the effects, however?\nWhile using random forests has not improved accuracy much but only variable importance scores, the interpretation of the random forests using partial dependence scores is still somewhat problematic: Figure First, the values of the partial dependence scores in the small data set are nearly perfectly aligned with the monofactorial accuracy percentages each predictor can score: P1s's scores deviate from 0 most (corresponding to the fact that it scores 70% accuracy), followed by P2s and then P3s.\nIn other words, in this case, the random forest's partial dependence scores just replicate the simple observed percentages attainable from cross-tabulation.\nFor the large data set, the situation is better, but the difference between the (totally irrelevant) predictor P1l and the predictors P2l and P3l (highly relevant in an interaction) is way smaller than we would like it to be.\nSecond and relatedly, this of course also means that we do not get a score for the interaction of P2s:P3s/P2l:P3l.\nWhat if we generate a random forest out of conditional inference trees instead (using party::cforest, see 3\nThe practice of summarizing a forest with a tree on all the data is problematic\nAs just mentioned, the above discussion already indicates why summarizing a random forest with a tree grown on all the data is risky or worse.\nFirst, this practice seems problematic already on a very general level: How could a single tree that was fit on all the data (no sampling of cases) and with all predictors all the time (no sampling of predictors) possibly be great at summarizing a forest of hundreds or thousands of different trees based on that very sampling of cases and predictors?\nSecond and more concretely here, it is problematic on empirical grounds: We saw above that the random forest results are not necessarily reflected well by any of the trees.\nSpecifically, if one uses any of the trees grown on data set 1 to \"visualize the random forest\" grown on data set 1, then one runs into the problem that the trees either discovered no structure at all or considered P1 to be (one of) the most important predictors, while at the same time, P1 is the predictor that the random forests return as less or least important; in other words, how can Figures 4-9 possibly be seen as summarizing the results underlying Tables\nNot only is this ironic given how many people consider tree-based approaches as good at finding/representing interactions, but there is also a much more important consequence:\nThe above results also point to an issue that seems not to be discussed in linguistic applications of random forests, namely, the question of how good forests and their variable importance measures are at capturing interactions and/or detecting interactions, where capturing refers to a random forest identifying a variable that \"contributes to the classification with an interaction effect\" and where detecting refers to a random forest identifying \"the interaction effect per se and the predictor variables interacting with each other\"\nThus and exactly as demonstrated by our above data, RF methodologies are commonly claimed, often in rather vague terms, to be able to handle interactions […], although, by construction, the predictor defining the first split of a tree is selected as the one with the strongest main effect on the response variable.\nAn alternative pointed out to me by one reviewer is the functionality offered by the package randomForestSRC\nA final potentially interesting method could be reinforcement learning trees, an improvement over random forests whose main characteristics sound exactly like what would be needed to address the problems discussed above: first, […] choose variable(s) for each split which will bring the largest return from future branching splits rather than only focusing on the immediate consequences of the split via marginal effects.\nSuch a splitting mechanism can break any hidden structure and avoid inconsistency by forcing splits on strong variables even if they do not show any marginal effect; second, progressively muting noise variables as we go deeper down a tree so that even as the sample size decreases rapidly towards a terminal node, the strong variable(s) can still be properly identified from the reduced space; third, the proposed method enables linear combination splitting rules at very little extra computational cost.\nIn sum, tree-based approaches can in fact be much less good at (1) being parsimonious and at (2) detecting interactions than is commonly assumed; this is true especially if interactions are not forced into a set of predictors explicitly and/or not explored once a first forest has been grown.\nHowever, once either of these options is pursued, both trees and random forests as used here can nearly always recover the structure in the data perfectly and parsimoniously.\nRepresenting random forests: representative trees\nLet us now turn to the question of how an existing random forest can be visualized better.\nWe have seen that visualizing a random forest with a single tree fit on all the data can be\nhighly misleading\n.\nHowever, a better option is available, namely, what is called a representative tree.\nThis approach is implemented in Representing random forests: effects (plots)\nAnother possibility of representing some of the structure in a random forestwith or without interaction predictorsinvolves applying a logic/tool that is widely used in regression modeling contexts, namely, that of effects plots For example, if one fitted a forest modeling the response variable in the large data set as a function of P1l, P2l, and P3l, this means that the predicted effect of P1l:a would be the mean of the predicted probabilities for the four combinations listed below weighted by their frequencies:\n- This seems like an appealing approach because it represents a predictor's effect in a random forest in a way that is not only well known from regression modeling but also controls for all other predictors in a way that goes beyond what observed frequencies/percentages can provide.\nSome readers might recognize that this approach is similar to partial dependence scores provided by randomForest::partialPlot, but the above discussion is still relevant for two reasons: (1)\nIf I understand the documentation for randomForest::partialPlot correctly (see the documentation of partialPlot and Molnar 2018: Section 5.1), that function computes only an unweighted mean of the predicted probabilities/ logits (and the results from pdp::partial are the same), whereas the proposal above involves computing a weighted mean, i.e. one that is weighted by the frequency distribution of the actual data.\nThis could be superior since it includes more information about our data (a logic that everyone using effects::effect seems to implicitly acknowledge).\n( Representing random forests: global surrogate models\nOne final and more preliminary suggestion for the interpretation of random forests involves the notion of global surrogate models (GSMs).\nThey are based on the notion discussed by\nIn the present case, we could choose either one of two options: fit a binary logistic regression model on the categorical predictions of a random forest; fit a linear model on the predicted probabilities of Response: y from a random forest.\nIn either case, one might use a forward model selection process, i.e. one would first fit a model using only an intercept and then add predictors as long as that makes the bigger model significantly better (e.g. in terms of p-values) or substantially better (e.g. in terms of AIC or BIC values).\nFor this, one might consider adding predictors in the order determined by the regression modeling process or by the variable importance scores of the random forest (as in Deshors & Gries, accepted pending revision, who apply this logic to results from a random forest fit with interaction predictors).\nOnce the chosen model selection process is completed, the final model is visualized with, for instance, effects plots\nIf GSMs are applied to the present artificial data set (here as a backward selection example using MASS::stepAIC), the final model finds the interaction between P2 and P3 and thus achieves perfect accuracy and represents the effects faithfullyhowever, the results are less than ideal in terms of significance tests and confidence intervals because the didactically motivated perfect split/complete separation in the simulated data renders all coefficients insignificantwith real data, however, where perfect predictability is much less likely, this is correspondingly much less likely to happen.\nIn sum, GSMs seem to be gaining ground as more and more relatively hardto-interpret (deep) machine learning methods are employed and researchers are struggling with making sense of what such black boxes return.\nWhile I have not seen them being used in linguistics, GSMs are an interesting alternative worth exploring and certainly more reasonable than trying to interpret a random forest with a single tree fit on all the data.\nConcluding remarks\nAs the size and complexity of data sets in different subfields of linguistics grow, it becomes more and more important to study them with methods that do their complexity justice.\nWhile it is great that more and more linguists are using multifactorial methods to find patterns in their data, this also increases the risk of applications that raise more problems than they might solve.\nTreebased methods have become a welcome alternative for data sets that defy regression-based methods especially in noisy and unbalanced corpus data, and that, in and of itself, is potentially a good thing.\nHowever, in this paper, I showed that there can be patterns in data that make trees underperform considerably when it comes to accuracy, variable importance/parsimony, and effects interpretation; that random/conditional inference forests can sometimes help with (some of) these issues, but that the way in which some studies try to interpret random forestswith a single treeis also not ideal.\nThese issues are especially problematic given the otherwise positive trend that corpus-linguistic data and methods are now informing linguistic theorizing more than they have for a long time.\nIn order to address these problems, I discussed several analytical possibilities including explicitly created interaction variables in trees and random forests and interpreting random forests on the basis of (1) (ideally multiple) representative trees, (2) effects plots, and (3) GSMs.\nThis paper can obviously only stimulate discussion rather than settle the matter(s) at handin fact, it seems every single aspect of random forests is currently being lively discussed in bioinformatics journals: sampling of data (with or without replacement), splitting criteria (Gini vs. p-values), variable importance measures (error rate vs. permutation-based versus AUC [the latter two conditional or unconditional]), variable selection, whether random forests can capture or detect interactions in the presence of correlated predictors, imbalanced response variables, etc., all of which affect the (quality of the) results ….\nHowever, I hope that the above observations and suggestions lead to a greater awareness of the potential pitfalls of trees and forests, but also the opportunities they offer.\n",
        "entities": [
            [
                149,
                164,
                "TERMINO"
            ],
            [
                289,
                304,
                "TERMINO"
            ],
            [
                1097,
                1113,
                "TERMINO"
            ],
            [
                1297,
                1313,
                "TERMINO"
            ],
            [
                1494,
                1520,
                "TERMINO"
            ],
            [
                1607,
                1625,
                "TERMINO"
            ],
            [
                1807,
                1825,
                "TERMINO"
            ],
            [
                1943,
                1962,
                "TERMINO"
            ],
            [
                2048,
                2066,
                "TERMINO"
            ],
            [
                2119,
                2137,
                "TERMINO"
            ],
            [
                2222,
                2240,
                "TERMINO"
            ],
            [
                2455,
                2474,
                "TERMINO"
            ],
            [
                2629,
                2647,
                "TERMINO"
            ],
            [
                3309,
                3325,
                "TERMINO"
            ],
            [
                3483,
                3502,
                "TERMINO"
            ],
            [
                3895,
                3914,
                "TERMINO"
            ],
            [
                4946,
                4962,
                "TERMINO"
            ],
            [
                5098,
                5117,
                "TERMINO"
            ],
            [
                5156,
                5175,
                "TERMINO"
            ],
            [
                5338,
                5346,
                "TERMINO"
            ],
            [
                5532,
                5558,
                "TERMINO"
            ],
            [
                5607,
                5614,
                "TERMINO"
            ],
            [
                6547,
                6566,
                "TERMINO"
            ],
            [
                6950,
                6966,
                "TERMINO"
            ],
            [
                6987,
                7013,
                "TERMINO"
            ],
            [
                7133,
                7141,
                "TERMINO"
            ],
            [
                7175,
                7183,
                "TERMINO"
            ],
            [
                7289,
                7297,
                "TERMINO"
            ],
            [
                7572,
                7598,
                "TERMINO"
            ],
            [
                7824,
                7849,
                "TERMINO"
            ],
            [
                8101,
                8125,
                "TERMINO"
            ],
            [
                8696,
                8704,
                "TERMINO"
            ],
            [
                8981,
                9006,
                "TERMINO"
            ],
            [
                9015,
                9023,
                "TERMINO"
            ],
            [
                9106,
                9125,
                "TERMINO"
            ],
            [
                9175,
                9201,
                "TERMINO"
            ],
            [
                9713,
                9721,
                "TERMINO"
            ],
            [
                9779,
                9787,
                "TERMINO"
            ],
            [
                9813,
                9821,
                "TERMINO"
            ],
            [
                9912,
                9920,
                "TERMINO"
            ],
            [
                9949,
                9964,
                "TERMINO"
            ],
            [
                10024,
                10032,
                "TERMINO"
            ],
            [
                10152,
                10160,
                "TERMINO"
            ],
            [
                10327,
                10335,
                "TERMINO"
            ],
            [
                10501,
                10509,
                "TERMINO"
            ],
            [
                10996,
                11004,
                "TERMINO"
            ],
            [
                11342,
                11361,
                "TERMINO"
            ],
            [
                11469,
                11477,
                "TERMINO"
            ],
            [
                11501,
                11509,
                "TERMINO"
            ],
            [
                11740,
                11751,
                "TERMINO"
            ],
            [
                12003,
                12020,
                "TERMINO"
            ],
            [
                12117,
                12125,
                "TERMINO"
            ],
            [
                12596,
                12604,
                "TERMINO"
            ],
            [
                12730,
                12738,
                "TERMINO"
            ],
            [
                13335,
                13359,
                "TERMINO"
            ],
            [
                13610,
                13618,
                "TERMINO"
            ],
            [
                13687,
                13695,
                "TERMINO"
            ],
            [
                13822,
                13833,
                "TERMINO"
            ],
            [
                13870,
                13896,
                "TERMINO"
            ],
            [
                13990,
                14001,
                "TERMINO"
            ],
            [
                14029,
                14037,
                "TERMINO"
            ],
            [
                14184,
                14195,
                "TERMINO"
            ],
            [
                14262,
                14270,
                "TERMINO"
            ],
            [
                14315,
                14338,
                "TERMINO"
            ],
            [
                14700,
                14708,
                "TERMINO"
            ],
            [
                14729,
                14740,
                "TERMINO"
            ],
            [
                14859,
                14867,
                "TERMINO"
            ],
            [
                14883,
                14909,
                "TERMINO"
            ],
            [
                14971,
                14997,
                "TERMINO"
            ],
            [
                15171,
                15182,
                "TERMINO"
            ],
            [
                15186,
                15194,
                "TERMINO"
            ],
            [
                15501,
                15516,
                "TERMINO"
            ],
            [
                15793,
                15818,
                "TERMINO"
            ],
            [
                15845,
                15870,
                "TERMINO"
            ],
            [
                16041,
                16065,
                "TERMINO"
            ],
            [
                16180,
                16205,
                "TERMINO"
            ],
            [
                16508,
                16516,
                "TERMINO"
            ],
            [
                17045,
                17056,
                "TERMINO"
            ],
            [
                17324,
                17350,
                "TERMINO"
            ],
            [
                17676,
                17684,
                "TERMINO"
            ],
            [
                18216,
                18235,
                "TERMINO"
            ],
            [
                18369,
                18377,
                "TERMINO"
            ],
            [
                18390,
                18398,
                "TERMINO"
            ],
            [
                18688,
                18696,
                "TERMINO"
            ],
            [
                18756,
                18764,
                "TERMINO"
            ],
            [
                18953,
                18978,
                "TERMINO"
            ],
            [
                19028,
                19052,
                "TERMINO"
            ],
            [
                19117,
                19141,
                "TERMINO"
            ],
            [
                19156,
                19164,
                "TERMINO"
            ],
            [
                19434,
                19458,
                "TERMINO"
            ],
            [
                19555,
                19563,
                "TERMINO"
            ],
            [
                19925,
                19951,
                "TERMINO"
            ],
            [
                20790,
                20798,
                "TERMINO"
            ],
            [
                20843,
                20851,
                "TERMINO"
            ],
            [
                22213,
                22230,
                "TERMINO"
            ],
            [
                23022,
                23033,
                "TERMINO"
            ],
            [
                24319,
                24335,
                "TERMINO"
            ],
            [
                24428,
                24445,
                "TERMINO"
            ],
            [
                24459,
                24467,
                "TERMINO"
            ],
            [
                24813,
                24829,
                "TERMINO"
            ],
            [
                25015,
                25039,
                "TERMINO"
            ],
            [
                25519,
                25541,
                "TERMINO"
            ],
            [
                26035,
                26051,
                "TERMINO"
            ],
            [
                26230,
                26245,
                "TERMINO"
            ],
            [
                26564,
                26580,
                "TERMINO"
            ],
            [
                26602,
                26627,
                "TERMINO"
            ],
            [
                26804,
                26819,
                "TERMINO"
            ],
            [
                26952,
                26960,
                "TERMINO"
            ],
            [
                27225,
                27244,
                "TERMINO"
            ],
            [
                27979,
                27987,
                "TERMINO"
            ],
            [
                28390,
                28398,
                "TERMINO"
            ],
            [
                29963,
                29980,
                "TERMINO"
            ]
        ]
    },
    {
        "text": "From fallacies and pitfalls to solutions and future directions Navigating the evolving terrain of corpus linguistics In a short but important paper published thirty-five years ago in the ICAME Journal, The insightfulness of Rissanen's article has been generally recognized by corpus linguists (see, e.g., As solutions to the issues raised, In general, critical self-assessment is unquestionably an essential part in the evolution of any new methodological approach into language study, and corpus linguists have consistently recognized the theoretical and practical challenges inherent in the compilation, design, organization, and analysis of data.\nOver the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.\nThe attitudes of corpus linguists seem to include both a healthy sense of humility as regards what is possible to achieve, as well as an understanding of the need to continuously remind the scholarly community of the core concerns in the field.\nAs illustrations of the former idea, we can consider the often-quoted statements that \" Having noted that it is only sensible to be aware of the impossibility to create perfect corpora and analytical methods which are entirely free of pitfalls, it would feel wrong not to take note of the increase of the levels of analytical sophistication and efficiency in the field.\nThe toolbox that a corpus linguist has today is immensely superior to a corresponding one twenty or thirty years ago.\nHowever, some problems persist, and it is safe to assume that the number of pitfalls that a corpus linguist needs to try to avoid has not necessarily decreased, quite the contrary.\nBut it is not difficult to imagine that for novice users of corpora, the numerous functions readily available in concordancers and online corpus interfaces have created another fallacy that might be called a \"fallacy of sophisticated technology\": with such state-of-the-art systems, what could go wrong?\nIn a way this would be related to Rissanen's \"God's truth fallacy\", but instead of a false sense of security arising from the seemingly representative datasets, the sheer impressiveness of the software and tools of analysis may suggest that pitfalls do not exist.\nImprovements in some areas do not mean that all of the old problems have been solved.\nIn many ways we can then say that the challenges do not only relate to corpora and their use themselves, but also to how we educate new corpus users and inform them about the fundamental concepts and concerns.\nThere are undoubtedly many types of persistent problems, common frustrations, and messiness in corpus data that seasoned scholars have encountered and know about, but which are seldom specifically addressed.\nYet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.\nThis is also the main motivating factor behind the present volume.\nComprising eight chapters, the book discusses the nature of these problems and seek solutions to the perplexities faced by themselves and other scholars.\nWhile the chapters mostly concentrate on English corpora, many of the themes are relevant from a general corpus-linguistic point of view.\nThe topics range from addressing issues relating to grammatical annotation\nIt deserves to be mentioned that Rissanen's concerns on the use of corpora were explicitly presented from the point of view of historical linguistics, and while many of the issues he raises are also relevant to the study of contemporary forms of languages, historical corpus linguistics faces significant challenges of its own.\nThe chapter by Turo Vartiainen and Tanja Säily (Chapter 2) raises a number of practical points which still pose problems in the analysis of historical corpora.\nThe authors examine a set of case studies, highlighting issues like part-of-speech annotation, miscategorization challenges, and problems related to metadata and digitized texts in historical databases, such as the 10.5-billion-word Eighteenth Century Collections Online, a database which was originally not compiled for linguistic purposes.\nReporting on a number of instances where initial findings from corpora turned out to be misleading or inconclusive upon closer inspection of the data, the authors advocate for methodological improvements, user feedback channels, and balancing subgenres.\nThey stress the significance of \"knowing one's data\" for researchers dealing with vast corpora, emphasizing collaboration with historians, and systematic text examination for qualitative analyses and data interpretation.\nThe impact of insufficient metadata is also the topic of Chapter 3 by Mark Kaunisto, who examines the problems seen in the annotation of named entities in corpora, delving into the problems that arise in interpreting corpus data.\nDespite the long-recognized importance of considering proper nouns and names in corpus annotation schemes, many contemporary linguistic corpora lack the capability to exclude these items effectively when setting up queries.\nThe presence of names in corpora introduces a layer of complexity, potentially including items that may not reflect the active linguistic choices of the represented writers or speakers.\nThrough small-scale case studies utilizing prominent English language corpora like the British National Corpus and the Corpus of Contemporary American English, the chapter underscores the necessity for careful post-processing of search results.\nThe occurrence of searched items within named entities poses challenges in analyzing word frequencies and collocational behavior.\nThe chapter advocates for more detailed annotation of named entities in large linguistic corpora that are already available.\nNotable improvements could also be gained by the increased collaboration between traditional corpus linguists and scholars in the fields of computational linguistics and NLP, of which there are already encouraging examples.\nChapter 4 by Marcus Callies discusses challenges related to the special characteristics of learner corpus data, emphasizing the importance of valid data representation for studying L2 production and development.\nLearner corpora, particularly those containing academic texts, often include instances of multilingual practices, code-switching, and borrowed content, presenting difficulties for annotation and analysis.\nSpecific attention is called for in tagging elements like expert terminology, metalinguistic language use, and quoted passages to ensure the accuracy of word counts and concordance analyses.\nCompilers and users of learner corpora also grapple with the issue of unwanted lexical bias, stemming from task topics, writing prompts, or other input materials.\nIdentifying and addressing lexical bias is crucial, as it can impact the validity of research findings, especially given the consideration of lexical variation as a proxy for L2 proficiency.\nMethods to tackle lexical bias involve treating biased words as stopwords or excluding L2 structures likely induced by bias.\nAdditionally, task and prompt materials may influence the recurrent use of specific grammatical constructions, highlighting the need for careful analysis.\nCallies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines.\nOverall, the chapter emphasizes the need to move beyond a monolingual native-speaker norm in assessing learner data, acknowledging the significance of diverse linguistic expressions.\nCallies also raises an important point about the challenges that the use of AI or other writing tools may pose in the compilation of learner corpora in the future -a point that will also be relevant to the compilation of many different types of corpora -as one needs to make sure that the samples compiled truly reflect the writers' own language choices.\nIn Chapter 5, the question of the useability of databases for corpus linguistic purposes is addressed by Turo Hiltunen, who discusses issues relating to the use of materials in the British Library Newspapers database.\nPerspectives on the matter vary between scholarly approaches: the criteria for determining what is \"good data\" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics.\nFor a corpus linguist, it is important to have detailed knowledge of how the data has been compiled, what editorial or reformatting practices have been applied to the data, and with what tools the data can be examined.\nHiltunen highlights ways in which the useability of databases can be assessed, also noting the different conceptualizations of notions such as register in different scholarly disciplines as posing a challenge.\nAs a way forward in trying to find solutions to improve the adaptability of databases for linguistic research, Hiltunen calls for more interdisciplinary discussions and collaboration between corpus linguistics, Digital Humanities, and related disciplines, which are still too far from each other because of the disparities in their research agendas.\nThe subject of accessibility of data and its repercussions in corpus study is also covered by Stefan Hartmann (Chapter 6), who brings up the problem of replicability of corpus studies, often resulting from the limited accessibility of the corpora studied, as some corpora are only available behind a paywall.\nSeeing connections between these problems and those outlined by With the rise of social media and web data, the question of how texts of distinctly different shapes and forms, compiled together into a corpus, can reliably be examined becomes increasingly pertinent.\nAatu Liimatta's chapter (Chapter 7) focusses on the challenges posed by variation in text length and the specific issue of short texts, an issue which so far has not received much attention from quantitative corpus linguists.\nLargely the reason for this is how previously short texts have not been regarded as being a major problem, but with the advent of new forms in contemporary digital communication, the 'problem of text length' , as Liimatta calls it, needs to be addressed.\nAlthough solutions have been proposed, they often appear to be limited as regards their suitability in different kinds of studies.\nLiimatta proposes potential avenues for improvement and new method development, including the exploration of resampling methods for estimating distribution and approaches utilizing the large size of datasets.\nWhile perfect solutions may not yet exist, Liimatta offers useful insights to the study of linguistic questions within the context of varying text lengths.\nIn Chapter 8, Daniel Ocic Ihrmark explores the challenges of categorizing fiction genres in corpus compilation, especially when catering to both linguistic and literary research fields.\nGeneral-purpose linguistic corpora have traditionally aimed to include works of fiction; however, as Ihrmark notes, the practices of categorizing (and subcategorizing) literary genres in the disciplines tend to differ, which may result in difficulties in trying to make use of corpora in literary studies.\nThe differences between how literary genres are categorized largely reflect the different viewpoints and goals between the fields, with corpus linguistic categorizations aiming to focus on the different communicative purposes of texts, whereas the descriptions of genres by literature scholars are based more on content and stylistic concerns.\nIhrmark examines various methods employed in corpus stylistics, such as keyword analysis and n-gram searches, and their reliance on genre categorization for comparative studies.\nOverall, Ihrmark suggests adopting broader genre categorizations at higher levels for wider applicability, while allowing for additional granularity as needed.\nAs observed in other chapters in the volume, corpus linguistics stands to benefit from the innovative ideas, methods, and expanded possibilities developed in related fields such as natural language processing and computational linguistics, enriching its analytical toolkit and enhancing its capacity for nuanced linguistic analysis.\nIn the concluding chapter of the volume, Filip Miletić, Anne Introduction When Professor Matti Rissanen wrote a short article entitled \"Three problems connected with the use of diachronic corpora\" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today.\nAt the time, Rissanen was leading a group of scholars responsible for the compilation of the first diachronic corpus of English, the Helsinki Corpus of English Texts.\nThe Helsinki Corpus, which was to be published in 1991, would revolutionize the historical research of English for decades to come, and its importance for historical corpus linguistics cannot be overstated.\nAgainst this background, it is interesting to note that Rissanen felt compelled to discuss some potential pitfalls related to historical corpus linguistics two years prior to the publication of the Helsinki Corpus.\nFrom a present-day perspective, some of the issues raised by Rissanen have been addressed, while others remain a source for concern.\nFurthermore, there are some new trends in corpus-based research that Rissanen could not have anticipated in his article, and these introduce challenges of their own.\nIn this paper, we revisit Rissanen's ideas in order to identify some of the present-day pitfalls in historical corpus linguistics.\nWe pay particular attention to what\nHowever, it is not our intention to advise against the use of big-data corpora or sophisticated statistical methods in historical corpus linguistics.\nHaving larger corpora at our disposal is arguably one of the most important advances of the recent decades, and the more statistically-oriented research projects have permitted the analysis of highly complex questions pertaining to language change with increased quantitative rigour\nAccordingly, we propose that the changing focus from detailed analyses of small datasets to more abstract and statistically-oriented analyses of big data requires a reconceptualization of the philologist's dilemma.\nWhile the original formulation of the philologist's dilemma is no less relevant for scholars engaged in, say, historical sociolinguistics, we argue that in more grammatically oriented studies that make use of big data, increasing attention should be paid to the corpus itself: to its sampling principles and layers of annotation.\nIndeed, while the handling of linguistic big data always includes a degree of uncertainty, there is no excuse for ignoring the potential problems related to these resources.\nWe acknowledge that some of the issues that we discuss in this paper may not be specific to historical corpora (see, e.g, After this short introduction, we proceed directly to our examples of presentday pitfalls in historical corpus-based research.\nEach of the following sections is intended to illustrate one general problem associated with historical corpus linguistics by discussing individual studies that we have carried out previously.\nIn Section 2, the focus is on part-of-speech (POS) annotation, which can be regarded as one of the most useful kinds of linguistic annotation in contemporary research, but which, as we shall see, does not come without problems.\nSection 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus.\nIn Section 4, the focus is on linguistic databases, and on Eighteenth Century Collections Online, in particular.\nHere, we discuss issues related to the reliability of the database with respect to OCR (Optical Character Recognition) accuracy as well as problems pertaining to the balance of the database and the metadata associated with it.\nSection 5 brings the chapter to a close with a discussion of the main topics and some final conclusions.\nPOS annotation in diachronic datasets Part-of-speech (POS) annotation arguably provides one of the most useful layers of linguistic annotation to assist the researcher in the retrieval of relevant constructions from corpus data.\nHaving each word in the corpus tagged according to its part of speech (i.e., word class) permits, for instance, the study of partiallyfilled constructions (e.g., N-PROP of N-PROP; \"the Einstein of Italy\") and of lexical items of a specific part of speech (e.g., love_V; \"I love you\").\nHowever, applying software developed for Present-day English to historical data is somewhat problematic\nAccounting for category change\nOne of the areas of research where POS annotation might fail is category change, that is, a process where a word of one word class gradually begins to be used like a word from another class (see, e.g., (1) I think that the role of the Ambassador in the Soviet Union is a very key one.\nBoth key and fun in the above examples are correctly tagged as adjectives in COHA.\nWhether or not the tag has been probabilistically assigned by the tagger or manually inserted as a rule is not obvious, but at this stage we can note that the tagger can correctly identify at least some usages of these items as adjectival.\nFigure (3) (4) (5) (6) The situation becomes even more complicated when the research focuses on items that are attested with a lower frequency, or on items whose adjectival use has not yet become conventionalized to the same degree as key and fun.\nIn (7) to (11), taken from COHA's sister corpus, the Corpus of Contemporary American English (COCA), all the \"nouns\" in boldface are used in a descriptive and non-referential function (i.e., similarly to key and fun in Examples (1) to (4) above), and yet none of them has been annotated as an adjective in the corpus.\n(11) The construction project was mammoth by the standards of the day.\n(12) (COCA, Fic, 2008)\nHe wasn't just killer good-looking.\nHe was to die for.\n(13) (COCA, Fic, 2002)\nBusiness meeting:\nClaudia Lester was textbook perfect.\nTo summarize, the pitfall related to POS annotation in the case of category change is that even though word classes are treated as static entities in corpus annotation, they are in fact dynamic; word classes exhibit both category-internal and category-external gradience, and this gradience may be a result of ongoing language change.\nThis problem is of course not limited to historical corpora but also affects present-day corpora when the process of change has not reached its conclusion.\nWhile software like CLAWS also provide probabilistic information about word class, in practice this information is typically not available to the end users of the corpus.\nA possible solution to these kinds of problems is to make use of queries that not only combine lexical items with POS tags (e.g., key_j, fun_j) but also target the surrounding context of the item under study.\nTheoretical choices in the design of the annotation scheme\nThere are also cases where it may be impossible to make use of POS annotation because of the idiosyncratic tagging of the corpus.\nFor example, in However, our approach was partly unsuccessful because of the theoretical choices made in the tagging of the corpus (see Whether or not Huddleston and Pullum's analysis is ideal from the perspective of word class theory is not relevant in this case; the important thing is that the PCEEC is annotated in a way that makes it impossible to study colloquialization by measuring the frequency of prepositions in different time periods.\nAs pointed out in Annotation tailored to specific research questions Our third example of a potential POS-related pitfall pertains to a case where the corpus compilers were themselves working on the long diachrony of English and used a conservative annotation scheme to facilitate comparability over time.\nThe PCEEC is one of the parsed corpora of historical English produced in collaboration between the universities of Penn, Helsinki, and York, among others.\nThese corpora are intended to cover all stages of the history of English, and as such, the annotation scheme has been designed to be backwards compatible all the way to Old English.\nThis is of course a laudable aim and makes the corpora invaluable for research on historical syntax.\nHowever, it severely complicates the study of certain types of research questions that rely on POS annotation, as will become apparent below.\nIn our case, we were interested in examining changes in the POS frequencies in the PCEEC over time.\nIn our first explorations, we focused on the frequency of nouns and personal pronouns in the corpus\nWe initially assumed that any compound tag ending in a noun tag could be counted as a noun.\nSometimes this was indeed the case, as in sixpence_NUM+N.\nAt other times, however, these represented other parts of speech that had grammaticalized from nouns (see, e.g., To resolve these issues, we ended up retagging the entire corpus in order to move the grammaticalized items from nouns to their current parts of speech.\nThis was highly labour-intensive, especially as some of the compound tags were ambiguous in that they included both genuine instances of nouns, as in gentle-man_ADJ+N, as well as adverbs, as in likewise_ADJ+N; there were 1,199 instances of ADJ+N alone.\nMoreover, because the corpus contains private letters from a period before English was standardized, there is a great deal of spelling variation in the corpus.\nConsequently, it was not enough to simply list all adverbs etc. that could have been tagged as nouns; instead, we had to identify all their spelling variants as well, including those that had been written as two words (e.g., like_ADJ wise_N).\nThese tokens were combined and reclassified, so the retagging involved changes in tokenization, too.\nTime-consuming though this process was, it enabled us to gain interesting and reliable results, which we refined and augmented in our study discussed above Large corpora Inaccuracies in text sampling\nIn this section, we give an example of a potential pitfall related to the semi-automated sampling of corpus texts.\nHere, it is not our intention to argue that the texts included in big-data corpora should be sampled manually; indeed, when the corpus includes samples from tens or even hundreds of thousands of individual texts, it is obviously not feasible to check all the data sources individually.\nConsequently, big-data corpora are likely to contain errors pertaining to the dating of some of the texts, their genre, and even the language variety, which the researcher must be aware of.\nNeedless to say, even though some amount of data-related noise may be tolerable in the analysis, sampling errors like these can sometimes lead to disastrous results if one does not exercise sufficient care in data collection and analysis.\nAs an example of a pitfall related to text sampling, we discuss a particular usage of the complex adverb as well, which caught our attention some time ago.\nWhile as well is commonly used in a clause-final position in cases like ( (14) Not only are we stranded on this dreadful planet, but we are starving as well.\n(COHA, TV/Mov, 1968)\n(15) This interaction with readers continues not only through the mail but on-line (COHA, NF/Acad, 1995) as well.\n(16) These communities have significant African-American and Hispanic-American populations, among others.\nAs well, they are largely low-income (COHA, NF/Acad, 1994) communities.\nWe consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English.\nAccording to our corpus queries, the frequency of the usage was generally very low in the corpus, but it was clearly becoming more common in the late twentieth century\n(Figure\nImportantly for our preliminary analysis, the frequency of these potential bridging contexts and the frequency of the sentence-initial connective uses converged in an interesting way in the corpus: we see the frequency of the bridging contexts decrease as the frequency of the sentence-initial connective uses increases (Figure (18)\nAn attempt is made to bind up and fetter this country's expanding energies and prospects with Old World theories and methods.\nAs well attempt to put (COHA, Mag, 1874) baby-clothes upon the statue of Hercules.\nConsidering all this evidence, we were optimistic that the phenomenon was indeed real and that we were studying a usage that had thus far been ignored in the literature.\nWe were only concerned about the low frequency of the form: in the last period studied , there were 32 tokens of the sentence-initial usage in COHA, which meant that our results might have been tarnished by errors made in the sampling of the corpus.\nUnfortunately, this is exactly what had happened.\nIndeed, our initial enthusiasm was quickly dampened by the observation that the connective use of as well was dispersed very unevenly in the corpus.\nOn closer inspection, we were able to establish that 13 of the 32 tokens, all from the 1990s and the 2000s, were in fact produced by Canadian authors.\nSix additional tokens in the 1980s sample were produced by an author from Portland, Oregon, and one from Seattle, Washington, which raised strong suspicion of Canadian influence.\nFinally, the author of one sentence-initial usage turned out to be Irish.\nWhen these data are excluded from the results, the frequency of the form is so negligible that the argument for an incoming connective use of as well in American English can no longer be maintained.\nNow that we knew that the connective use was probably due to sampling errors in COHA, we conducted a new literature survey with a focus on Canadian English (CanE).\nWe promptly found brief mentions of the sentence-initial usage in Changes in the balance of subgenres\nOur second example pertains to genre balance, and in particular to the effect of including new subgenres in a corpus over time.\nIn (20) (COHA, News, 2017) We are very pleased with the court's ruling.\nBecause we were interested in the potential impact of gender on the change, we chose as our dataset the fiction section of COHA, which contains named authors for whom gender metadata has been generated by\nHowever, when we began to look for potential reasons for this lag, we noticed that the internal balance of the fiction subcorpus changes over time.\nWhile most of the corpus consists of novels, the most recent periods in the corpus include increasing amounts of drama, short stories and movie scripts, for example.\nWhen we restricted our corpus queries to novels alone, which is possible if one uses the downloadable version of COHA rather than the online interface, the gender difference disappeared.\nThis example serves as another illustration of the pitfalls of mega-corpora that are not as carefully sampled and balanced as smaller corpora.\nOf course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability.\nIn the same vein, it is completely justified to include movie scripts in the fiction section of COHA as movies begin to be made in the twentieth century.\nOn the other hand, the history of many fiction genres, including drama and short stories, is certainly longer than the coverage of COHA, so their proportions could have been more optimally balanced in the corpus; considering the automated sampling of COHA, their better representation in the most recent periods probably reflects their increased availability in digital form.\nHistorical databases Issues with balance and metadata\nIn recent years, corpus linguists have been increasingly interested in making use of massive historical databases, such as the Eighteenth Century Collections Online (ECCO), in their research.\nECCO consists of more than half of all known British publications in the eighteenth century, or about 200,000 texts and 10.5 billion running words\nFortunately, there is ongoing research that aims to shed more light on what exactly the database contains.\nFor example, With the help of the ESTC metadata, harmonized and augmented by the Helsinki Computational History Group (COMHIS), the ECCO dataset can be narrowed down to first editions only, which greatly facilitates linguistic research.\nThe metadata also provides opportunities for generating principled subcorpora of ECCO, such as economic literature OCR errors\nCrucially, the digitized texts in ECCO do not even represent \"God's truth\" about the original publications on which they are based, owing to severe errors in optical character recognition (OCR) in the digitization process.\nThese errors, the rate of which varies between Parts I and II Hapax legomena\nTo give a concrete example, many studies of lexical and morphological productivity rely on accurate type counts, particularly of hapax legomena, that is, words that only occur once in the corpus.\nHere, the small subset of ECCO that has been keyed in manually by the Text Creation Partnership (ECCO-TCP), and which is hence free from OCR errors, provides an illustrative example.\nComparing the texts shared by ECCO and ECCO-TCP, we find that more than 90% of the hapax legomena in the ECCO sample (henceforth ECCO-OCR) are not found in ECCO-TCP.\nThis means that they are in fact spurious OCR errors and that the precision of queries based on rarity is abysmal (Figure Historical lexis\nA recent study of economic vocabulary in ECCO\nThe second issue relates to the fact that some characters are more likely to result in OCR errors than others: in particular, the long \"s\" and ligatures are more likely to be incorrectly identified.\nFor instance, we aimed to study the spelling variants of economy to analyse the diachrony of the transition from the Latinate variants, \"oeconomy\" and \"oeconomy\", to \"economy\".\nHowever, by taking a small sample of the instances and going back to the document images, we discovered that about a third of the instances of \"economy\" were in fact OCR errors for \"oeconomy\".\nThis made accurate timing impossible, unless we were to take a more representative sample and check all the document images manually, which would have been a very labour-intensive task.\nMoreover, in our multi-dimensional analysis of economic vocabulary where we mapped the words in a subset of ECCO to the \"trade and finance\" section of the Historical Thesaurus of the Oxford English Dictionary, none of the words that emerged as significant had an \"s\" in them, suggesting that these might have been missed owing to the problem with the long \"s\".\nHere one solution would be fuzzy searching, or at least adding variants with \"f \" or \"l\" for \"s\" to the queries.\nThe texts in ECCO were digitized and OCR' d in the 1990s and 2000s.\nSince then, OCR methods have vastly improved, so it is to be hoped that the document images -poorly scanned as they may sometimes be -could be re-OCR' d in the future, with much better results.\nThese methods rely on deep learning and neural networks, which are becoming increasingly common in other linguistic applications as well, for instance word embeddings, which are used for lexical semantics.\nDeep learning is an area of computer science that is highly resource intensive, which means that the restrictions of hardware and software, already mentioned by Discussion and conclusion\nIn this paper, we have discussed pitfalls related to some of the widely used historical corpora and databases in the context of current corpus-linguistic research.\nOur main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata.\nIn other words, the increased use of big-data resources in historical research has resulted in fundamental changes in research methods, data analysis and research questions, and while these changes have provided researchers with exciting new possibilities, they have also presented new challenges.\nWe also suggest that these challenges are ultimately not so different from the pitfalls discussed by Rissanen in his \"philologist's dilemma\": just like in the early days of historical corpus linguistics, today's challenges pertain to the linguist's knowledge and understanding of their data.\nHowever, because the focus of research has in many cases shifted from small, carefully compiled corpora to bigdata resources, the nature of the \"data\" and methods with which historical corpus linguists typically work has changed.\nConsequently, we suggest that it would be prudent to think about the principle of \"knowing one's data\" from a new perspective: as the sheer size of the modern mega-corpora prevents scholars from engaging with either all or the majority of the original source texts in as much detail as in the early days of historical corpus linguistics, they should strive for an intimate understanding of the historical corpora as mediators of the original texts.\nAdmittedly, this perspective signifies a trade-off between two types of knowledge.\nOn the one hand, the linguist must accept that while sociocultural contextualization remains as important as ever, it is not always possible to check every corpus text or concordance line manually.\nOn the other hand, the linguist can compensate for this to some degree by acquiring a thorough understanding of the corpus and its description.\nThis knowledge is more abstract in nature, and it is partly a reflection of the more abstract kinds of research questions explored in corpus-based research today, as well as of a shifting focus towards an increasingly statistical orientation in research design.\nThis shift also means that researchers must be willing to accept a certain degree of data-related uncertainty or \"noise\", which cannot always be controlled as well as when one works with smaller corpora.\nIn the following, we discuss these issues and potential solutions to them in light of our case studies.\nOur first examples concerned problems related to part-of-speech annotation, and these challenges are hard to overcome entirely.\nEven when the coding schemes are intended to be theoretically as neutral as possible, the POS tags always add an analytical layer to the corpus, which reflects a particular theoretical stance towards word class categorization.\nBecause of this analytical interference, In our case study on as well in Section 3.1, the confounding influence of miscategorized Canadian sources could be controlled because of the low frequency of the form, but if the frequency of the form had been higher, the likelihood of our spotting the miscategorized texts would have been lower.\nHowever, based on the proportion of the Canadian texts that yielded a number of hits of as well in our query out of all the texts sampled in COHA, the effect of CanE on most research questions studied with COHA is likely to be negligible.\nIndeed, we wish to emphasize that resources like COHA are invaluable for diachronic linguistic research, and that these resources are even more useful when one knows exactly how they are constructed and annotated.\nFurthermore, the biases can be combatted methodologically, for example, by using \"dispersion-aware\" methods\nIn the case of changes in the balance of subgenres (Section 3.2), one solution is for corpus compilers to provide detailed, or at least basic metadata, that enables users to zoom in on maximally comparable subcorpora or to take balanced samples of their own.\nAs\nalso\nsuggested by\nIn addition to problems with metadata, the noise present in the digitized texts in historical databases (Section 4) may lead to compromises in accuracy in corpus-linguistic research.\nWhile the precision of queries can always be increased by going through the hits manually or semi-automatically (taking a smaller sample of the full dataset if needed), striving for perfect recall in data afflicted by OCR errors may prove to be a doomed endeavour.\nAs noted in Section 4.2.1, however, if the OCR errors are distributed relatively equally across the corpus data, settling for lower recall may be justified, as there is so much data that missing some of it does not significantly impact the results.\nWhether or not lower recall is acceptable ultimately depends on the research question.\nThe principle of knowing one's data and being \"on really intimate terms\" with the data To conclude, we argue that all linguistic research conducted on historical corpora and text databases benefits from an understanding of not only the texts and the historical language variety but also from the sociocultural contexts in which the texts were produced.\nFrom the corpus compiler's perspective, one way of contextualizing the texts is to provide relevant metadata to the end users (e.g., Named entities as potentially problematic items in corpora Mark Kaunisto Tampere University\nThis chapter discusses problems in the interpretation of corpus data arising from the insufficiencies in the annotation of named entities.\nMany corpora nowadays still do not adequately enable corpus users to set up queries that would exclude items appearing in names when needed to improve precision of the searches.\nThrough an examination of case studies in major English language corpora, the chapter highlights the need to carefully post-process the search results, as irrelevant occurrences of named entities may pose challenges in the analyses of word frequencies and their collocational behaviour.\nThe chapter calls for more detailed annotation of named entities in already available large linguistic corpora and reminds of the importance of close inspection of the search hits.\nKeywords: named entities, proper names, annotation, corpus linguistics Introduction\nAs we have reached the 2020s, corpus linguistics as a branch of linguistic study has progressed in many ways from its early days, nowadays offering a multitude of possibilities of examining various facets of language use.\nNot only have new corpora become available that are massive in size compared to those compiled in the 1980s and 1990s, but search engines or online interfaces have also become technically more and more sophisticated, giving users the opportunity to observe statistically organised search results that go far beyond the presentation of mere concordance lines.\nIn concrete terms, the changes are noticeable on the level of educating students about the basics of corpus linguistics: with the evolving nature of the field, there is a constant need to update course materials to provide novices with an overview of both the possibilities and challenges relevant to corpus study.\nWith new corpora containing billions of words and allowing complex searches by making use of various search functions and different types of linguistic anno-tation, novice corpus users may find it difficult to question the relevance of their search results adequately and carefully.\nIn line with the \"God's truth fallacy\" noted by There are many types of items commonly found in corpora which, while they are perfectly representative of language use, may present problems when investigating patterns of language use.\nFor example, There are also items which, because of their high frequency, pose other, quite serious challenges in corpus analyses, and the ubiquitous references to named entities can be identified as one such issue.\nCorpus users are probably well aware that items found in proper names -for example, words found in titles of books, articles, films, etc. -may be regarded as frozen items and would normally be excluded from further analysis because the choice of the exact words in such instances was usually made by someone else than the authors or speakers themselves.\nThe identification of multi-word units functioning as proper names is a task that has received a great deal of attention from scholars to solve problems relating to different purposes -for example, data mining, automatic translation, named entity recognition -generally in the field of Natural Language Processing (NLP).\nMuch work has gone into developing efficient annotation strategies for differentiating items belonging to named entities from those representing regular uses of the words.\nThe task is obviously not straightforward, as names differ greatly as regards their structural complexity, and in some cases, they can be rather long, as in Example (1) from a review of a music album.\n(1) Originally titled 'The Southern Harmony And Musical Companion Featuring A Choice Collection Of Tomes, Songs, Odes And Anthems From The Most Eminent Authors In The United States' , this is the record that was struggling to be (BNC, CHA 4244) heard inside the more studio-constricted 'Money Maker' .\nIn this example, we can see that the title of the album is a lengthy one, and it can be observed that although the mention of the title in practice constitutes a single reference to only one entity, the 25 words of the name are similar to a quote in that none of the words and their combinations were originally produced by the author of the review.\nProper names and multi-word proper names in particular pose challenges to the study of language use, and many currently available linguistic corpora are lacking in this kind of annotation.\nIn automated collocational analyses, for instance, it may not always be possible to make use of part-of-speech tags to distinguish between instances of words where the word has been a part of a proper name or not.\nThe situation is somewhat easier if we need to distinguish between proper nouns and common nouns, as far as we can rely on the accuracy of the tagging.\nBut as regards, for example, the occurrences of adjectives within proper names -as in magical in Magical Mystery Tour\n-no annotation is necessarily available for us to separate such instances from regular uses of the adjective, and close manual inspection is therefore needed to exclude such items from the analysis.\nIn this chapter, I will examine through different types of examples how the frequencies of English proper name uses can distort studies focussing on word frequency and collocational behaviour, and how the occurrences of proper name use may show different degrees of prominence of words in different genres and regional varieties.\nSection 2 provides the broader background in relation to English proper nouns, proper names, and the strategies of annotating names.\nSection 3 presents small-scale studies of corpus searches involving words that may be frequently used as named entities, or parts of named entities.\nSection 4 provides further discussion and concluding remarks.\nOverall, the main argument is that due caution must be given to such items and sufficient manual inspection of concordance lines is needed to avoid the possibility of misinterpreting initial findings from corpus data.\nBackground\nThe concepts of proper nouns and proper names\nMany studies on the annotation of named entities start off with descriptions on how the concept of \"name\", or \"proper name\" has been described and defined, going back to the theoretical postulations by philosophers such as John Stuart Mill, Bertrand Russell, and Ludwig Wittgenstein (see, e.g., Some grammarians (e.g., In the domain of NLP, the term 'named entities' is often used as a broader umbrella term, including temporal and numerical expressions in addition to conceptually more straightforward 'proper nouns' (see, e.g., Annotation of named entities As regards the task of annotating corpus data, the identification and annotation of named entities has been an area of interest from the very beginning, both in terms of manual and automated annotation strategies.\nThere have been different ways of annotating named entities in corpus data, and the strategies tend to reflect both the various interests (e.g., translation and information retrieval, alongside general language study) as well as the practical possibilities of doing so.\nThe guidelines used for the earliest English corpora compiled for linguistic analysis such as the Brown Corpus already observed the problems in the tagging of proper names.\nThe annotation of multi-word proper names was also recommended in the corresponding guidelines for the Penn Treebank Project, in which Considerably more in-depth work on the development of automatic algorithms to identify and annotate named entities in digital texts has been done in the realm of NLP and Computational Linguistics, with the aim of increasing accuracy and precision related to tasks such as data mining for named entities and machine translation.\nWithin NLP, Named Entity Recognition (NER) is now seen as one of the major tasks under the broader field of Information Extraction (IE).\nThe challenges in such tasks include, for example, taking into consideration the differences between languages in terms of how the concepts of proper nouns and proper names are understood and how such items manifest themselves on a grammatical level.\nThe use of capital initials is not a fully reliable marker of the status of a proper noun even in English (see, e.g., To study annotated corpora efficiently, it is important to be aware of what kinds of things have been annotated and how, as this ultimately affects our understanding of the expected levels of precision and accuracy of our search hits.\nIn addition, it is also true, as stated by Case studies\nThis section looks into a number of lexical items whose analyses based on corpus searches are complicated by a high number of named entity instances found among the search results.\nWhat is the extent of potential noise, and why is the only option often to manually inspect the concordance lines in order to exclude irrelevant items from the analysis?\nSome of the items examined here are selected based on observations of individual problematic instances made over the years, while some near-synonymous word pairs chosen for closer study have one member of the pair fairly frequently occurring in different types of named entities, and the aim is to see how much such cases influence the studies of the near-synonyms.\nIn the 100-million-word British National Corpus, (for the purpose of the searches for this chapter, I have used the BNCweb interface),\nCommon nouns used as (parts of ) proper nouns: Lifespan and samurai\nIf one performs a frequency list search of the singular nouns in the BNCweb with the _NN1 tag and begins to browse through the most frequently occurring nouns on the list, paying attention to the structural make-up of the words, it is probably unsurprising to see that the most common singular nouns are monomorphemic ones.\nNouns such as time, way, year, day, and man appear at the top of the list.\nNot much further down the list, we also find nouns formed with a root and an affix (e.g., government, business, and development).\nThe most frequent singular noun consisting of two roots on the list is chairman, and other nouns of this type include football, newspaper, background, and bedroom.\nAmong the highest-ranking nouns of this type is lifespan, followed by words such as database, policeman and classroom.\nConsidering the most common two-root singular nouns, we might be surprised to find lifespan featuring as one of the most frequent ones.\nWith a total frequency of 3,700, and a normalized frequency of 37.6 instances per million words, it can be regarded as a high-frequency word in the corpus.\nHowever, a closer look into the instances of lifespan informs us that all the 3,700 hits are found in only 83 texts, which suggests the word probably has a very uneven dispersion across the corpus.\n(2) Customers who have purchased a LIFESPAN maintenance agreement are enti-(BNC, HWF 15266) tled to call upon the services of the LIFESPAN help desk.\nThe case of lifespan is an example of the problem of accurate annotation in cases where common nouns are used as proper nouns, as has been observed in previous studies by, for example,\nAnother interesting common word that is often found in proper names is the loan word samurai.\nIn her MA dissertation on the occurrences of Japanese borrowings in six regional varieties in the Corpus of Global Web-Based English (GloWbE; Davies 2013), All instances of samurai in GloWbE are tagged as singular common nouns, (3) a.\nInside he said he saw Mr Fisher holding a samurai sword.\n(GloWbE, Great Britain, General, www.thisisstaffordshire.co.uk) b.\nBelieving he was carrying a Samurai sword, the officer called for backup and the police helicopter was scrambled\n[…\n]\n(GloWbE, Great Britain, General, www.southwalesargus.co.uk)\nTable\nThe differences seen in the rates of samurai in named entities potentially also reflect some cultural differences and the interests of the writers represented in the corpus: the named entities including samurai in the GB and US sections included more references to drama films (The Seven Samurai, The Last Samurai), the Asian sections featured references to food-related items, computer software (Market Samurai, a keyword research tool), action toy figures (Samurai Predator AC-01).\nThe popularity of the word in the names of video games or board games (The Way of the Samurai 4, Samurai Warriors 3, Samurai Spirit) was seen in all sections examined.\n5 3.2 Near-synonymous adjectives in named entities: Limited/restricted, royal/regal and fantastic/fabulous\nIn addition to difficulties caused by common nouns used as parts of names, words representing other parts of speech may also be problematic when they are included in a name.\nFor instance, in popular culture, the names of entities such as films, songs, books, etc. may feature a high number of adjectives, and some adjectives may appear more commonly in them than others.\nThe tagging of some of the most well-known named entities in the BNCweb and the Corpus of Contemporary American English (COCA; Davies 2008-; which both use the CLAWS tagger), such as the White House (the official residence of the president of the United States), seems to take account of the use of capital initials, and mostly White in that name is tagged as a proper noun (_np or _NP0).\nAlthough for the most part, white in references to the White House is tagged as a proper noun, there are also some instances in BNCweb and COCA where it is tagged as an adjective; particularly in COCA, there are altogether 1,497 hits (searched August 27th, 2022) with this tagging, showing various combinations of capital or lowercase spelling of initial letters of white and house (mostly referring to the building in Washington D.C.).\nInterestingly, the House part in the name is usually tagged as a common noun.\nLists of high-frequency names are often used when designing automatic taggers, which might account for the fact that with names of lower frequencies, the tagging of white in names appears less systematic.\n6 5.\nConsidering the fashionableness of the concept of samurai in popular culture today, it is quite possible that the occurrences of the word in named entities would show changing trends in diachronic corpora, and the proportions of hits of the word in names as opposed to common noun uses vary from one period to another.\n6.\nAs noted previously, in the BNC2 manual, One kind of implication of difficulties in the tagging of named entities can be seen in the corpus-based analysis of near-synonyms, and here we will take a look at some examples of adjective pairs to illustrate the types of problems that can be posed by the occurrences of adjectives in proper names.\nOn a purely intuitive basis, we might predict that the comparison between the adjectives limited and restricted could be affected by the fact that limited appears in names of\nIn the BNCweb, there are no instances of limited which are tagged as a proper noun, regardless of the use of a capital or lowercase initial.\nThere is variation in how limited and restricted are tagged, but it concerns the identification of the words as either adjectives or participial forms of their underlying verbs.\nTo keep the analyses simple, the words were searched in BNCweb with the tag query _AJ*.\nThe wildcard * allows one to target instances where the words were either assigned an adjectival tag or the portmanteau tag (sometimes also referred to as an ambiguity tag) _AJ0-VVN.\nThe order of the two tags in a portmanteau tag is significant: in this case, the automatic tagger has found insufficient evidence to determine the accurate part of speech, but that it was more likely an adjective -the first element of the tag -than a past participial form of a lexical verb -identified by the tag _VVN.\nIn order to assess the frequencies of the adjectives limited and restricted, and then to manually inspect the frequencies with which they occur in names, searches were made in three written sections of the BNC -Academic prose, Non-academic prose and biography, and Newspapers, which also allows for comparisons of the frequencies between written domains.\nThe frequencies of the adjectival uses of the words, starting with total numbers of hits and then providing separate figures for nonnamed entity and named entity uses, are presented in Table As can be seen in Table 7\n.\nThe adjective restricted was rarely used in named entities; for example, the few instances of the word in newspaper texts related to the name of a horse racing category.\nIn other words, the potential of finding unwanted or irrelevant items may in some cases vary between different sections of the corpus.\nSimilar problems are encountered if we examine the adjective pair royal and regal.\nIn the BNCweb, there are 56 instances where royal with a capital initial is tagged as a proper noun (NP0), as in Royal Exchange Square (as a part of an address), Palais Royal, Park Royal, and Musée Royal des Beaux-Arts, and in these cases all elements of the names are applied with the NP0 tag.\nHowever, out of the total number of hits of royal in the corpus -14,628 hits in 1,825 texts -it appears with a capital initial as often as 10,237 times in 1,603 texts).\nIn the vast majority of instances with an capital initial royal is tagged as an adjective, as in the Royal Academy, the Royal Shakespeare Company, the Royal Exchange Theatre, the Royal Commission on Environmental Pollution, the Royal Mile, the Royal Albert Hall, the Royal Navy, and so on.\n8 The adjective regal is overall considerably less frequent 8.\nConsidering the names in which royal with a capital initial is tagged as an adjective instead of a proper noun, the types of entities in question is reflected in the tagging, as noted in the BNC2 manual; in the names of institutions or charters, royal is an adjective, in the names of locations it is treated as a proper name.\nthan royal in the BNCweb, with 304 instances found in 152 texts, all of which are tagged as adjectives, although in a number of cases the word appears in names, as in the Regal Scottish Masters (a snooker tournament), the Regal (a cinema theatre), the Regal Arms (a hotel), and Regal Trophy (a rugby league).\nThere are altogether 194 hits of regal spelled with a capital initial, although not all instances are proper names.\nIn other words, in the case of both royal and regal, substantial proportions of the occurrences of the words in the corpus appear in proper names, and it would be crucial to exclude such instances if we wanted to study of the use of the words as non-frozen elements.\nIn lexical studies, near-synonyms are investigated for various purposes and interests: to reveal fine-grained distinctions and principles behind word choices, to examine variation between dialects and sociolects, or to provide assistance to translators and language learners, to name but a few.\nWhen we examine the characteristics of near-synonyms with corpus data and attempt to analyze the differences in the uses of the words, we typically examine their collocational behaviour.\nMany corpus interfaces and tools allow for the analysis of the strongest collocates of words based on different methods of assessment: Mutual information, log-likelihood, T-scores, raw frequency ranking, and so on.\nIn such analyses, it might be beneficial to be able to exclude those tokens which appear as a part of a name.\nIf the taggings of the corpus data are not helpful in this regard, one can try to perform case-sensitive searches; in the BNCweb, the searches could be restricted to all-lowercase spellings of royal and regal, for example.\nHowever, this would compromise those cases where the words appear at the beginning of a sentence.\nConversely, particularly in British English, the adjective royal is sometimes spelled with a capital initial, perhaps out of respect to the monarchy, even in cases where it is not a part of a name, as in \"The old Royal adage of never complain, never explain just won't do any longer\" (BNC, CH6 6584) and \"Princess Diana got the star billing at a Royal banquet on board the Britannia last night\" (CBF 9645).\nIn addition, in some collocations there are variant types of spelling of royal: for example, we find instances of royal family, Royal family, and Royal Family, and the form of spelling does not necessarily make it clear whether the authors are using the phrase as a name.\nAll such considerations potentially add to the problems of analysing of collocational strength, which in the case of royal and regal may be more complex with British English data.\nSome corpus interfaces such as the one at English-Corpora.org allow users to compare the collocates of two words, and the collocates which are more typically used with the compared words are listed in two separate columns in a ranked order, based on a score that takes into account the frequencies and comparative ratios of the two words in the corpus, the frequencies and ratios of different combinations of the words and their collocates, as well as the frequencies of the individual collocating words across the corpus.\nThe listings are very useful in highlighting the differences between near-synonyms which are reflected in their collocational behaviour.\nHowever, we can note that analyses of collocations can also be affected by occurrences of the compared words in names; for example, we can consider the adjectives fantastic and fabulous.\nAdjectives sometimes differ as regards their adverbial modification, and adverbs have also been seen to differ as regards their degrees of use in different regional varieties of English (see, e.g., None of the instances of absolutely fantastic in the US, GB, and IE sections were names or parts of names.\nSo, when we examine the adverb modifiers of adjectives, we know that the collocation scores probably exaggerate the association between the adverb absolutely and the adjective fabulous, as the scores are skewed by the frequent occurrences of the combination of those two words in names.\nIf we tried to adjust and recalculate the scores of collocational ratios of absolutely fantastic and absolutely fabulous by removing those instances of absolutely fabulous found in names, we would also need to subtract the corresponding instances from the numbers of instances of fabulous and absolutely fabulous.\nThe recalculated scores for collocational strength for absolutely fantastic and absolutely fabulous would then be reversed in the Great Britain and Ireland data (in the GB section, the scores for absolutely fantastic and for absolutely fabulous are 1.7 and 0.6, and in the IE section, the corresponding scores are 1.2 and 0.8), indicating a tendency for absolutely to be used more frequently as a modifier to fantastic than fabulous, although the difference between the scores would not be high enough for absolutely to be identified as collocating strongly with fantastic in comparison with fabulous (i.e., absolutely would not be highlighted in the results list with green colour).\nBut such recalculations, of course, are not entirely accurate, as we would be assuming that the scores for all the other adverbs modifying the two adjectives would be unaffected.\nSubtracting the instances of absolutely fabulous might not affect the rankings of other collocations to a notable degree, but for things to be ideal, we would need information on the named entity uses of all combinations, and for that purpose manual inspection is practically impossible.\nNevertheless, based on these observations, it would be highly advisable to inspect concordance lines of the highest-ranking collocations to check for possible skewing effects by names.\nDiscussion and conclusion\nConsidering the occurrences of names in the corpora examined, it can be argued that it would be beneficial to have the option of building a search query that would enable one to include or exclude items if they constitute a part of a name.\nUltimately names also contribute to the word count of a corpus, and it might also be beneficial to exclude named entities from automated calculations in cases where names -or other types of items in a corpus which may be irrelevant to what one is trying to assess by the calculations -do not reflect the active linguistic choices of the writers or speakers represented in the corpus.\nOf course, the uses of words in different types of names is an interesting question in its own right, and the words found in names definitely carry meanings beyond the moment of giving an entity a name.\nAs mentioned, the question of exclusion arises from the idea that a person referring to the entity by its conventionalized name is bound by this convention and is not at liberty to use other words than the ones in the name, expect for any understood or established variants of the name.\nBy drawing attention to the question of words appearing in names, the present chapter has made the point that it is not always necessarily clear from the outset how much and what kind of post-processing of the search results is needed.\nAs all users of corpora are not necessarily aware of the variety of things to watch out for, a set of examples was presented of cases where occasionally large proportions of search hits can turn out to be -depending on the point of view of the study -instances that one might want to exclude from the study, which may not always be a straightforward matter.\nAs has been observed in instances such as the noun lifespan in the British National Corpus, and the adjectival phrase absolutely fabulous in British and Irish sections of the News on the Web corpus, sometimes the irrelevant tokens may far outnumber the relevant ones.\nThe concerns and challenges addressed in the present chapter are by no means new ones, and many existing corpora are frequently updated and improved to make the search interfaces more user-friendly as well as to increase the reliability of the findings.\nAs a lot of work has been done on named entity recognition in the field of computational linguistics, a key to address the problems raised in commonly used, large corpora in corpus linguistics would be to increase the collaboration between the areas, and to integrate the systems and practices developed in NLP and computational linguistics into corpora such as the BNC.\nAn encouraging example of such collaboration is the Clean Corpus of Historical American English Thinking back to the problems relating to the use of corpora as outlined by Challenges in the compilation, annotation, and analysis of learner corpus data Marcus Callies University of Bremen\nThis chapter highlights and discusses the special characteristics of learner corpus data and the challenges they may present for corpus compilation, annotation, and analysis.\nBecause learner corpus and SLA researchers use their data to study L2 production and development, it is of utmost importance that the data are valid, that is, they represent \"authentic\" L2 production, which means that the data must stem from the studied learners' own language production.\nI discuss challenges in three areas:\nIntroduction and general remarks\nLearner Corpus Research (LCR) is a relative newcomer to the scene of research paradigms and methodologies within applied linguistics and second language acquisition (SLA) research.\nLCR as a field only emerged and became visible in the course of the 1990s in the context of the popularization of corpus linguistics at large but has rapidly evolved and grown in scope and sophistication over the past four decades.\nTwo handbooks that survey the field and discuss its links to the major neighbouring disciplines of corpus linguistics, SLA and language teaching bear witness to this rapid development Challenges and how to respond to them\nMultilingual practices and metalinguistic language use\nTexts contained in learner corpora have typically been produced by bi-or even multilingual individuals, and learner data are rich in multilingual practices and phenomena induced by language contact, such as code-switching, foreignizing (the morphophonological modification of an L1 form to adapt to the structure of the L2) or calquing (literal translations of expressions from the L1 into the L2).\nImportantly, they are indicative of interlanguage development.\nIn SLA, the term and concept of 'interlanguage' refers to a systematic and independent developing learner grammar that should be studied in its own right and contains elements of the learner's L1 and L2 but also independent structures\nMoreover, learner corpora, especially those of academic texts, contain expert terminology, metalinguistic language use such as contextualised examples of language use, citations or mentioned items, and sometimes even whole abstracts or thematic summaries from other languages.\nSuch instances usually do not represent the respective learners' own written production as they are typically taken over or copied from secondary sources.\nOn the one hand, from the point of view of quantitative L2 analysis, these could thus be considered unwanted items or 'false positives' as their inclusion in word counts and concordance analyses will affect findings on what structures learners may have acquired and are able to produce.\nThus, such cases have to be treated separately so that they can be excluded from search results and word counts to not distort the data in learner corpus studies.\nOn the other hand, they are part of the text, they cannot simply be removed, as this may reduce the context or even render the text illegible.\nAdditionally, their textual embedding and glossing may be of interest for future analyses, for instance in studies of intertextuality and referencing in academic writing, so that it seems desirable to preserve them in their original form and syntactic function.\nResponse Data of the kind discussed above should be specifically annotated/tagged.\nFrom a practical point of view, the annotation of instances of multilingual practices in learner corpora facilitates their automatic search and identification through corpus software such as concordancers.\nIt also allows for their exclusion from analysis and frequency counts if necessary.\nDespite the pervasiveness and importance of instances of multilingual language use in SLA, learner corpora are not commonly annotated for such features.\nIn some corpora, however, specific types of multilingual language use have explicitly been annotated.\nOne example is the Louvain International Database of Spoken English Interlanguage (LINDSEI; (1) I don't know the the . the name in English but (eh) here we call it <foreign> (LINDSEI; SP025) Traducción e Interpretación </foreign>\nA similar practice has been followed in the annotation of the Spanish Learner Language Oral Corpora (SPLLOC), a set of corpora of L2 Spanish that were transcribed using the CHAT system developed by the CHILDES project.\n(2) a.\n*\nP63\n:\ny cómo se dice scuba@s:d diving@s:v ?\nb.\n*P51: no están en el sol están en shade@s:d.\n(3)\nAdditionally, learners' use of indeterminate forms and idiosyncratic neologisms (referred to as \"invented words\", see SPLLOC Transcription Guidelines 2008: 18), apparently mostly cases of foreignizing or calquing, were marked with the code @n at the end of the word as shown in ( (4) *P54: um ehm detrás de lo eh pictura@n eh hay [/] hay un número de turistas.\nThe transcription guidelines also provide for codes to mark the use of a third language different from Spanish or English (see From the perspective of the corpus user, the annotation of instances of multilingual strategies such as codeswitching, foreignizing, and calquing may prove useful as it makes it possible to systematically retrieve and study them in a learner corpus.\nAnalyzing such passages can provide new insights into interlanguage development, specifically in terms of the multilingual strategies and communicative competencies of the learners.\nAs for academic texts that contain expert terminology and instances of metalinguistic language use, Task effects\nA further challenge that compilers and users of learner corpora have to deal with is unwanted lexical or constructional bias which results in an accidental overrepresentation of certain structures in the data.\nThis bias can be introduced either indirectly by the topic of the task in that, for example, a large batch of essays on the topic of friendship in a corpus will lead to a disproportionally high frequencies of words and phrases that belong to the lexical field of friendship.\nLexical or constructional bias can also occur when learners use words, phrases or syntactic constructions from the task description, the writing prompt or other input material.\nThis may include patterns that are copied as unanalysed sequences by the learners to complete the task, a 'play-it-safe' strategy without necessarily having acquired the respective structure.\nLexical material in the task instructions or the writing prompt may even trigger the recurrent use of a whole grammatical construction.\nFor example, -Explicit encouragement to include a specific structure (e.g., a gerund) or certain type of structure (e.g., a relative clause) in a specific writing task, leading to a higher frequency of use in that particular task; -A task elicits a certain type of structure implicitly.\nA prompt may elicit a high number of occurrences of a particular structure (e.g., temporal clauses, pronouns) as a natural consequence of language required to meet functional communicative requirements of the task (e.g., past tense narrative) -A task is neutral with regard to the elicitation of a specific structure.\n-Copying directly from input prompt.\nFinally, as Response\nIt is important that researchers detect and control for the effects of lexical and/ or constructional bias but identifying it can be challenging.\nEssay topics, task prompts, and writing instructions should be checked carefully if related and recurrent patterns are noticed in the data.\nWords identified to cause lexical bias are either treated as stopwords and thus excluded from corpus queries, and likewise, L2 structures that are likely to have been brought about by lexical or constructional bias (and thus may have been copied as unanalysed chunks by the writer) are excluded from the analysis (as in \"Discourse of deficit\" and learner corpus annotation\nFinally, I would like to discuss the potential bias of certain annotation methods used in LCR and in other disciplines.\nLCR has been influenced by the \"discourse of deficit\"\nA special kind of annotation in learner corpora is error annotation.\nThis is typically carried out on the basis of an error tagset, which usually derives from error taxonomies based on structural linguistic categories (see, e.g., Creative and innovative but 'non-standard' interlanguage forms are often contact-induced or formed on the basis of semantic or structural analogy to frequent and recurrent patterns identified in the L2 input.\nAn often-discussed example is particle verbs like discuss about, enter into, return back (see Response As for error annotation,\nBy contrast, instances of multilingual language use and lexical innovations should not be annotated as errors.\nBy innovations I refer to forms that are unattested or infrequent in the main reference varieties of British and American English, but products of morphological regularity or creativity in that they are formed by either adapting L1 elements to fit L2 forms, or by recombining L2 elements Summary\nand conclusion\nIn this chapter I have argued that in LCR and SLA the occurrence and contextual use of particular structure(s) in learner data is taken as evidence for the (process of ) acquisition and productive use, hence L2 data must be valid in that they represent language actually produced by the respective learners under study.\nSince texts compiled in learner corpora have been produced by multilingual individuals, learner data are rich in phenomena induced by multilingualism and language con-5.\nIn addition, ICE uses two more tags relevant to the present discussion: <foreign> (for a \"word or sequence of words that is foreign and non-naturalised\") and <indig> (marking words which are \"non-English but are indigenous to the country in which the corpus is being compiled\").\n6.\nIn fact, as ICE components have been compiled over three decades, annotation practices seem to vary from component to component.\ntact which have a hybrid nature and are challenging to classify.\nLearner corpora of academic writing, on the other hand, contain various kinds of metalinguistic language use and language taken over or copied from secondary sources.\nI have suggested that multilingual practices, metalinguistic language use and instances of intertextual reference should be identified and annotated so that they can be dealt with in or be excluded from corpus analysis.\nResearchers also need to pay attention to recurrent patterns in the task instructions, writing prompts and other input material (provided this has been documented!) and if these may induce lexical or constructional bias.\nWhen compiling a new corpus, the task instructions and the input material should be neutral with regard to the elicitation of specific structure(s).\nFinally, annotation and analysis should be separated from normative, oftentimes (over)prescriptive interpretation.\nConceptual and methodological reform and innovation remain a strong desideratum in a young discipline like LCR.\nTo provide a certain incentive for researchers to document and share their methodological practices and tools, and to give researchers engaged in advancing the methodological expertise of the field the recognition they deserve\n, the International Journal of Learner Corpus Research decided to introduce new publication formats (see Issues of standardisation and best practices have frequently been addressed (see, for instance, Gilquin 2015), but when it comes to corpus compilation and annotation, such guidelines for best practice do not seem to be available yet when compared to corpus linguistics in general (see, for instance, Finally, given that LCR is still biased towards written learner corpora that typically contain only the final product of the writing process, it is important to consider that more and more electronic writing tools have become available to help the learner.\nMore recently, the field has seen some initiatives and innovative research that seeks to explore the actual writing process, for example by means of keystroke logging to examine textual revisions Early newspapers as data for corpus linguistics (and Digital Humanities) 1. Introduction\nThe availability of massive text archives holds great promise for corpus linguistic work, but at the same time they also present considerable methodological challenges for users (see, e.g., The recent proliferation of large digital archives as part of Digital Humanities research projects has raised the question of whether they could be readily used as linguistic corpora, which would allow scholars to spend less time on the compilation and pre-processing of data, and more time on actual research.\nWhile this prospect is extremely attractive, anecdotal evidence suggests in many cases this is far less straightforward than what might seem to be the case at the outset (as also noted by Vartiainen & Säily in Chapter 2 in the present volume) and that there are many pitfalls that could limit the effectiveness of this approach, and even undermine the validity of the results that are obtained.\nThe aim of this chapter is to reflect on some of the pitfalls involved in using the British Library Newspapers database as a corpus and consider some ways in which they can be avoided.\nI argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute \"good data\" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021).\nTo use large, digitised archives as data for corpus linguistic research, it is essential to critically think about the ways in which they have been collected and processed, as well as what kinds of software tools are available to search them.\nIn practical terms, this may involve different processes of \"remediation\"\nSection 2 of this chapter provides a brief outline of different approaches to digital text analysis.\nSection 3 identifies three pitfalls specific to the British Library Newspapers database and reviews some possible solutions to them.\nIn Section 4 I summarise the main findings and discuss their implications to the corpus linguistic study of historical newspaper discourse.\nDigital text analysis in the humanities\nThere is a high degree of consensus across humanities disciplines that the emergence of digitised materials and techniques for analysing them is a major advantage for scholarship.\nFor example, in recent Digital Humanities literature, a great deal has been written on the nature of these advantages over \"traditional\" ones.\nThe argument is that owing to the availability of new sources of data, digital approaches have great potential for increased productivity, enabling scholars to automate many of the tasks that previously have taken up a lot of time.\nAnother widely recognised advantage of the proliferation of data is the possibility to obtain new perspectives on old questions which simply would not have been feasible previously, and this is seen as conducive to significant new epistemological advances.\nThese views are nicely captured in the two quotations by\nThe phrase [a telescope for the mind?] is Margaret Masterman's; the question mark is mine.\n[…] She used the phrase to suggest computing's potential to transform our conception of the human world just as in the seventeenth century the optical telescope set in motion a fundamental rethink of our relation to the physical one.\nThe question mark denotes my own and others' anxious interrogation of research in the digital humanities for signs that her vision, or something like it, is (McCarty 2012: 113) being realized or that demonstrable progress has been made.\n(Gale 2023)\nGale Digital Scholar Lab provides a new lens to explore history.\nVery similar discourse is often found in corpus linguistic literature in support of a corpus-based approach.\nFor example, in their widely used textbook Corpus Linguistics: Theory, Method and Practice, It may refine and redefine a range of theories of language.\nIt may also enable us to use theories of language which were at best difficult to explore prior to the development of corpora of suitable size and machines of sufficient power to exploit (McEnery & Hardie 2011: 1) them.\nHowever, to discuss possible pitfalls associated with research at the interface of Digital Humanities and corpus linguistics, it is necessary to briefly review some basic assumptions associated with these respective areas of study.\nDigital Humanities Central to the field of Digital Humanities are computers and their application to the analysis of large masses of text, but beyond that, this broad field can be defined in different ways.\nIn a recent article, Given the breadth of the field of Digital Humanities, sub-disciplinary specialisms naturally exist within the field.\n1. Digitised humanities (research broadly relying on digitized data) 2. Numerical humanities (which focuses specifically on numerical and, more broadly, formal models, e.g., computational social science or social informatics)\n3. Humanities of the digital (i.e., the study of Computer-mediated interaction)\nHe further argues that ( As previously indicated, the role of computing in Digital Humanities is often framed with the help of a visual metaphor as being that of an instrument that enables scholars to get a new perspective on the data that would not be possible through any other means.\nIrrespective of the orientation, it is generally agreed that advantages of a DH approach are said to be epistemological: by making available large amounts of data, DH provides new ways of looking at data and enables scholars to ask new questions and to be more productive.\nCorpus linguistics\nIt is clear that many of the points made in the previous section apply equally well to corpus linguistics, where research is based on large amounts of language data which have been processed and turned into linguistic corpora.\nEven the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data.\nIn fact, the field of corpus linguistics is often conceptualised as being part of the Digital Humanities (e.g., Roth 2018; Zottola 2020; Mehl 2021) despite the fact that it has a longer history.\nIn the modern sense, the term \"corpus linguistics\" became established early in the second half of the 20th century, with such milestones as the publication of the Brown Corpus (1964) and the formation of The International Computer Archive of Modern English (ICAME) in the early 1970s\nHowever, there is one major difference that sets corpus linguistics apart from the Digital Humanities, and this relates to the definition of the term corpus.\nCorpus linguistics typically adopts a narrow definition: a corpus is not just any collection of texts, but a collection that has been selected to represent a language or a sublanguage following an extensional view of language (e.g., Towards a useful synergy\nThe differences between Digital Humanities and corpus linguistics should of course not be overstated, given that the two fields share many research goals and practices.\nFor example, the use of unstructured archives as data is not exclusive to Digital Humanities, but they are also made use of by corpus linguists as \"opportunistic corpora\"\nThe argument that I want to make in this chapter is that in order to take advantage of the synergy between corpus linguistics and Digital Humanities, it is often necessary to critically reflect on the digital materials, how they have been collected, processed, and made available.\nIn particular, it is often the case that the default workflow for accessing a specific archive is not ideal to tackle corpus linguistic research questions, which may make it difficult to compare results to previous work.\n1 To avoid such pitfalls, adjustments are clearly needed, and this often requires carefully considering the influence of register, which is a crucial notion in most types of corpus linguistics.\nIf this is possible -and in particular if the availability of data is not tied to a particular platform or infrastructurewe can improve the quality of opportunistic corpora and ultimately obtain results that are more accurate and are more easily situated within previous research.\nMy aim here is to illustrate this argument by identifying and describing issues that are specific to the British Library Newspapers database, an archive that has clearly not been put together with corpus linguistic research designs in mind, presenting some possible solutions to them, and reflecting on the overall feasibility of these solutions.\n1.\nFor a fuller discussion of replicability and reproducibility specifically in the context of corpus linguistics, see Historical newspaper prose and the British Library Newspapers database The\nTo peruse larger masses of historical newspaper language, scholars can nowadays turn their attention to the British Library Newspapers database, which is a massive digital archive of newspaper writing archive hosted by Gale.\nIt contains around 5.5 million pages from national and regional newspapers from Britain between the 18th and the 20th centuries.\nTo corpus linguists, however, the database also presents multiple issues, which need to be addressed to make full use of it.\nIn the following sections, I will focus on four such issues: 1. Problem with available search tools 2. Sampling, balance, representativeness 3. Register/sub-register considerations 4. Quality of Optical Character Recognition (OCR) Problems with available search tools\nThe British Library Newspapers database can be accessed through the Gale Primary Sources platform,\nTo a corpus linguist, the main shortcoming of Gale Primary Sources is the fact that the searches produce lists of documents, effectively suggesting that the next logical step in the analysis would be to focus on individual texts and their particularities.\nWhile such an \"individual-document approach\" (McEnery & Hardie 2011: 232) might indeed be appropriate for some purposes, corpus linguists would usually be interested in looking at a concordance to identify patterns in the use of the search term or calculate and compare its normalised frequency across different sections of the database.\nWith Primary Sources this is only possible if the documents are individually downloaded and analysed using some other corpus tool.\nTo some extent, these shortcomings are addressed by Gale's Digital Scholar Lab, 5 a text analytics platform through which the British Library Newspapers database can also be accessed.\nIt enables users to build \"content sets\" based on search terms and apply different Digital Humanities tools on them, including document clustering, named entity recognition, n-grams, parts-of-speech, sentiment analysis, and topic modelling.\nOne clear advantage of Digital Scholar Lab over Primary Sources is that it allows entire content sets to be downloaded by one action.\nHowever, it should be noted that the number of documents to download is currently limited to 10,000, which for many sets is insufficient for a comprehensive analysis.\nThe usefulness of the available text mining tools is also seriously limited by their lack of customisability.\nFor example, the part-of-speech tool (using spaCy) only offers visualisations of POS-frequencies but does not give direct access to the tagged texts or include them in the downloadable content sets, which effectively means that the morpho-syntactic annotation provided by Digital Scholar Lab is of no use for corpus research.\nFortunately, there are alternative ways of accessing the textual data.\nThe massive full-text archive of British Library Newspapers is available to subscribing institutions as plain text XML files, and with the help of Octavo, a tool for Digital Humanities research created by Eetu Mäkelä To sum up, as many workflows in corpus linguistics rely on the availability of data as plain text, it is clear that the possibility to access the data in the British Library Newspapers database using Octavo addresses many of the concerns that 5. 〈https\nSampling\n,\nbalance, and representativeness\nWhile a Digital Humanities research project might start by mining the entire digital archive in a bottom-up fashion, this approach is often seen as problematic in corpus linguistics, as lack of structure in the dataset makes it difficult to interpret the findings (see, e.g., Representativeness and balance of data are known to be tricky issues in corpus linguistics in general: while Although the database includes articles from both 18th and 20th century, my focus here is exclusively on the 19th century, which is far more comprehensively represented in terms of the newspaper issues available.\nEven so, it is immediately clear that the distribution of texts is not even: as can be seen in Figure Figure 1.\nOverall distribution of 19C issues\nChoosing the appropriate sampling strategy depends on the aims of the research, and the chosen method should also take into account the contents of the database itself.\nFor example, the most basic sampling method, a simple random sampling, is not sensitive to the increase in available issues and might result in the second half of the 19th century being overrepresented in the sample.\nIf this is deemed problematic, then a stratified sampling strategy, where the database is first divided into smaller, temporally-defined strata (e.g., periods of 1, 5, or 10 years) before extracting the samples, is likely to be a better choice.\nStratified sampling is also more appropriate if the aim is to investigate differences between different newspapers and their linguistic characteristics, both synchronically and diachronically.\nTo achieve this, it is necessary to first identify newspapers like Leeds Mercury, which are sufficiently well-represented in the database and meaningfully cover the entire century, and use them as the strata Registers\nand\nsubregisters\nThe importance accorded to register in contemporary corpus linguistics can hardly be overstated.\nRegister has been established as a major determinant of linguistic variation\nThe database contains potentially relevant information about text categories under two headings: Document type contains information about \"the format, genre, or other characteristics of the document\", and Publication section allows users to limit searches to a specific section of the publication.\nOf these, the latter indicates whether the text belongs to section A, B, etc., and as such is of limited use to register analysis.\nDocument type is much more useful: it categorises each text as belonging to one of the seven types listed in Table With that said, the adoption of the text categories in Table\nBut even if the existing text categorisation is accepted as a basis for corpus compilation, it is still necessary to consider the implications from the perspective of representativeness.\nThe register categories (like newspaper titles in the previous section) are not evenly distributed across the database, and different choices will inevitably result in different representations of the text-external reality, and as before, this information is only accessible to users systematically looking into this.\nThis issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals.\nOut of the 3,475 texts, approximately 88% (3,081) represented a single text category, namely the category News; this is shown in Figure Optical Character Recognition (OCR)\nThe third major pitfall in the context of the British Library Newspapers database is the poor accuracy of Optical Character Recognition (OCR).\nOCR quality is a well-known issue in the Digital Humanities in general, and specifically for digitised newspapers (e.g., (3) The circumstance of Mr. Addington being ■shout to be appointed Speaker of the House ©f Lou!-., lias given rise to a report of the following] changes, which we mention without vouching the ' avi-uracy ef any part of the statement :I\nIt is evident that the accuracy of the text depends crucially on the quality of the image used as the basis of the digitised text.\nContemporary OCR software is able to reach a high accuracy with clean, present-day English text\nIntuitively, the usefulness of digitised texts depends crucially on the overall frequency and distribution of the OCR errors that they contain, and if this is the case, then there indeed appears to be reason for some concern\n: Tanner, Munoz and Hemy Ros (2009) have estimated the average OCR accuracy across the British Library Newspapers database to be 83.6% for characters and 78% for words, and suggest that if word accuracy is higher than 80%, a fuzzy search engine would nonetheless be able to reach a high search accuracy.\nHowever, achieving this across the entire database appears unrealistic, given that this level of word accuracy was only reached by a quarter of the texts in their sample, and this impression is borne out by the trial searches reported by On a more positive note, it has been shown that not all corpus linguistic research tasks are equally sensitive to OCR errors, with for example, the identification of frequent collocations providing robust results even with texts containing relatively large numbers of errors\nTo do this, we can make use of the figures for OCR confidence (0-99.99%), which are available for each text in the British Library Newspapers database and \"[represent] the OCR engine's confidence in the accuracy of the conversion from image to text\".\nThe OCR confidence value corresponding to each text can be conveniently accessed with Octavo, like any other metadata.\nWhile it is unclear how the values for OCR confidence are determined, After evaluating a number of files representing different levels of OCR confidence, it was decided that texts reaching 90% were sufficiently clean to provide accurate research, and this criterion was adopted for Corpus A.\nThis ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks.\nHowever, the disadvantage is that we also discard an enormous amount of potentially interesting lin-7.\nSee 〈https\nTo sum up, removing low-quality texts is essential for accurate corpus linguistic work, and it can be accomplished by filtering out texts that do not meet a pre-determined OCR confidence value.\nThe flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.\nDiscussion\nAfter identifying and reviewing four major pitfalls and suggesting possible ways of avoiding them, it is possible to offer some preliminary conclusions about the usefulness of the British Library Newspapers database for corpus linguistic research.\nStarting with the positives, an important advantage over traditional, relatively small linguistic corpora is that the database enables an exploratory data-driven approach with reasonable effort.\nIn other words, as data extraction can easily be automated, it becomes straightforward to create multiple corpora with the search parameters and frequency thresholds and assess their suitability for different research tasks.\nThe structure of the database also allows researchers to readily incorporate a register perspective into the study design.\nFinally, as it is possible to automatically discard low-quality samples, this workflow also enables the creation of reasonably tidy corpora.\nAs a result, the British Library Newspapers database is an attractive alternative for the corpus-linguistic analysis of historical newspaper prose.\nHowever, there are also obvious caveats to consider.\nCompared to carefully constructed traditional corpora with often hand-picked text samples, there is obviously much less control over individual choices, and the fact that the exact 8.\nIncluding the text in Figure basis of text categorisation is unclear is likewise not optimal.\nYet the by far most serious issue is the presence of errors in the source material, which introduces errors to analyses, and, in the worst case, may compromise the representativeness of corpora.\nHow problematic these issues are depends on the goal of the individual research project.\nAs the database is large, the omission of some publications or volumes due to low OCR quality might not matter too much for the analysis of general trends in language and discourse, whereas it may effectively preclude the study of specific questions or narrower time periods.\nGiven these caveats, corpus linguistics arguably still has a place for \"small and tidy\" Open Corpus Linguistics -or How to overcome common problems in dealing with corpus data by adopting open research practices Stefan Hartmann Heinrich Heine University Düsseldorf\nIn recent years, many researchers have called attention to the fact that research results very often cannot be replicated -a phenomenon that has been called replication crisis.\nThe replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available.\nEspecially in English linguistics, the full versions of many widely used corpora are still behind paywalls, which means that they are not accessible to parts of the global research community, and even when parts of the data are freely accessible, this presents problems for state-of-the-art methods of data analysis.\nIn this paper, I discuss the challenges that have led to this situation and address some possible solutions.\nIn particular, I argue for using smaller but openly available corpora whenever possible and for adopting open research practices as far as possible even when using commercial corpora.\nIntroduction\nIn a seminal paper, More than thirty years later, corpus linguists still struggle with some of the issues that Rissanen has identified.\nBut in recent years, additional issues have emerged.\nPerhaps most importantly for the purposes of the present chapter, the \"replication crisis\" that has permeated various quantitatively oriented disciplines in recent years and decades has also had a significant impact on methodological discussions in (corpus) linguistics The term \"replication crisis\" refers to the observation that many scientific findings have been found to be much less replicable than many believe they should be\nRegardless of the exact approach to replication, it has become clear that the lack of replicability is also a topic in linguistics.\nTo mention only one prominent example from experimental linguistics, a recent multi-lab effort to replicate the seminal study by Sönning and Werner (2021: 1182) mention the following list of problems that have been identified as potential causes of non-replicability: -a lack of transparency in methodology and data analysis, -the non-reproducibility of scholarly work, as, for example, original data and analysis procedures are not accessible, -reluctance to undertake replication studies as purportedly \"unoriginal\" (and unprestigious) despite their potential to put previous findings in perspective, and -concerns about high rates of false-positive findings in the published scientific literature.\nAt first glance, it might seem quite far-fetched to link Rissanen's problems to the issues related to the \"replication crisis\".\nIn this paper, however, I will argue that there are important connections between the different issues mentioned above.\nAnd more importantly, I will argue that the measures that have been proposed to help overcome the replication crisis can also solve Rissanen's problems -at least partly.\nSpecifically, I will make a case for what I call Open Corpus Linguistics.\nThis entails putting into practice principles of open research at various levels and at various (ideally, all) stages of the research process.\nIn a best-case scenario, it involves the open availability of the entire corpus the researcher draws on, as well as sharing of concordances, annotations, and analysis scripts (if applicable).\nThis also helps other researchers to put one's findings into perspective, which may be seen as the main overarching issue underlying Rissanen's problems.\nThe remainder of this contribution is structured as follows: In Section 2, I explicate Rissanen's problems in more detail, relating each of them to specific issues raised in the replication debate.\nIn Section 3, I discuss the main principles of Open Corpus Linguistics, taking potential challenges and pitfalls into account.\nSection 4 concludes the paper by bringing the two strands of the discussion together by showing how Open Corpus Linguistics can contribute to overcome several widely discussed problems in corpus linguistics.\nWhile my focus in this paper is on English corpora, many considerations brought forward here of course apply to corpora of all languages.\nRevisiting Rissanen's problems\nIn many cases, it can therefore make sense to look for alternative corpora that can be considered equally representative for the language the researcher wants to investigate, or even to compile one's own corpus -possibly by drawing on existing (open) corpora and using relevant subcorpora of each corpus.\nThis can also be advantageous with regard to the \"mystery of vanishing reliability\", that is, the phenomenon that each datapoint tends to become less reliable the more parameters (in corpus-linguistic terms, annotations) one adds.\nWe can think about this in terms of a simple spreadsheet: The number of data points (rows) remains the same, the number of columns, however, increases.\nThe problem now is of course not that more parameters are added but rather that the number of \"cells\" (in relation to the number of datapoints) increases and, as such, the potential for error.\nThe obvious solution, then, is not to reduce the number of columns 1 but, ideally, to increase the number of datapoints so that the individual errors weigh in less.\nAs such, the \"mystery of vanishing reliability\" can be reframed in terms of a lack of extensibility: Being in control over the compilation of a corpus allows us to easily extend the database if necessary.\nThe problem, after all, is not so much that each data point becomes less reliable if we add more annotation categories, but rather that we often do not have enough data points to obtain a truly informative picture when addressing research questions that require us to take many categories into account simultaneously.\nBut there is another dimension to extensibility: The reliability of a particular annotation can also \"vanish\" because it turns out to be misguided, for whatever reason.\nFor example, it could turn out that an annotation set used for a corpus is based on false assumptions.\nThus, it is tremendously helpful if a corpus is extensible, that is, existing annotations can be amended or improved and new ones can be added, both by the original creators and by people who reuse the data.\nAs Open Corpus Linguistics: Perspectives and challenges In the previous section, I argued that open research practices can provide (partial) solutions to common corpus-linguistic problems.\nThis raises the question of how exactly these open research principles can and should be put into practice, and which challenges this entails.\nThere exist several standards and guidelines that can provide orientation.\nAfter all, the problems discussed here are not specifically corpus-linguistic ones.\nIn terms of open data, the FAIR guiding principles In an ideal world, then, all linguistic corpora would be available for free in re-usable and interoperable formats under a Creative Commons license.\nIn practice, however, there are some obstacles.\nOne obvious problem is that corpora are usually themselves derivative works in the broadest sense, that is, they draw on existing material.\nAnd in the default case, the existing material is subject to copyright.\nIn the case of, say, newspaper texts, the copyright holders are usually easy to find but hard to convince to make their content available for free; in the case of web data, by contrast, the copyright situation is often unclear, which can make the redistribution of data crawled from the web problematic\nApart from copyright, personality rights can of course also be an issue.\nIn the case of spoken corpora or child language corpora, for example, we are usually dealing with elicited data, requiring the participants' (or their parents') informed consent.\nEspecially in the case of child language data, the recordings can contain sensitive information such as the child's or the parents' name or the place where they live.\nThus, it is important to anonymize or pseudonymize the data.\nBut data crawled from the web can also contain sensitive information.\nGiven that the information was public at the time of crawling, one could make a case that including it in the corpus is unproblematic, but it is quite easy to imagine scenarios in which the publication of data crawled from the web can lead to legally or ethically challenging situations.\nIn some cases, it can therefore be useful to publish a corpus in password-protected form, even though generally, the ideal should of course be maximal accessibility.\nHow should we ideally approach the copyright problem now if we want to follow the principles of Open Corpus Linguistics?\nIf we do not need full texts to address our research questions, the problem is quite negligible.\nIn that case, we can work with concordances, and to the best of my knowledge, nothing usually speaks against sharing concordances via dedicated repositories like OSF 〈https\nIf we need full texts, my suggestion is that we should always consider using an openly available corpus like the BNC or the Open American National Corpus first.\nIn a broader sense, the aforementioned COW can be considered open corpora, too -for legal reasons, they are released under a relatively restrictive license, but they are freely available for academic purposes.\nNaturally, there will be situations in which we cannot use such open corpora because we need more or different data.\nIn such cases, we might have to either fall back on commercial corpora or compile our own corpus.\nThanks to the availability of powerful programming languages such as R or Python, this is easier than ever before; even novice users can quite easily get familiar with a tool like\nAlso, thanks to relatively permissive new legislation at least in (parts of ) the European Union, 4 many data mining activities that used to take place in a legal grey zone are explicitly legal now (see, e.g., Conclusion: Open Corpus Linguistics in practice\nIn the preceding sections, I argued for adopting open research practices in corpus linguistics and examined a number of potential problems that such an endeavor entails.\nIn this section, I discuss how Open Corpus Linguistics can work in practice, and how it contributes to overcoming the pertinent problems addressed in Section 2.\nIn Section 1, I argued that adopting open research practices -and in the ideal case, using openly available corpora -helps us to overcome \"Rissanen's problems\": In the best-case scenario, we can use corpora whose full texts are readily available, which can contribute to overcoming \"the philologist's dilemma\".\nSuch a scenario also provides some flexibility in working with pre-compiled corpora, as we are not at the mercy of the corpus creators with regard to the composition of the corpus.\nInstead, we can work with custom subcorpora or work with custom compilations of subcorpora from different corpora, which can help to solve the problem of \"God's truth fallacy\".\nAnd finally, such an approach ensures replicability and reproducibility, which partly solves the \"mystery of vanishing reliability\".\nThe latter is even true if we work with commercial corpora but make our concordances and analysis scripts available, as is increasingly common in corpus linguistics.\nFor this purpose, platforms like the Open Science Framework (OSF) or the Tromsø Repository of Language and Linguistics (TroLLing) can be used (see the FAQ in Table\nIn the previous section, I addressed some potential problems, many of which are, in my view, not insurmountable.\nSome of them, though, call for creative solutions such as Schäfer and Bildhauer's decision to work with sentence shuffles in the case of COW.\nNeedless to say, it would be desirable to have more legal certainty when using copyright-protected content for linguistic purposes -the aforementioned EU directive might be seen as a promising sign that (some) political stakeholders are aware of the need to reconcile research and copyright interests.\nFor the time being, following the ideals of Open Corpus Linguistics might in some cases require entering legal grey zones.\nThe risk of having to tread uncertain legal ground can, however, be minimized by keeping the question of how the data should be published in mind from the earliest design phase on, and by choosing open corpora wherever possible, as discussed above.\nIn other words, following the principles of Open Corpus Linguistics requires us to invest considerable time in Research Data Management (RDM).\nWhile this can be time-consuming, it is very likely that it will save others and ourselves much time later on.\nOne topic that I have not addressed yet is open-access publication.\nAs one reviewer correctly points out, open research practices and open-access publication ideally go in tandem, even though they can be treated as separate topics.\nMany of the arguments in favor of open research practices mentioned above also apply to open-access publication, ideally in the form of \"gold open access\" (i.e., the final publication is available free of charge) or alternatively in the form of \"green open access\" (i.e., a preprint is published on a pertinent repository; see Eve 2014 for more details and discussion).\nWhile linguistic research questions and the corpus-linguistic scenarios required to address them are too diverse to provide anything like a \"cookbook\" for Open Corpus Linguistics, this chapter has hopefully provided some helpful guidelines, some of which are summarized in the form of Frequently Asked Questions in Table Table 1.\nAnswers to some frequently asked questions about open research practices in corpus linguistics Category\nQuestions and answers\nExisting corpora\nWhich corpora should I use to follow the ideal of Open Corpus Linguistics?\nThe choice of corpus has to be guided, first and foremost, by the research question.\nBut in many cases, there are open alternatives to the widely-used default choices.\nExamples for synchronic English data include the Open American National Corpus for spoken and written American English, as well as the ENCOW corpus for World Englishes as used on the web.\nThe BNC, which is a paradigm example of an open corpus, hardly needs to be mentioned as it is already widely used.\nThere are also a number of multilingual corpora, for example, the COW family of corpora to which the above-mentioned ENCOW belongs, or the WaCky corpora Corpus compilation\nWhich principles should I follow when compiling new corpora?\nIf possible, try to create a corpus that can be published freely under an open license.\nTo do so, it is very important to address legal questions at the very beginning of a project.\nIf you cannot make the full texts freely available for copyright reasons, try to make the corpus as accessible as possible, for example, by allowing queries via flexible search engines such as NoSketchEngine or CQPweb and by publishing word and lemma lists (and ideally, n-gram lists), or by publishing it in password-protected form via a repository that allows for closed-access corpora (e.g., CLARIN).\nData repositories that fit your needs can be found via 〈https Repositories\nWhere can I publish my research data?\nThere are dedicated repositories such as osf.io, zenodo.org, the TroLLing Dataverse ( Table 1. (continued)\nCategory\nQuestions and answers\nCan I publish my paper (draft) along with my data?\nIn most cases, this shouldn't be a problem, even if you submit the paper to a commercial journal.\nThe Sherpa-Romeo database gives a good overview of different journals' and publishers' open-access policies 〈https\nTo what should I pay attention when publishing my research data in repositories?\nMake sure that everything is well-documented and self-explanatory.\n(This is harder than it sounds, which is why I'm not referring to any of my own repositories here as a best-practice example.)\nMake sure that the repository is actually public when your paper is published (OSF, for instance, offers private and public repositories).\nMake sure that your analysis scripts are extensively commented (formats like R Markdown or Jupyter Notebooks invite extensive comments, but plain text scripts can also be used, of course).\nMake use of the possibility to assign a DOI to the dataset(s).\nI would like to share my dataset and analysis scripts with reviewers.\nHow can I do this without compromising the anonymity of peer-review?\nOSF offers \"view-only\" links that you can share with reviewers.\nNote that the nonanonymous repository can easily be retrieved from the view-only link as soon as it is public -if you want to make sure that you remain anonymous, keep it private (the view-only link will still work).\nThis is probably quite common -not only in corpus linguistics but also in other domains, for example, when it comes to publishing programming scripts.\nBut the thing is: Nobody expects us to be perfect.\nEverybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it.\nAs such, nobody will blame you for releasing a corpus that is still more of a raw diamond.\nPublishing your \"raw diamond\" will give other people the opportunity to build on it, or to work with it while you are still continuing to develop it.\n* Thanks to a reviewer for bringing this up!\nTo sum up, Open Corpus Linguistics can be a challenging endeavor, but given the \"replication crisis\", it is a necessary one.\nIn the long term, adopting open research practices can also help us to focus on actual linguistic research questions, rather than spending hours and hours of work on things that other people have done before, without making the results publicly available.\nAdopting open research practices is ethically a good choice, and it is in our own best interest, both as individual researchers and as an empirical discipline. Introduction\nIt is a well-known fact that variation in text length is an unavoidable source of nonuniformity in corpora and can cause issues in quantitative corpus-linguistic analyses.\nAt the very basic level, the confounding effect of variation in text length is obvious.\nSince a longer text by definition contains more words, there are also more opportunities for any given linguistic item or feature to appear.\nIn other words, a longer text will, on average, contain more instances of any item or feature simply because it is longer.\nThis is a problem particularly for text-analytic corpuslinguistic studies, which are interested in comparing how many of these items appear in different types of texts: if two texts have a different number of occurrences of the feature of interest simply because they are of different lengths, it can be very difficult to compare texts of different lengths with each other.\n1 1.\nVariationist corpus linguistics, which focuses on the proportions of variant items or constructions, is not as heavily affected by the issue.\nHowever, even variationist analyses may be affected by the distribution of text lengths in their dataset.\nFortunately, this basic problem has a simple mathematical solution, which commonly forms the basis of typical quantitative corpus-linguistic inquiries.\nThe number of occurrences of the feature of interest in a text can be divided by the number of words\nNormalization is a tried-and-true method of comparing texts of different lengths with each other.\nHowever, while it is a working solution to a huge potential problem in a large number of cases, normalization is not without problems itself.\nOne problem with normalization becomes particularly salient when applying the method to short texts.\nThe problem is based on the mathematical fact that the smaller the divisor becomes, the larger the result of the division will be.\nIn other words, the fewer words there are in a text, the larger the normalized value is.\nThis is of course the very basis on which the normalization method is built to enable comparisons of texts of different lengths.\nHowever, when the divisor becomes very small, the effect gets magnified and the result of the calculation inflates to meaningless levels.\nFor instance, consider a short text of only five words (such as a tweet, a postcard, or a sticky note) which contains one instance of a feature, for example, a single first-person pronoun.\nIf we calculate the normalized frequency of first-person pronouns in this text, we get as the result 200 first-person pronouns per 1,000 words.\nThis is the mathematical solution to the formula, but the result is quite useless in terms of comparing texts of different lengths with each other.\nWhile not every five-word text contains a first-person pronoun, it is also not that unusual if one does.\nBut the same normalized rate of occurrence value also applies to a text of 1,000 words which contains 200 first person pronouns.\nWhile both the five-word text and the 1,000-word text have the same rate of occurrence of first-person pronouns, surely the 1,000-word text with 200 first-person pronouns is much more unusual than the five-word text with one first-person pronoun.\nClearly, these calculated rates of occurrence are not meaningful measures for the comparison of short and longer texts in linguistics.\nIn other words, there are two related problems caused by text length.\nFirst, texts of different lengths cannot be directly compared because they have different numbers of everything simply due to their difference in length.\nI call this the problem of text length.\nSecond, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short.\nI call this the problem of short texts.\nIn order to talk about these problems, in this chapter, I use the words \"long\" and \"short\" to refer to texts which are and are not long enough for typical quantitative corpus-linguistic analysis, respectively, though the line between the two is of course fuzzy.\nTo combat these two problems, a number of solutions and workarounds of varying sophistication have been devised.\nHowever, many of these solutions have problems of their own.\nDespite the ubiquity of the problem, and the oftensuboptimal nature of its solutions, the problem is not that often discussed in much depth.\nIn this chapter, my goal is to bring more attention to the problems of text length and short texts, and to encourage the development and application of new and improved approaches to the problem.\nIn Section 2, I describe how the problem of text length was historically less of an issue but is coming to the forefront with the rise of research into the language of social media.\nI also refer to and summarize results from earlier studies, which suggest that texts of all lengths are of interest.\nIn Section 3, I cover various methods which have been used to either solve or work around the issues caused by the problem of short texts, the problem of text length, and related problems, and discuss their upsides and downsides, as well as suggest best practices and propose potential improvements to these methods.\nFurthermore, I will briefly discuss the related topic of the effect of text length on measures of lexical diversity, which has been studied in more detail.\nFinally, in Section 4, I will conclude this chapter with some final thoughts on the problems caused by text length and their solutions.\nBackground Text length, corpora, and social media\nThe problem of text length and short texts is caused by a simple mathematical relationship, and as such it has been known of since the beginning of quantitative corpus linguistics.\nHowever, historically, the practical problems it has resulted in have arguably been of relatively little actual consequence.\nMost genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media.\nBecause of the comparatively long text length in such genres, the normalization method works reasonably well with them.\nThe reasons for the focus on genres with \"longer\" texts have been manifold.\nA major reason has of course been, and still is, availability of data.\nResearchers have to use whatever data they have access to.\nHistorically, this has largely been published corpora compiled by teams of researchers.\nBut the compilers of such corpora have also been working with the data they can get access to in large enough quantities.\nThese include genres such as newspaper articles, fiction writing, academic papers, and countless others.\nMany of these genres have editorial guidelines or genre conventions which place certain requirements for the length of the piece of writing.\nAnother reason for the focus on longer texts is what has been considered important and influential enough to study.\nThe impact of, for example, newspaper articles, academic writing, casual conversations and personal letters on people, society, and language has been evident to all, and as such it is only natural that texts in such genres are of interest to anyone studying language.\nBut these texts also tend to be long enough for reasonable quantitative corpus-linguistic analysis.\nIt is often easier to overlook the societal and linguistic impact of genres with mainly shorter texts.\nFor instance, personal notes, post cards, and shopping lists are also something written and read regularly, but we might not even think to consider them and other similar genres as research subjects.\nThe question of influence and significance also comes back to the question of availability, since it is of course more difficult to collect a large and representative corpus of post cards or shopping lists than of newspaper articles.\nHowever, it is also, to an extent, a chicken-and-egg situation.\nWith more interest in such genres, it is possible that more such data would be collected into corpora; and with more such corpora available, there might be more interest in such genres.\nA similar vicious circle has also developed when it comes to the development of analysis methods which would allow us to better approach genres with short texts.\nIf genres with longer texts are easier to quantify and the available methods work better with them, it is only natural to focus on such genres; and if the focus typically is on genres with longer texts, there is no particular need to develop methods and approaches which could help analyze genres with shorter texts in more detail.\nHowever, over the past decades, many of these paradigms have been gradually but firmly upended.\nA central catalyst for the change has been the spread of the internet.\nThe web and other means of computer-mediated communication have become easily accessible sources of linguistic data, which has greatly facilitated the building of new corpora and datasets to match the needs of the researcher and to enable the study of entirely new kinds of genres and registers.\nOf course, published corpora are still being compiled to this day, and they are a valuable tool used in a wide variety of linguistic research.\nBut the easy access to textual data online has allowed quantitative corpus linguists to cast a wider net in terms of their research topics than ever before.\nAt the same time, the rise of web and CMC texts has brought many of the issues with text length to the forefront.\nIn contrast to the more \"traditional\" genres which make up many compiled corpora, texts from many internet genres tend to be less bound by word count limits or guidelines.\nWhile many online genres have a highly variable text length and a large proportion of shorter texts, such as blog posts or Wikipedia articles, this is particularly true for computer-mediated communication and social language use on the internet, such as postings on various social media platforms.\nMost social media platforms, such as Facebook or Reddit, do not limit the length of their postings to a meaningful degree.\nPlatforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics.\nFew online platforms require a minimum length for postings or even recommend postings to be of a specific length, whereas such requirements are commonplace within the publishing industry and for many of the genres included in typical published corpora, such as newspaper articles or academic writing.\nAt the same time, online data has brought even the shortest texts to the center stage, making their societal and linguistic importance much more evident in comparison with the more traditional short genres.\nIn other words, the free nature of internet writing has brought texts with a wide variety of lengths into the corpora of many linguists, and consequently made the problem of text length and, particularly, the problem of short texts more central than ever.\nThe importance of text length\nIt is clear that variation in text length, particularly the very shortest texts, causes mathematical issues in quantitative corpus-linguistic analyses.\nBut do we actually have to care about text length?\nCan't we simply ignore the problematic cases when conducting quantitative linguistic studies?\nOr should we work towards finding more ways to make it possible to include texts of all lengths in our analyses?\nIn order to specifically focus on the functional variation taking place across text lengths, This kind of analysis requires a very large dataset, as there need to be enough texts of every length in the dataset for meaningful results.\nConsequently, social media is a good source of data for this method.\nReddit in particular is arguably a very fruitful source of material for quantitative linguistic analyses overall, and especially for the analysis of the effects of variation in text length.\nFirst of all, Reddit enables access to large amounts of publicly available textual data.\nSome other social media platforms, such as Facebook, theoretically also have a lot of data available, but in practice a large portion of it is visible only to one's friends on the platform or those who have joined any specific discussion group.\nIn other cases, the data may be public but difficult to access in large quantities in practice.\nFurthermore, since Reddit is divided into topic-based subforums called subreddits, the data is naturally subdivided into subcategories by topic and by register (see, e.g., The analysis conducted by Liimatta (2022b) performs a similar analysis but zooms in further to focus on a number of popular subreddits to find out whether all subreddits follow similar patterns, or if the same text length can have different functions in different subreddits.\nIn the analysis, most subreddits analyzed are shown to follow similar patterns with each other.\nFor example, the short comments in most subreddits contain more features which are more casual and involved, and longer comments contain more informational features.\nSimilarly, comments of all lengths appear to be roughly equally narrative in all of the subreddits included in the analysis.\nHowever, a handful of the analyzed subreddits, which are more focused in terms of their topic in comparison with the very relative topics of most of the included subreddits, often differ greatly from both the general patterns and from each other.\nFor instance, in the AskReddit subreddit, longer comments are much more narrative than the shorter ones, whereas for some other subreddits the opposite is the case.\nFigure\nThese results show that not only does text length play a role in linguistic variation, but that text length can be associated with functions differently within different register categories.\nSolutions and workarounds\nAs the problems of text length and short texts have been recognized, a number of solutions and workarounds for it have also been devised.\nIn this section, I will cover some of these approaches, and some related methods.\nSome of these approaches help solve or work around the problems caused by variation in text length, some the problem of short texts, and some can help alleviate the effects of both.\nAdditionally, I will describe a closely related problem, that of measures of lexical diversity.\nWhile the solutions to the problems with lexical diversity measures are not directly applicable to the problem of text length, they may still provide inspiration and starting points for new ways of approaching the problem of text length.\nI have divided the solutions and workarounds to the problem of text length and short texts into two main categories.\nIn the first group of approaches, the original set of texts is manipulated in some way, after which standard methods are applied.\nThese could also be considered the more \"traditional\" approaches to the problem.\nTheir advantage is that they are simpler to implement, but this means that their downsides are often greater.\nConversely, the approaches in the second group make use of various statistical and/or computational methods to see the existing data in a new light.\nThese approaches are more complicated to implement and often only work for specific kinds of analyses, but they are much more powerful in the situations for which they are well-suited.\nThese two groups of course overlap in practice, and methods within and between the groups can even be used together.\nManipulation of the data\nExclusion\nA commonly used workaround for the problem of short texts is to simply exclude all texts shorter than some threshold from the analysis.\nFor instance, we might simply choose to remove all texts shorter than, say, 400 words, 500 words, or 1,000 words from our dataset.\nIf the aim is to be able to include as much of the data in our analysis as possible, this approach is at its most reasonable when there are only a small number of outliers under the chosen length limit, as the exclusion of a handful of outliers do not affect the overall results from a good-sized corpus very much.\nIt could even be argued that clear outliers do not even represent the varieties of interest in the corpus particularly well, and that therefore it would even be beneficial to exclude them.\nHowever, the larger the proportion of the texts in the corpus which fall under the chosen length limit, the more problematic the exclusion method becomes.\nParticularly when typical texts from the shorter end of the length range start to be excluded from the analysis in addition to obvious outliers, it is clear that the dataset starts to lose some of the information potentially available within the data.\nOf course, it is not obvious where the line should be drawn when considering whether a text of a certain length should be considered an outlier, but from the point of view of the data, the best practice would be to have the length limit as low as possible.\nThe optimal cutoff length when using the exclusion approach would be low enough that as many texts as possible are included in the analysis, but high enough that the desired analysis is still possible to conduct reliably.\nFor a slightly more statistically-based approach than simply choosing some round number such as 400 or 500 as the limit, it is also possible to define the cutoff point as, for example, the 1% quantile of the length distribution, or whichever percentage gives a length limit which is workable with the chosen methods and the research questions being investigated, since this helps quantify the amount of data which has been left out.\nHowever, there are datasets for which the exclusion approach is utterly unsuitable.\nFor instance, most social media postings are very short, and therefore would need to be excluded from the analysis under any commonly used cutoff length which would allow analysis of the data using typical analysis methods.\nConsequently, different solutions and workarounds to the problems of short texts and variation in text length need to be used when dealing with such data.\nCombining\nIn situations where discarding any data is undesirable, another workaround for the problem of short texts is available.\nIn many studies working with, for example, social media data and other genres which have a relatively large proportion of shorter texts, texts deemed too short to comfortably conduct the intended analysis on are combined to create new \"texts\" which are sufficiently long for the analysis.\nFor instance, we could decide to combine texts so that each of the combined texts is over some length limit, such as 500 or 1,000 words.\nAs with the exclusion approach, the desirable length depends on the methods being used for the analysis and the research questions being investigated.\nThe main upside of the combining method, when compared to the exclusion method, is that no data is completely ignored: all text available for the analysis is included in the analysis.\nHowever, the downside is that by combining texts together, the texts lose their individual nature.\nFor example, if one text is highly edited in style, and another one is highly casual, combining them together results in a loss of a lot of this information, and makes the combined text look somewhat average on both counts.\nIn this way, the combining approach to the problem of short texts may very easily blur out some of the variation in the data.\nOn the other hand, it is also possible that texts may end up combined in such a way that the resulting dataset overstates the importance of some feature which is actually quite rare overall, for instance, if a feature is highly frequent in a small number of texts.\nThe combining approach also easily results in a violation of the \"independence assumption\" inherent in various statistical procedures, including those commonly used by corpus linguists, such as Chi-square testing\nThe issue of blurring out or overstating variation can in some situations be mitigated by the choice of the basis of combination.\nIf the texts which are combined are chosen in an essentially random manner, as is often the case, these potential obscuring effects cannot be reduced.\nHowever, in many cases it is possible to use a more principled basis for the combining.\nIn the simplest case, the texts are combined based on some metadata in which we are interested in our analysis.\nFor instance, if the analysis focuses on texts written by different sociolinguistic groups, any combining of texts needs to be done by the sociolinguistic groups in question.\nThis, however, is done out of necessity, and it does not really help to reduce the blurring of variation taking place within the groups.\nFor example, if we are comparing personal letters and official letters, we of course need to combine the shorter texts separately within the two categories, personal letters and official letters.\nBut even in this case there may be variation within these two categories which gets either blurred out or overstated.\nIn order to lessen the blurring and overstating effect, it might be useful to consider combining texts which are as similar as possible in their production circumstances, as far as reasonably possible.\nWhat texts exactly are considered \"similar\" is however a question which depends on the dataset and research questions.\nAs a rule of thumb, however, the highest number of matching or similar metadata field values might be a good starting point.\nRaising the level of analysis might be considered a special case of the principled metadata-based combining approach.\nFor instance, instead of studying individual classified advertisements, we might consider the entire classified advertisements section a single text for the purposes of our analysis.\nOr instead of focusing on individual social media comments, we might choose to focus on full comment threads.\nThe line between raising the level of analysis and the more general approach of principled metadata-based combining becomes blurred, however, in the case of, for example, combining Twitter tweets with their replies together to form individual texts.\nChunking\nA different, slightly less-used approach to dealing with variation in text length is the opposite of combining shorter texts together: to cut longer texts into shorter pieces of (near) equal length.\nFor instance, Hiltunen and Tyrkkö (2019) make use of this approach when studying Wikipedia articles, which are extremely variable in length, by dividing the articles into 200-word pieces for their analysis.\nWhen using this method, the fact that all texts included in the analysis are of (roughly) the same length facilitates their comparison using feature counts or rates of occurrence, since the confounding effects of variation in text length have been diminished.\nAt the same time, all of the textual information is included in the analysis and not discarded, even if it has been cut into smaller pieces.\nTexts can be split up in various ways.\nA straightforward approach is to simply split a text into chunks of a certain number of words.\nHowever, since sentences are a basic structural unit of language, placing chunk boundaries at sentence boundaries, making sure that every chunk includes enough words, is likely to be a more desirable solution in many cases.\nAnother solution, which keeps the structural and discourse units of a text together even more, is to divide the text into its paragraphs, or multi-paragraph chunks.\nIn addition to the simple chunking options above, chunking can also make use of various computational methods to create chunks which are meaningful in terms of the discourse structure.\nFor example, The chunking approach may or may not help with the problem of short texts.\nIt would be difficult to meaningfully divide the longer texts into chunks of equivalent length if the shortest texts in the dataset are very short, such as on social media.\nOn the other hand, if the shortest texts are still of reasonable length, dividing the longer texts into chunks of similar length might actually make them more easily comparable.\nComputational and statistical approaches Lengthwise analysis\nIn order to make feature frequencies more comparable across text lengths, Liimatta (2020) proposes a family of methods called lengthwise scaling.\nClosely related to the lengthwise analysis described above, this family of methods is also based on the idea that it is trivial to compare texts which are the exact same length.\nIn lengthwise scaling methods, feature counts in each text are first compared against texts of the exact same length (\"intra-length comparison\") using some suitable method of comparison.\nBased on the results of this comparison, each text receives a new, scaled value, which is a representation of how typical the text is in terms of the range of variation seen in all texts of the exact same length.\nThese scaled values can then be compared between text lengths like normalized frequencies would be, such as by visual exploration of graphs or by using some further statistical or computational analysis.\nWhile the idea behind lengthwise scaling can be applied in various ways, Liimatta (2020) demonstrates the method family with two specific implementations, lengthwise rarity scaling and lengthwise quantile scaling.\nIn lengthwise rarity scaling, when computing the scaled value for a feature count, each feature count is compared against the full set of feature counts in texts of the same length.\nEach feature count is then replaced with the percentage of smaller feature counts in texts of the same length.\nIn other words, the following question is asked for each text: \"what percentage of all of the texts of the same length as this text has fewer instances of this feature?\"\nOf the two implementations, lengthwise rarity scaling is noted by Liimatta (2020) to be particularly useful for visual exploration of data.\nThe advantages of this scaling method include the fact that it particularly highlights smaller differences in rates of occurrence within the data, making it easier to pick up on subtler differences between groups of texts, and that it scales the observed variation into a constrained range between 0% and 100%, facilitating the graphing and interpretation of the results.\nFigures\nSince lengthwise quantile scaling is based on the median and certain quantiles of the data, the -1 and 1 lines are particularly useful for the interpretation of the data in terms of recognizing texts with uncharacteristically high or low feature counts for any given text length.\nOn the other hand, in contrast with lengthwise rarity scaling, lengthwise quantile scaling does not confine the scaled values into any particular range.\nIt also does not highlight smaller differences as well as lengthwise rarity scaling.\nHowever, thanks to its basis on common statistical measures, lengthwise quantile scaling may be the better of the two methods to use as a preprocessing step for further statistical or computational analysis.\nThe main downside to both lengthwise rarity scaling and lengthwise quantile scaling is that they require a very large dataset, so that there are enough texts of every individual length to make it possible to compare texts of the same length.\nSuch datasets are most readily based on social media and other online sources.\nHowever, if the dataset mostly contains longer texts, even a slightly smaller dataset will do if texts of adjacent lengths are binned together.\nIf the dataset is even smaller still, and/or includes shorter texts as well, the two methods may not work too well.\nBut these two methods are only two potential implementations of the lengthwise scaling method family.\nIn situations where the dataset is relatively small and includes a large number of shorter texts, other kinds of implementations of the lengthwise scaling method family may work better.\nFor instance, However, even this method is unlikely to work with the smallest corpora and the shortest texts, which do not have enough text for each text range to estimate the population parameters with any reliability.\nMultiple Correspondence Analysis\nThere also exist methods for specific purposes which can be used with shorter texts.\nFor instance, factor analysis methods, such as those used in the multidimensional method of register analysis, rely on feature frequencies, and as such the methodology is difficult to apply to genres which include a large proportion of short texts.\nIn order to get around this issue in their multi-dimensional analyses of Twitter tweets, However, while MCA works well with genres with only short texts, it cannot be used with datasets which include longer texts.\nThis is because the longer a text becomes, the more likely it is to include any given feature.\nAs the texts get longer, more and more of the features of interest start appearing in every text.\nDue to this, the co-occurrence patterns end up saturated when analyzing longer texts, rendering the method unusable with such texts.\nResampling methods\nResampling methods are powerful statistical methods which \"make the best use of the available data\"\nResampling methods have been used in various studies of linguistic variation.\nThey can be used simply to estimate the rate of occurrence together with its confidence intervals, or to enable analysis in situations where using the standard method of normalization is difficult (e.g., A related problem: Lexical diversity While the effects of text length have generally speaking not been studied very much in corpus-linguistic research, there is a group of measures, whose relationship with text length has received some more attention: the type-token ratio and other measures of lexical diversity (or \"lexical richness\").\nWhile the type-token ratio differs as a measure from the typical calculated normalized frequencies, its relationship to text length still bears discussing in this context.\nLike its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens).\nThis ratio is notoriously sensitive to variation in the length of the text it is calculated for.\nDue to this sensitivity, for the results to be comparable, the ratio should be calculated for texts of almost the exact same length.\nHowever, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text.\nWhile this workaround has been used for a long time to good effect, it is also not optimal, since in many cases it excludes a large majority of the text from the calculation.\nThe solution is a lot less optimal still for datasets with a lot of variation in text length, since the 400-word sample covers a different fraction of each text, which means that every text is represented differently by the sampling.\nDue to these problems, and the fact that being able to measure lexical diversity in a meaningful way would be very desirable for many linguistic questions, the question of whether a method which is less sensitive to text length could be devised has received a decent amount of attention from corpus linguists and others.\nThe problem of lexical diversity measures is closely related to the problem of text length and short texts in focus in the present study.\nWhile the efforts to develop a measure of lexical diversity which is less affected by text length do not directly target the problem of text length and short texts, the implication of these efforts is clear: methods which lessen the confounding effects of variation in text length can be developed.\nMaybe some method created for the purpose of measuring lexical diversity could even be adapted to help with the problem of text length in feature frequencies.\nConclusion\nThe present chapter has discussed two related problems, the more general problem of variation in text length and the more specific problem of short texts.\nWhile these problems have not received as much attention than they could have from quantitative corpus linguists (as evidenced by, e.g., the body of research on measures of lexical diversity), the difficulties caused by the confounding effects of text length are only going to become more central to many studies, as more and more research is being done on social media and web data.\nA number of solutions and workarounds to remedy the problems have been devised, all with their own advantages and disadvantages.\nThese solutions can be used to good effect in many kinds of linguistic investigations.\nHowever, there still is no one-size-fits-all solution to the problems caused by text length and short texts in quantitative text-analytic corpus-linguistic studies.\nSome potential avenues for improvements and new method development have been proposed in the present chapter.\nSince resampling methods are very powerful for estimating the distribution based on smaller datasets, they appear as a potentially useful avenue for the development of new methods for the analysis of texts across text lengths.\nAt the same time, larger datasets contain more information about the variation inside them, so various approaches making use of the large size of the data, such as those developed by Even if a perfect all-encompassing solution does not exist yet, or is not possible at all, the solutions mentioned in this chapter can still be used to study many linguistic questions, given that one is aware of the potential implications of their use.\nThere certainly exist many other approaches not mentioned here, particularly various more advanced statistical and computational methods, which are less affected by variation in text length.\nNevertheless, there is still a lot of room left for the development of new ways to analyze datasets with a wide range of text lengths, and particularly datasets which contain extremely short texts, which are more common today than ever.\nIntroduction Fiction is inherently messy to work with.\nThis is not due to the material itself, but rather due to the field that surrounds it and the needs of different theoretical approaches to the reading of the materials.\nSince The concept of special corpora, defined by Tognini-Bonelli (2010: 13) as corpora where the selection is not made to be representative of a language but of a specific use-case, such as the learner corpora available in the International Corpus of Learner English (ICLE)\nAs the interpretation of data begins, further questions regarding the categorizations arise, often to do with genre and style, and one must consider whether the American author active during the 1920s to 1960s was a modernist and so forth.\nConsequently, comparing the fiction of one author to that of any other author matching the temporal and spatial categorization becomes problematic.\nIt becomes more of an issue when presenting to an audience mainly engaged with other aspects of the author's work rather than the \"when and where\" (as attempted in\nAs the intersection between linguistics and literature becomes more popular and more populated with resources, it becomes important to discuss how these uses of corpora as contrastive, or comparative resources beyond language variants and variation could look.\nHow could this new arena influence our categorization habits, and what are the consequences of deeper categorization of fictional texts?\nThis chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields.\nThe chapter aims to contribute towards a more explicit communication of our genre categorization practices, and avoidance of miscommunication and confusion due to the genre term being understood differently within different disciplines and backgrounds.\nLooking up from the pit I have presented papers using reference corpora to distinguish author-specific traits on several occasions (for instance, Ihrmark 2018 and Ihrmark 2019) and regularly received questions specifically about the issue of the comparative aspects.\nOne paper was presented to the Hemingway Society in 2018, and compared sentence lengths, noun distribution and lexical density in Ernest Hemingway's writing, using the COHA fiction category as reference.\nThe reference corpus was further limited to include only materials produced between 1900 and 1960 to correctly match the time period.\nOne line of questioning after the presentation was especially interesting for the current paper: First, was my selection of reference materials appropriate?\nAnd, second, what should be considered appropriate materials for distinguishing features specific to Hemingway?\nRegarding the first question about whether my selection was appropriate or not, I had approached it from an admittedly simplistic perspective.\nMy line of thought had been that the texts belonged within the same category as they were all fiction, and that they were produced during the same time period.\nIn retrospect, the temporal aspect had likely played too large of a role in my justification, and more time should instead have been spent considering the nature of the content.\nWhile the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task.\nPartially this has to do with the sub-categorization of the reference corpus in question, but a more prominent issue is the fluctuating idea of Hemingway's genre belonging throughout his work, such as his short stories\nMoving on to the second question: what should be considered appropriate materials for distinguishing features specific to Hemingway's writing?\nThe way we end up in the pit, to me, begins with what we consider as \"specific to an author's writing\".\nFrom a general standpoint, one could do as I did and simply use fiction in general as the reference, and asking the question of what makes this author stand out amongst a sizeable sample of other authors active during the same period.\nHowever, this leaves the comparison vulnerable for criticisms regarding the oversimplification of fiction, as distinguishing what is specific to an author from what is specific to their peers can be done on many levels.\nContinuing with Hemingway as an example, it could for instance be operationalized as asking what the differences are between Hemingway and other authors belonging in the Lost Generation of post-war American expatriates, other authors who write about war, other authors sharing his journalistic background, or other authors connected to Gertrude Stein or to Sherwood\nThe initial rationale for applying these two categories was that they were easily applied with an acceptably high consensus for a majority of the authors based on other descriptions of the Lost Generation, and that they provided a clear, functional separation of texts.\nThis allows for the corpus to be used for comparisons between the authors short stories and novels, as well as comparisons between different authors considered to belong within the same group.\nHowever, the corpus can only be used for questions based on comparisons within the Lost Generation in a broad sense, and it does not allow the user to perform comparisons along other lines of inquiry, for instance, carrying out comparisons between different genres of fiction.\nSo far, the categorization is fairly intuitive and creates no overlapping tags, that is, a text is never both a short story and a novel.\nFrom a qualitative perspective and using a close-reading method, as is often the case in literature research, the use of overlapping genre categorizations can provide a way of connecting the content and style of the text being read to different movements or periods from which the author might have drawn their inspiration or within which they might have been an active participant.\nThe genre features found in a piece of fiction can also be used to extend an argument regarding the intertextual properties of a text in such a way that the intellectual context of the writing can be connected to the written text.\nAn example of this could be the use of genre tropes to create connection through allusions to other texts or even direct references to previous works that add to the narrative being conveyed, for instance a character coming across a different text within the story and having it influence the narrative.\nGenre plays a role in these examples as it draws on the expected previous reading of the audience, their context, and readers of one genre can more often be expected to be familiar with other texts of the same genre.\n1 This is a very different use-case than that of genre categorization as a sorting mechanism for achieving portioning of large datasets, which would often be the purpose of genre categorization of literary materials or fiction in the distantreading approach commonly used within corpus stylistics or digital humanities\nTurning briefly to the world of fiction in film, combinations of genres have been shown to be viable from a commercial perspective.\nStar the intuitive taxonomy inferred by genre labelling in fiction could be when applying genre as a sorting mechanism.\nSeveral novels are highlighted as examples by Text genre categorization in literature Let us first start by discussing genre as a literary concept.\nTodorov's description emphasizes the influence of conventional choices on the structure of a piece, in this case a sonnet, but the concept of choices made conventional by society is an interesting one for the discussion carried in this chapter.\nSeeing the literary text as a social event adhering to (and understood through) the expectations of the receiving audience does connect well to intertextual ideas of literature, for instance, the work of Barthes proclaiming the death of the author as the birth of the reader Genre being a theoretical minefield does not begin in the 1990s, however, as Levin points out in his 1984 review of Fowler's Kinds of Literature: An Introduction to the Theory of Text genre categorization in linguistics It is important to start out with a distinction between text type and genre, as the terms occasionally fulfill a similar function The use of genres as a sorting mechanism also has practical roots amongst linguists, as Genres have also been defined from a more theoretical perspective within linguistics, some of which see genre as being dependent on the intended social context of meaning-making.\nBased on that conceptualization and the earlier work of Paltridge explores both text type and genre in the classroom setting and indicates a division according to generic structures and text structures, with the former being tied to genre and the latter to text type\nTurning to corpora, text categorization often takes place first at a higher level, where genre can, for instance, be defined as \"Fiction\", as is seen in the widely used COHA or the COCA corpora.\nThe two corpora then provide further granularity by introducing sub-categorization.\nIn the case of COHA, these sub-categories are explicitly tied to the Library of Congress taxonomy of non-fiction and academic materials (Figure\nThe genre category pitfall\nTo approach the potential genre category pitfall, this paper will first consider the position of corpus linguists and literature scholars based on the previous sections.\nIn general, their positions on genre could be described to as defined by their relationship to the text as an object.\nThe components highlighted as interesting from a linguistic perspective are features such as style, intended readership, intended function and the context in which the text is intended to fulfill a communicative purpose.\nThe different purposes of the text are also of interest, especially when seen in relation to the other features.\nThe position could be oversimplified as being one that is interested in the purpose and form of a text, and often focused on the intersection between the two.\nThe occasional overlap between the idea of a genre and that of a text type serves to further muddy the waters in linguistic usage of the genre term.\nThe perspective afforded by the field of literature, on the other hand, seems more interested in the intersection between style and content.\nStyle is close to the idea of form as used in the definitions from the linguistics side of things, but it brings with it quite different connotations (see Aquilina 2014 for a thorough discussion).\nWhile style in linguistics can be discussed as having to do with formality, register and fitness-for-purpose, style in literature takes on an intertextual meaning connected to the idea of a genre.\nIn addition, genres are often defined by their content to a higher degree, as the setting is used for genre descriptions in genres such as science fiction or westerns, whereas style and plot can play a larger defining role in genres such as horror or fantasy.\nReturning to the Star Wars dilemma, or the issue of expectations tied to the genre term amongst different audiences raised by Genres as a mode of categorization in corpus resource creation vis-à-vis use as descriptive tags used for fiction should also be considered an issue in the same pit, as the expected rules governing the use are different in nature.\nOverlapping genre descriptions in literature or fiction are not considered an issue.\nIn fact, the combination of genre tropes and features within a piece can often be seen as a strength, as indicated by Turning from the issues to do with the positions of the audiences to the practice-oriented issues of the intersection between corpus linguistics and literature, there must first be an understanding of which tasks are performed therein.\nHowever, the projects in which these tasks are performed often deal with determining what is especially characteristic for a specific author or a specific text\nFrom a distant-reading linguistic perspective, the categorizations relying on text-internal features, such as the one presented by\nFrom a close-reading literature perspective, the resulting category \"Novels\" becomes too broad to be useful.\nIt does not influence the close-reading method in a meaningful way and does not indicate information that could tie the text(s) to a broader intellectual context.\nInstead, a more useful taxonomy would be tied to fiction genres, allowing the researcher to connect the texts to their contexts and content more clearly.\n3.\nAs pointed out by one of the reviewers, this could also lead to circularity if the texts are later explored for linguistic features tied to the genre.\nFor example, if the genre \"Reports\" is defined by text-internal features, those features would be very frequent within the resulting category.\nFor those stuck between linguistics and literature, it becomes a decision regarding the intended audience of the corpus.\nIn the case of a resource created for the linguistics community it would make sense to include both the text type and the broader genre, as both have a theoretical background through which results could be connected to previous and future research, as well as inform about the nature of the item in an objective way.\nIt is also important to consider which methods are going to be applied, as the methods relying on comparisons exemplified by Returning to my own pit for a moment, the use of the \"Novel\" and \"Short story\" categorization in the Lost Generation Corpus starts to appear more palatable to me than it was earlier.\nWhile very broad categories, they still provide some clear idea of their content in terms of it being fiction of a certain length, albeit with a large helping of internal variation as exemplified by Conclusion\nThe main conclusion drawn is that fiction is difficult to categorize according to genre for corpus compilation, and especially so when trying to communicate useful information to two distinct fields of research with complex theoretical backgrounds connected to the term.\nIn practice, this means that work taking place at the intersection between literature and linguistics must be explicit about how the term \"genre\" is being used, and what exactly is meant by it.\nThe previous research referred to in this chapter could be said to highlight the communication taking place implicitly through the use of the term \"genre\" as the main culprit through its setting of different expectations depending on the audience.\nHowever, the connection between the approach to genre labelling and the methods being applied provides a concrete bridge across that rift.\nConsidering genre categorization of fiction as a part of the methodology in a study could open up for both an explicit argumentation regarding why one has decided on the labelling being implemented, as well as a clear communication to the reader about what is actually intended by the labels.\nMoving the act of genre labelling for increased granularity to the methodologies of individual studies according to their specific needs, the broader categorizations make a lot of sense due to the corpora then having a wider applicability.\nIn summary, Modeling fine-grained sociolinguistic variation The promises and pitfalls of Twitter corpora and neural word embeddings Stuttgart\nThis chapter examines the use of recent data sources and computational methods to study fine-grained sociolinguistic phenomena.\nWe deploy a custom-built corpus of tweets Introduction\nIn this chapter, we deploy a novel corpus-based approach to the study of a complex type of language variation.\nOur focus is on contact-induced semantic shifts in Quebec English, that is, preexisting English words used with a meaning typical of a similar French word.\nConsider the following example taken from a tweet posted by a speaker from Montreal:\n(1) I really want to go to an art museum or an art exposition.\nHere, the word exposition refers to what is usually known as an art exhibition.\nThis meaning is not conventionally used in English; it is instead associated with the homographic French word exposition.\nThis phenomenon is explained by the local sociolinguistic context: Quebec is the only predominantly French-speaking Canadian province.\nAs of 2021, 74.8% of its inhabitants -close to 6.3 million people -report that their mother tongue is French.\nTen times fewer Quebecers -7.6% of the population, or just under 640,000 individuals -are native speakers of English\nOur analyses rely on a particular type of linguistic data -a large, custombuilt corpus of tweets -as well as a recent computational approach to modeling lexical semantic phenomena -neural word embeddings.\nLarge-scale computational studies of language variation increasingly rely on both of these methodological choices\nMore specifically, we analyze patterns of regional variation in our corpus of tweets; drawing on underlying demographic distinctions, we aim to isolate instances of contact-induced semantic shifts and further characterize their use.\nFor a given lexical item, our method produces computational representations of its occurrences in the corpus and splits them into groups based on semantic similarity, prioritizing those that are expected to reflect the influence of French.\nDespite extensive data filtering and a carefully adapted implementation of a recent neural language model, our approach highlights not only contact-induced semantic shifts, but also a range of noise-related phenomena.\nWhile this means that it cannot provide reliable results in an unsupervised manner, we show that a coarse manual annotation -conducted on the automatically identified clusters of tweets rather than individual occurrences -provides an efficient way of eliminating false positives.\nComplementing an earlier analysis of the technical impact of these issues The remainder of this chapter is organized as follows.\nWe first introduce a summary of related work (Section 2) and a more detailed description of the data and methods we deployed (Section 3).\nWe then present the key results of our analysis (Section 4) and conclude with a discussion and main takeaways (Section 5).\nTheoretical and methodological background\nIn this section, we contextualize our work with respect to research on semantic shifts in Quebec English, which represent our descriptive focus.\nWe further discuss our key methodological choices, thus addressing the use of Twitter corpora and neural word embeddings.\nSemantic shifts in Quebec English: The need for corpus studies\nThe use of English in Quebec is influenced by ongoing contact with French, particularly on the lexical level.\nEvidence for this claim comes from sociolinguistic studies (e.g., These observations, however, are mainly based on studies of French loanwords; descriptions of other types of contact-related lexical influence are considerably more limited.\nThis is particularly the case for the previously mentioned issue of semantic shifts, on which we focus in this paper.\nWe understand this phenomenon as the presence of a sense in a preexisting English word that is explained by the presence of the equivalent sense in a formally and/or semantically similar French word.\nWe know from Most of these studies rely on traditional sociolinguistic methods, which involve recording the speech production of carefully sampled speakers using face-to-face interviews Twitter-based corpora for language variation\nThe development of social media has enabled large-scale analyses of language variation relying on publicly available posts from these websites.\nThis is particularly true of Twitter, a social network created in 2006, where users can post 280-character messages known as tweets.\nThe sheer amount of data available on Twitter is routinely presented as a key advantage compared to traditional sociolinguistic studies.\nHowever, this is counterbalanced by issues such as a lack of reliable demographic informationa mainstay of sociolinguistic research -as well as sources of bias inherent to the platform, affecting key information such as user location Vector space models for lexical semantic variation As previously suggested, the persistent challenges in systematic analyses of lexical semantic variation -in sociolinguistic studies in general Methods such as these constitute the cornerstone of recent computational approaches to semantic change.\nStarting from a diachronic corpus, different VSMs have been used to quantify the change in meaning of all words in a corpus (or any subset of them) over time (see However, most of these studies focus on computational issues.\nExisting descriptive applications include assessing longstanding hypotheses on semantic change\nData and method\nOur approach relies on contrasting synchronic data from different Canadian regions, under the assumption that linguistic behaviors that are specific to Quebec but absent from areas where the use of French is limited, are likely to reflect the influence of language contact.\nThis is inspired by the comparative sociolinguistic approach\nA corpus of tweets We use a previously created corpus of Canadian English tweets published by users from Montreal, Toronto, and Vancouver\nThe data were collected from January to November 2019.\nWe initially used Twitter's Search API to look up tweets tagged as written in English and associated with the geographic area of one of the target cities.\nThe users identified in this way were narrowed down to those whose free-text profile location strictly corresponded to one of the three target cities.\nWe then crawled their profiles, collecting up to 3,200 most recent tweets per user; this allowed us to increase the amount of data and obtain basic sociolinguistic information.\nFor instance, we stored the distribution of language tags in the users' tweet production and subsequently used it as a rough estimate of their degree of bilingualism.\nFinally, we only retained the tweets tagged by Twitter as written in English, and we automatically removed near-duplicates posted by individual users.\nExploratory analyses have shown that the retained data are both specific to the target cities and comparable across them (for more details, see Miletić et al. 2020).\n3\nIn addition to the preprocessing decisions applied to the original corpus, we introduced additional filtering for the experiments presented in this paper.\nFirst, we removed the content posted before 2016 in order to limit the likelihood of picking up diachronic effects; the tweets in the original corpus date back to 2006.\nIn determining the cut-off point, our aim was to find a reasonable tradeoff between a reduction in time span and the remaining amount of data.\nWe then 3.\nIn accordance with Twitter's developer terms, the corpus is released in the form of tweet IDs, which can be used with off-the-shelf software to collect the underlying data: 〈http\nIn identifying the semantic shifts, we relied on descriptions provided in the literature on Quebec English ically identify occurrences used in similar contexts and quantify sense distributions, focusing on both regional and user-level patterns.\nNeural word embeddings\nFor each of the 40 lexical items from Section 3.2, we first produced word embeddings for their individual occurrences.\nEach corresponds to a slightly different vector, which incorporates general distributional information captured during model pretraining and is further informed by the target item's immediate linguistic context.\nWe then used these representations to automatically group the occurrences into clusters, which were expected to reflect similar contexts (and thereby similar uses of the target lexical item).\nThis allows for a more efficient subsequent analysis of the full range of uses exhibited by a lexical item: for instance, the fact that similar occurrences are grouped together means that it is not necessary to disambiguate them one at a time.\nWord embeddings were produced using the previously discussed BERT model, and specifically the Hugging Face implementation\nFor each analyzed lexical item, we extracted the tweets in which it appears in all three regional subcorpora.\nIn order to limit processing and memory requirements, we retained no more than 1,000 total occurrences per word and used a random sample for more frequent items.\nWe fed each tweet in its raw text form as a single sequence into BERT, which then produced context-informed vectors for each token in the tweet.\nThe model outputs multiple vector representations per token, each corresponding to a different hidden layer in the neural network architecture.\nSimilarly to other recent studies (e.g., Clustering and annotating the uses of a lexical item Similar uses of a lexical item were automatically identified by clustering its tokenlevel vectors using affinity propagation, an algorithm which performed well in other semantic change studies (e.g., In analyzing the output of the analysis, we considered the clusters containing at least five tweets, and retained them if more than half of the tweets were from the Montreal subcorpus.\nThis is because of the focus on the uses which are clearly 7.\nFigure inspired by Jay Alammar's illustrations available at 〈http\nMore specifically, a target item's use in a cluster was annotated as contactrelated if it was regionally specific to Montreal and potentially explained by the influence of a formally and/or semantically related French word.\nThis determination relied on the same evidence used to select the target set of semantic shifts, that is, previous sociolinguistic studies and lexicographic sources (see Section 3.2).\nRecurrent phenomena that were not annotated as contact-related included a range of noise-related issues; these will be discussed in more detail below.\nA 15-word sample (91 clusters) was annotated by two annotators in order to test the reliability of the general procedure, obtaining a reasonably high interannotator agreement (Cohen's kappa coefficient of 0.55).\nOn average, 8 clusters per word (min = 3, max = 10) were retained for annotation.\nThe mean number of tweets per cluster stands at 13 (averaged over the means for individual lexical items; min = 8, max = 20).\nAs shown by the examples discussed below, the clusters are largely homogeneous; although some are occasionally difficult to interpret, this is overall rare.\nOur analysis also allows for a degree of uncertainty, as the annotation targets the predominant use in a cluster.\nThe utility of this approach is confirmed by the fact that it led to the identification of at least one contact-related cluster for each of the 40 target items.\nFrom a practical standpoint, using cluster-level annotations was an order of magnitude faster than analyzing individual tweets.\nThis is due to the lower number of required decisions and the comparative ease in determining the meaning of a larger number of similar examples appearing together.\nResults\nThis section discusses the results derived from the annotated data.\nIt first presents a general overview of cluster types across lexical items; it then illustrates a range of true and false positives observed in the data; and it concludes with a case study examining the link of contact-induced semantic shifts with bilingualism.\nAn overview of regionally specific clusters A global overview of the analysis (Figure Types of variation captured by the analysis\nThis section discusses examples of tweets extracted from our corpus using the clustering analysis described above.\nSample clusters of tweets are presented in the keyword-in-context format, for ease of reading as well as to illustrate the effect of this approach on manual perusal of corpus data, as observed during the manual annotation.\nEach sample cluster contains three representative tweets published in Montreal and occurring in a single original cluster output by our analysis.\nFurther information on the size and regional composition of the clusters is also provided.\nIn order to protect the privacy of tweet authors, we only reproduce textual content without any metadata.\nFor the same reason, usernames, hashtags, URLs, and names of individuals are redacted from the tweets, except for widely known public figures or if necessary to interpret the meaning of the tweet.\nTrue positives\nWe begin by examining positive contributions of our computational system, focusing on the help it provided in distinguishing between conventional and contact-related uses based on documented patterns from the corpus.\nThis was beneficial across different semantic mechanisms and degrees of granularity of contact-related influence.\nA clear-cut distinction Perhaps the prototypical mechanism underlying contact-induced semantic shifts involves using an English lexical item to denote a referent conventionally designated by a formally similar French lexical item.\nOne such example is manifestation, which is generally used to signify 'a display of the existence of something' , but is also attested in Quebec English with the sense of 'protest, demonstration' , typical of the homographic French lexical item manifestation.\nThis sense is absent from the Canadian Oxford Dictionary (COD), but it is anecdotally reported by False positives\nWe have so far focused on the informativeness of our semi-automated analysis in understanding often fine-grained patterns of contact-related language use, but this process was complicated by different types of false positives.\nThis section provides a detailed analysis of the most frequent patterns that we encountered.\nWe distinguish between the following types of locally-specific word usage which do not constitute contact-induced semantic shifts: -cultural effects, where word usage is related to the local cultural context of Montreal; -the use of common nouns as proper names denoting locally specific referents; -French homographs of English words, attested in codeswitched tweets; -structural patterns, such as the position of the target item across tweets, which accidentally affect model performance.\nFor each category that we discuss, sample clusters of tweets are provided in order to illustrate the contrast between the contact-related use -the target of our analysis -and the noise that we identified along the way.\nCultural effects\nThe regionally specific character of some clusters output by our analysis is not related to the use of the target lexical items with a French-related sense, but rather to the local cultural context of Montreal.\nTake for example formation, whose English senses include 'the action or process of forming' and 'arrangement or disposition' .\nIn our data, it is also attested with the sense of 'course, training program' , typical of the French homograph formation.\nThis sense is absent from the COD and the OED, but its existence is noted in the sociolinguistic literature\nA different but related effect was observed in the case of animator.\nThis lexical item is generally used with the sense of 'creator of animated films' , whereas the formally similar French equivalent animateur also includes the sense 'group leader; organizer; facilitator' .\nThe use of animator with the former senses is attested in the sociolinguistic literature\nThe goalie formation is solid .\neven if Price gets injured , Montoya the contact-related use, but these were limited to a single cluster\n; the remaining eight regionally-specific clusters reflected the conventional sense (Table Proper names\nThe regional specificity of some clusters is explained by the target lexical item being used as a proper name, generally to denote a regionally-specific referent.\nTake for example deception, which in English refers to 'the action of misleading someone' , but whose French homograph also means 'disappointment' .\nThis use is not recorded in the OED or the COD, nor is it described in the sociolinguistic literature we reviewed.\nDeception Bay , the title track from @milknbone's\nThe new song Deception Bay , from Milk & Bone's second album , is Deception Bay on repeat !!\nCan't wait for the whole French homographs in codeswitched tweets\nThe performance of our computational system is occasionally affected by crosslingual homographs of the target lexical items.\nThey are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus.\nCodeswitching is overall rare in our corpus, but its relative frequency is considerably higher in Montreal, as can be expected given the prevalence of bilingual speakers in the city.\nThe practical implications are illustrated by the case of souvenir.\nOur analysis focused on the conventional English sense 'keepsake, memento' and the potential presence of the more abstract sense 'memory' , typical of the formally identical French equivalent.\nThe use of the English lexical item with the French-associated sense is not recorded in the COD.\nIt is however attested in the OED, though only as \"chiefly literary\", as well as in the sociolinguistic literature\nStructural patterns affecting model performance\nA final recurrent issue is that of clusters where tweets appear to be grouped together based solely on structural regularities.\nThis was observed in the case of trio, which conventionally means 'a group of three' , but is also used with the Francois Fournier a partagé un souvenir .\n1 h •\n7 years ago , i played my third gig with Old memories .\nVery old .\nOh , les vieux souvenirs!\n<url> sense of its Quebec French homograph, denoting a 'sandwich-fries-soda special, combo' .\nThis specific use is not attested in the lexicographic sources we consulted, but it is described in the sociolinguistic literature Deploying coarsely annotated data for linguistic description\nThe structure of the clusters output by our analysis shows that lexical items differ in terms of the diffusion of contact-related usage (how many tweets are related to contact, out of all those retained in the regionally-specific clusters) as well as its regional specificity (how many tweets in contact-related clusters come from Montreal).\nThese patterns may be indicative of different degrees and factors of diffusion of semantic shifts within the local speech community.\nTo explore the descriptive relevance of this information, we calculated scores reflecting the two points raised above for each of the 40 manually annotated lexical items: a diffusion score, corresponding to the proportion of tweets tagged as contact-related, out of all manually annotated tweets; and a regionality score, corresponding to the proportion of tweets posted in Montreal, out of all tweets tagged as contact-related.\nIn order to explore the potential impact of the degree of bilingualism on the use of semantic shifts, for each lexical item we also calculated a bilingualism score, corresponding to the mean proportion of tweets in English (out of tweets in English in French) posted by users who used the contact-related sense in the clusters tagged as such.\nIt ranges from 0 for users tweeting only in French to 1 for users tweeting only in English, with intermediate values indicating a production of tweets in both languages.\nWe first checked the relationship between the three scores by calculating Spearman's rank correlation coefficient.\nThe diffusion score is uncorrelated with both the regionality score (ρ = -0.13, p = 0.42) and the bilingualism score (ρ = 0.02, p = 0.90).\nHowever, the regionality and bilingualism scores exhibit a moderate negative correlation (ρ = -0.53, p < 0.001); this link is explored in more detail in Figure\nThe plotted results indicate that contact-related semantic shifts which are more regionally-specific (i.e., attested in Montreal to a higher extent) are also more directly related to the effects of bilingualism (i.e., a lower proportion of English, and hence a higher proportion of French, tweets).\nA typical example (bottom right) is the case of circulation, attested in the Quebec English data with the sense of 'traffic' , which is associated with the corresponding French homograph.\nAll of the tweets from clusters tagged as contact-related come from Montreal; moreover, the mean proportion of English tweets stands at 0.75 per user.\nThis may appear to be a relatively high value, but it is in fact just above the 10th percentile for all users in the corpus (0.73); at least within this dataset, this is suggestive of a comparatively and the bilingualism score (y-axis).\nDotted lines show the 10th and 20th percentile for the bilingualism score, for all users in the corpus important influence of bilingualism.\nPatterns at the other end of the spectrum (upper left) are illustrated by the verb remark; we focused on the sense 'notice' , with which the French verb remarquer is widely used.\nIt is less regionally-specific (62% of contact-related tweets posted in Montreal) and less strongly associated with bilingualism (higher mean proportion of English tweets per user, at 0.99).\nUnlike in the previous example, however, the contact-related sense is attested in dictionaries, but the OED marks it as rare in some syntactic contexts.\nWhile it is likely accessible to most English speakers, cross-linguistic influence might facilitate its wider use; this scenario is consistent with our data.\nIt is also relevant to look at the outliers from the general trend.\nFor instance, in the previously mentioned case of trio 'sandwich-fries-soda special, combo' (upper right in the plot above) all contact-related tweets similarly come from Montreal.\nHowever, the mean proportion of English tweets is higher, at 0.99 per user.\nThis is indicative of a use which is regionally-specific, but is widespread in the local linguistic community, including among monolingual speakers.\nThis is further sup-ported by existing descriptions which have shown it to be typical of the speech of native English-speaking Quebecers\nThese observations indicate that, barring some exceptions, the more region specific the contact-related use is, the more strongly it is associated with use of French.\nOnce again, it is important to note that the manual annotation was conducted on the level of clusters, rather than individual tweets, meaning that some non-contact-related occurrences may have been included in the counts.\nMoreover, the information on the use of French has the benefit of being empirically grounded in the attested use of languages by individual Twitter users, but it is only a very rough approximation of their linguistic profiles; for instance, there is no reliable way to determine their native language.\nThat said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors.\nIt also constituted the basis of a face-to-face sociolinguistic survey we conducted in January 2022, whose initial results confirm the overall relevance of our approach, but also highlight the distinct -and complementary -nature of corpus-based and in-person estimates of semantic change\nDiscussion and conclusion\nWe have presented an analysis of a fine-grained sociolinguistic phenomenoncontact-induced semantic shifts in Quebec English -using a large, custom-built corpus of tweets and a recent pretrained language model relying on a deep neural network architecture.\nThis approach has paved the way for a more detailed account of previously reported semantic shifts, contributing extensive empirical evidence where original descriptions often consisted in a single anecdotal mention of a lexical item of interest; our approach was also beneficial in more comprehensively characterizing previously undescribed semantic shifts, initially observed in isolated tweets.\nThe computational tools we used facilitated manual inspection of vast amounts of data, directing our attention to the most relevant subsets of occurrences; they also enabled broad quantitative estimates of the use of semantic shifts, highlighting possible interpretations and informing the design of subsequent studies.\nThe results have also broadly confirmed our highlevel assumption that regional variation in synchrony can be used as a proxy for detecting contact-induced phenomena.\nMore generally, the overall setup -data extraction, clustering based on semantic similarity, and analysis of the distribution of occurrences over an explanatory factor -can be generalized to other descriptive issues.\nHowever, we cannot gloss over the fact that our computational system provided actionable results only once it was complemented with extensive manual analyses.\nThe challenges that we encountered are related to several distinct issues: (i) a strong assumption on regional variation underpinning the methodological design -while some language use specific to Montreal is related to language contact, not all is; (ii) inherent limitations of the methods we used, with BERT occasionally capturing phenomena unrelated to lexical semantics; (iii) inherent limitations of the data we used, with a carefully filtered Twitter corpus representing an improvement on highly generic datasets, but still suffering from the 280-character limit and the limited ability to validate user descriptions, among other issues; (iv) the complexity of the phenomenon under study, which often involves very subtle -but nevertheless perceptible and socially meaningful -differences in language use.\nSome of the described false positives, such as French codeswitching and referents typical of Montreal, are specific to our corpus; however, they echo the observation that semantic change models capture different types of variation in word usage, also raised in other recent studies\nDespite these challenges, data-intensive computational approaches to lexical semantic phenomena, and to language variation in general, have an important role to play in descriptive linguistic research.\nThey can provide meaningful quantitative accounts of lexical phenomena, including for the whole vocabulary, based on data obtained in an unobtrusive way; this is clearly complementary to traditional sociolinguistic methods.\nWhile methods such as those we implemented still require adaptations to the task at hand as well as some manual analysis, they simplify the tasks required of the linguist.\nOne example of this approach is our analysis based on coarse cluster-level annotations; its relevance is confirmed by the fact that, together with the results presented in Funding\nThe computational analyses presented in this paper were carried out using the OSIRIM computing platform, administered by the IRIT research laboratory and supported by the CNRS, the Région Occitanie, the French Government and the European Regional Development Fund (see 〈https D M\n",
        "entities": [
            [
                98,
                113,
                "TERMINO"
            ],
            [
                276,
                291,
                "TERMINO"
            ],
            [
                490,
                505,
                "TERMINO"
            ],
            [
                1102,
                1117,
                "TERMINO"
            ],
            [
                1719,
                1734,
                "TERMINO"
            ],
            [
                1910,
                1925,
                "TERMINO"
            ],
            [
                2789,
                2800,
                "TERMINO"
            ],
            [
                3085,
                3096,
                "TERMINO"
            ],
            [
                3175,
                3190,
                "TERMINO"
            ],
            [
                3791,
                3805,
                "TERMINO"
            ],
            [
                3999,
                4014,
                "TERMINO"
            ],
            [
                4576,
                4594,
                "TERMINO"
            ],
            [
                5346,
                5363,
                "TERMINO"
            ],
            [
                6269,
                6284,
                "TERMINO"
            ],
            [
                6491,
                6505,
                "TERMINO"
            ],
            [
                6612,
                6627,
                "TERMINO"
            ],
            [
                6910,
                6922,
                "TERMINO"
            ],
            [
                7031,
                7046,
                "TERMINO"
            ],
            [
                7730,
                7744,
                "TERMINO"
            ],
            [
                8101,
                8116,
                "TERMINO"
            ],
            [
                8385,
                8400,
                "TERMINO"
            ],
            [
                8779,
                8794,
                "TERMINO"
            ],
            [
                8805,
                8820,
                "TERMINO"
            ],
            [
                9419,
                9434,
                "TERMINO"
            ],
            [
                9640,
                9652,
                "TERMINO"
            ],
            [
                10238,
                10249,
                "TERMINO"
            ],
            [
                10361,
                10376,
                "TERMINO"
            ],
            [
                10574,
                10585,
                "TERMINO"
            ],
            [
                11116,
                11127,
                "TERMINO"
            ],
            [
                11222,
                11240,
                "TERMINO"
            ],
            [
                11586,
                11600,
                "TERMINO"
            ],
            [
                11758,
                11773,
                "TERMINO"
            ],
            [
                12038,
                12054,
                "TERMINO"
            ],
            [
                12349,
                12364,
                "TERMINO"
            ],
            [
                12485,
                12512,
                "TERMINO"
            ],
            [
                12882,
                12897,
                "TERMINO"
            ],
            [
                13269,
                13284,
                "TERMINO"
            ],
            [
                13447,
                13462,
                "TERMINO"
            ],
            [
                13935,
                13950,
                "TERMINO"
            ],
            [
                14121,
                14136,
                "TERMINO"
            ],
            [
                14373,
                14388,
                "TERMINO"
            ],
            [
                15496,
                15511,
                "TERMINO"
            ],
            [
                16804,
                16818,
                "TERMINO"
            ],
            [
                16826,
                16836,
                "TERMINO"
            ],
            [
                16989,
                17003,
                "TERMINO"
            ],
            [
                17289,
                17299,
                "TERMINO"
            ],
            [
                18673,
                18683,
                "TERMINO"
            ],
            [
                18720,
                18737,
                "TERMINO"
            ],
            [
                18765,
                18775,
                "TERMINO"
            ],
            [
                18888,
                18903,
                "TERMINO"
            ],
            [
                19132,
                19142,
                "TERMINO"
            ],
            [
                19346,
                19353,
                "TERMINO"
            ],
            [
                19482,
                19499,
                "TERMINO"
            ],
            [
                19847,
                19857,
                "TERMINO"
            ],
            [
                20063,
                20074,
                "TERMINO"
            ],
            [
                20228,
                20243,
                "TERMINO"
            ],
            [
                20326,
                20343,
                "TERMINO"
            ],
            [
                20629,
                20646,
                "TERMINO"
            ],
            [
                23070,
                23086,
                "TERMINO"
            ],
            [
                28338,
                28353,
                "TERMINO"
            ],
            [
                31802,
                31816,
                "TERMINO"
            ],
            [
                33111,
                33126,
                "TERMINO"
            ],
            [
                33404,
                33419,
                "TERMINO"
            ],
            [
                33767,
                33782,
                "TERMINO"
            ],
            [
                34152,
                34168,
                "TERMINO"
            ],
            [
                34568,
                34583,
                "TERMINO"
            ],
            [
                35111,
                35118,
                "TERMINO"
            ],
            [
                35221,
                35231,
                "TERMINO"
            ],
            [
                36233,
                36248,
                "TERMINO"
            ],
            [
                37472,
                37488,
                "TERMINO"
            ],
            [
                37573,
                37588,
                "TERMINO"
            ],
            [
                37981,
                37992,
                "TERMINO"
            ],
            [
                38626,
                38641,
                "TERMINO"
            ],
            [
                38688,
                38703,
                "TERMINO"
            ],
            [
                38866,
                38878,
                "TERMINO"
            ],
            [
                39003,
                39016,
                "TERMINO"
            ],
            [
                39220,
                39236,
                "TERMINO"
            ],
            [
                39340,
                39355,
                "TERMINO"
            ],
            [
                39540,
                39552,
                "TERMINO"
            ],
            [
                39726,
                39737,
                "TERMINO"
            ],
            [
                39991,
                40003,
                "TERMINO"
            ],
            [
                40057,
                40069,
                "TERMINO"
            ],
            [
                40287,
                40298,
                "TERMINO"
            ],
            [
                40927,
                40954,
                "TERMINO"
            ],
            [
                42065,
                42082,
                "TERMINO"
            ],
            [
                43008,
                43022,
                "TERMINO"
            ],
            [
                43644,
                43660,
                "TERMINO"
            ],
            [
                44742,
                44763,
                "TERMINO"
            ],
            [
                46561,
                46577,
                "TERMINO"
            ],
            [
                47222,
                47236,
                "TERMINO"
            ],
            [
                53307,
                53321,
                "TERMINO"
            ],
            [
                55695,
                55709,
                "TERMINO"
            ],
            [
                61243,
                61259,
                "TERMINO"
            ],
            [
                63266,
                63276,
                "TERMINO"
            ],
            [
                63775,
                63790,
                "TERMINO"
            ],
            [
                64114,
                64128,
                "TERMINO"
            ],
            [
                64203,
                64217,
                "TERMINO"
            ],
            [
                64328,
                64342,
                "TERMINO"
            ],
            [
                64388,
                64406,
                "TERMINO"
            ],
            [
                64442,
                64456,
                "TERMINO"
            ],
            [
                64793,
                64807,
                "TERMINO"
            ],
            [
                64930,
                64957,
                "TERMINO"
            ],
            [
                65088,
                65103,
                "TERMINO"
            ],
            [
                65305,
                65320,
                "TERMINO"
            ],
            [
                65333,
                65350,
                "TERMINO"
            ],
            [
                65470,
                65482,
                "TERMINO"
            ],
            [
                65502,
                65517,
                "TERMINO"
            ],
            [
                66190,
                66205,
                "TERMINO"
            ],
            [
                66286,
                66298,
                "TERMINO"
            ],
            [
                66334,
                66346,
                "TERMINO"
            ],
            [
                67038,
                67052,
                "TERMINO"
            ],
            [
                67639,
                67654,
                "TERMINO"
            ],
            [
                67809,
                67824,
                "TERMINO"
            ],
            [
                67910,
                67922,
                "TERMINO"
            ],
            [
                67931,
                67946,
                "TERMINO"
            ],
            [
                68050,
                68062,
                "TERMINO"
            ],
            [
                69054,
                69060,
                "TERMINO"
            ],
            [
                69165,
                69176,
                "TERMINO"
            ],
            [
                69371,
                69385,
                "TERMINO"
            ],
            [
                69655,
                69667,
                "TERMINO"
            ],
            [
                69730,
                69745,
                "TERMINO"
            ],
            [
                70986,
                71006,
                "TERMINO"
            ],
            [
                71968,
                71982,
                "TERMINO"
            ],
            [
                72200,
                72215,
                "TERMINO"
            ],
            [
                72774,
                72786,
                "TERMINO"
            ],
            [
                73501,
                73516,
                "TERMINO"
            ],
            [
                73745,
                73761,
                "TERMINO"
            ],
            [
                74123,
                74138,
                "TERMINO"
            ],
            [
                74219,
                74231,
                "TERMINO"
            ],
            [
                74351,
                74363,
                "TERMINO"
            ],
            [
                74493,
                74508,
                "TERMINO"
            ],
            [
                75365,
                75379,
                "TERMINO"
            ],
            [
                75571,
                75589,
                "TERMINO"
            ],
            [
                75689,
                75704,
                "TERMINO"
            ],
            [
                75795,
                75810,
                "TERMINO"
            ],
            [
                76221,
                76236,
                "TERMINO"
            ],
            [
                76347,
                76362,
                "TERMINO"
            ],
            [
                77544,
                77559,
                "TERMINO"
            ],
            [
                77747,
                77762,
                "TERMINO"
            ],
            [
                77900,
                77913,
                "TERMINO"
            ],
            [
                78334,
                78349,
                "TERMINO"
            ],
            [
                79982,
                79997,
                "TERMINO"
            ],
            [
                80093,
                80108,
                "TERMINO"
            ],
            [
                80528,
                80543,
                "TERMINO"
            ],
            [
                81865,
                81880,
                "TERMINO"
            ],
            [
                81971,
                81986,
                "TERMINO"
            ],
            [
                82232,
                82247,
                "TERMINO"
            ],
            [
                82288,
                82303,
                "TERMINO"
            ],
            [
                82501,
                82516,
                "TERMINO"
            ],
            [
                82803,
                82818,
                "TERMINO"
            ],
            [
                82912,
                82927,
                "TERMINO"
            ],
            [
                82994,
                83012,
                "TERMINO"
            ],
            [
                83217,
                83232,
                "TERMINO"
            ],
            [
                83466,
                83481,
                "TERMINO"
            ],
            [
                83617,
                83632,
                "TERMINO"
            ],
            [
                83909,
                83924,
                "TERMINO"
            ],
            [
                84186,
                84201,
                "TERMINO"
            ],
            [
                84684,
                84699,
                "TERMINO"
            ],
            [
                84702,
                84717,
                "TERMINO"
            ],
            [
                84929,
                84944,
                "TERMINO"
            ],
            [
                85385,
                85400,
                "TERMINO"
            ],
            [
                85879,
                85894,
                "TERMINO"
            ],
            [
                86251,
                86266,
                "TERMINO"
            ],
            [
                86586,
                86597,
                "TERMINO"
            ],
            [
                87744,
                87759,
                "TERMINO"
            ],
            [
                88081,
                88096,
                "TERMINO"
            ],
            [
                88508,
                88523,
                "TERMINO"
            ],
            [
                88693,
                88708,
                "TERMINO"
            ],
            [
                90201,
                90216,
                "TERMINO"
            ],
            [
                90733,
                90750,
                "TERMINO"
            ],
            [
                91000,
                91018,
                "TERMINO"
            ],
            [
                91697,
                91710,
                "TERMINO"
            ],
            [
                92981,
                92994,
                "TERMINO"
            ],
            [
                93348,
                93363,
                "TERMINO"
            ],
            [
                94360,
                94375,
                "TERMINO"
            ],
            [
                94563,
                94578,
                "TERMINO"
            ],
            [
                95165,
                95180,
                "TERMINO"
            ],
            [
                96902,
                96913,
                "TERMINO"
            ],
            [
                96937,
                96952,
                "TERMINO"
            ],
            [
                97009,
                97024,
                "TERMINO"
            ],
            [
                98225,
                98240,
                "TERMINO"
            ],
            [
                100101,
                100116,
                "TERMINO"
            ],
            [
                100860,
                100875,
                "TERMINO"
            ],
            [
                101040,
                101055,
                "TERMINO"
            ],
            [
                101123,
                101138,
                "TERMINO"
            ],
            [
                103369,
                103384,
                "TERMINO"
            ],
            [
                105720,
                105735,
                "TERMINO"
            ],
            [
                107003,
                107018,
                "TERMINO"
            ],
            [
                107110,
                107125,
                "TERMINO"
            ],
            [
                107240,
                107255,
                "TERMINO"
            ],
            [
                108313,
                108328,
                "TERMINO"
            ],
            [
                109103,
                109118,
                "TERMINO"
            ],
            [
                109475,
                109490,
                "TERMINO"
            ],
            [
                110442,
                110457,
                "TERMINO"
            ],
            [
                110688,
                110703,
                "TERMINO"
            ],
            [
                110810,
                110825,
                "TERMINO"
            ],
            [
                111454,
                111472,
                "TERMINO"
            ],
            [
                111886,
                111899,
                "TERMINO"
            ],
            [
                113764,
                113779,
                "TERMINO"
            ],
            [
                113956,
                113971,
                "TERMINO"
            ],
            [
                114465,
                114480,
                "TERMINO"
            ],
            [
                115045,
                115056,
                "TERMINO"
            ],
            [
                115239,
                115250,
                "TERMINO"
            ],
            [
                115726,
                115746,
                "TERMINO"
            ],
            [
                115919,
                115934,
                "TERMINO"
            ],
            [
                116123,
                116134,
                "TERMINO"
            ],
            [
                116310,
                116330,
                "TERMINO"
            ],
            [
                116391,
                116405,
                "TERMINO"
            ],
            [
                116584,
                116598,
                "TERMINO"
            ],
            [
                117408,
                117422,
                "TERMINO"
            ],
            [
                117474,
                117488,
                "TERMINO"
            ],
            [
                117535,
                117549,
                "TERMINO"
            ],
            [
                117764,
                117778,
                "TERMINO"
            ],
            [
                117933,
                117947,
                "TERMINO"
            ],
            [
                118046,
                118060,
                "TERMINO"
            ],
            [
                118105,
                118119,
                "TERMINO"
            ],
            [
                118181,
                118195,
                "TERMINO"
            ],
            [
                118389,
                118400,
                "TERMINO"
            ],
            [
                118583,
                118594,
                "TERMINO"
            ],
            [
                119546,
                119557,
                "TERMINO"
            ],
            [
                119715,
                119726,
                "TERMINO"
            ],
            [
                120124,
                120135,
                "TERMINO"
            ],
            [
                120358,
                120369,
                "TERMINO"
            ],
            [
                120546,
                120557,
                "TERMINO"
            ],
            [
                120590,
                120601,
                "TERMINO"
            ],
            [
                120644,
                120655,
                "TERMINO"
            ],
            [
                120790,
                120805,
                "TERMINO"
            ],
            [
                120985,
                121000,
                "TERMINO"
            ],
            [
                121070,
                121081,
                "TERMINO"
            ],
            [
                121212,
                121223,
                "TERMINO"
            ],
            [
                124399,
                124414,
                "TERMINO"
            ],
            [
                124572,
                124583,
                "TERMINO"
            ],
            [
                124822,
                124833,
                "TERMINO"
            ],
            [
                124990,
                125002,
                "TERMINO"
            ],
            [
                125420,
                125435,
                "TERMINO"
            ],
            [
                126122,
                126133,
                "TERMINO"
            ],
            [
                126222,
                126233,
                "TERMINO"
            ],
            [
                126264,
                126275,
                "TERMINO"
            ],
            [
                126424,
                126435,
                "TERMINO"
            ],
            [
                126723,
                126734,
                "TERMINO"
            ],
            [
                127124,
                127135,
                "TERMINO"
            ],
            [
                127949,
                127960,
                "TERMINO"
            ],
            [
                128859,
                128870,
                "TERMINO"
            ],
            [
                128917,
                128928,
                "TERMINO"
            ],
            [
                129057,
                129068,
                "TERMINO"
            ],
            [
                129345,
                129356,
                "TERMINO"
            ],
            [
                129650,
                129661,
                "TERMINO"
            ],
            [
                129761,
                129772,
                "TERMINO"
            ],
            [
                129837,
                129848,
                "TERMINO"
            ],
            [
                133194,
                133205,
                "TERMINO"
            ],
            [
                134839,
                134853,
                "TERMINO"
            ],
            [
                135024,
                135039,
                "TERMINO"
            ],
            [
                137362,
                137373,
                "TERMINO"
            ],
            [
                137924,
                137935,
                "TERMINO"
            ],
            [
                138216,
                138230,
                "TERMINO"
            ],
            [
                139367,
                139378,
                "TERMINO"
            ],
            [
                140080,
                140091,
                "TERMINO"
            ],
            [
                141699,
                141710,
                "TERMINO"
            ],
            [
                143256,
                143279,
                "TERMINO"
            ],
            [
                143457,
                143474,
                "TERMINO"
            ],
            [
                144428,
                144447,
                "TERMINO"
            ],
            [
                144613,
                144624,
                "TERMINO"
            ],
            [
                144761,
                144772,
                "TERMINO"
            ],
            [
                145013,
                145024,
                "TERMINO"
            ],
            [
                145611,
                145625,
                "TERMINO"
            ],
            [
                145942,
                145953,
                "TERMINO"
            ],
            [
                146311,
                146322,
                "TERMINO"
            ],
            [
                146387,
                146402,
                "TERMINO"
            ],
            [
                146495,
                146506,
                "TERMINO"
            ],
            [
                146640,
                146651,
                "TERMINO"
            ],
            [
                146690,
                146701,
                "TERMINO"
            ],
            [
                146823,
                146834,
                "TERMINO"
            ],
            [
                146976,
                146987,
                "TERMINO"
            ],
            [
                147120,
                147131,
                "TERMINO"
            ],
            [
                147274,
                147289,
                "TERMINO"
            ],
            [
                147425,
                147436,
                "TERMINO"
            ],
            [
                147858,
                147869,
                "TERMINO"
            ],
            [
                148266,
                148277,
                "TERMINO"
            ],
            [
                148894,
                148905,
                "TERMINO"
            ],
            [
                149028,
                149039,
                "TERMINO"
            ],
            [
                149562,
                149577,
                "TERMINO"
            ],
            [
                150674,
                150689,
                "TERMINO"
            ],
            [
                151535,
                151551,
                "TERMINO"
            ],
            [
                151652,
                151663,
                "TERMINO"
            ],
            [
                152224,
                152235,
                "TERMINO"
            ],
            [
                152425,
                152441,
                "TERMINO"
            ],
            [
                152656,
                152672,
                "TERMINO"
            ],
            [
                157410,
                157419,
                "TERMINO"
            ],
            [
                157831,
                157840,
                "TERMINO"
            ],
            [
                158008,
                158017,
                "TERMINO"
            ],
            [
                158565,
                158580,
                "TERMINO"
            ],
            [
                159314,
                159323,
                "TERMINO"
            ],
            [
                160879,
                160894,
                "TERMINO"
            ],
            [
                162228,
                162237,
                "TERMINO"
            ],
            [
                163051,
                163069,
                "TERMINO"
            ],
            [
                164460,
                164474,
                "TERMINO"
            ],
            [
                164760,
                164778,
                "TERMINO"
            ],
            [
                165836,
                165850,
                "TERMINO"
            ],
            [
                165890,
                165908,
                "TERMINO"
            ],
            [
                167620,
                167634,
                "TERMINO"
            ],
            [
                168580,
                168598,
                "TERMINO"
            ],
            [
                168667,
                168685,
                "TERMINO"
            ],
            [
                169247,
                169259,
                "TERMINO"
            ],
            [
                171801,
                171810,
                "TERMINO"
            ],
            [
                172287,
                172301,
                "TERMINO"
            ],
            [
                172372,
                172386,
                "TERMINO"
            ],
            [
                173070,
                173084,
                "TERMINO"
            ],
            [
                178502,
                178514,
                "TERMINO"
            ],
            [
                183374,
                183390,
                "TERMINO"
            ],
            [
                184424,
                184447,
                "TERMINO"
            ],
            [
                187553,
                187568,
                "TERMINO"
            ],
            [
                189998,
                190010,
                "TERMINO"
            ],
            [
                190624,
                190636,
                "TERMINO"
            ],
            [
                191024,
                191042,
                "TERMINO"
            ]
        ]
    },
    {
        "text": "Introduction\nThere are two distinguishing features of Corpus Linguistics as a field of research.\nFirstly, it involves naturally-occurring discourse, and in relatively large quantities.\nWhat counts as 'relatively large' depends on the individual study and can be anything from a few hundred thousand words to hundreds of millions of words.\nAmong the assumptions that lie behind Corpus Linguistics, though, is the view that there are aspects of language use that are important but that are invisible to the human reader of texts.\nIn particular, the relative frequency with which words, phrases, and grammatical categories are used is of importance but can be established only with the help of search software.\nIn very basic cases, the language of the corpus is rearranged so that a reader is presented with an altered and focused view.\nTaking this one or more steps further, quantitative information is given that replaces the human reading process.\nSecondly, Corpus Linguistics attempts to make contributions to linguistic theory that are informed by quantitative information.\nAs will be noted below, those contributions may relate to the mechanisms by which language changes over time, or to the nature of the difference between registers, or to the relationship between lexis and grammar.\nThe question of 'what is language like' is one that Corpus Linguistics seeks to answer.\nIn some cases, that answer is very much aligned with other approaches to language; in other cases less so.\nWork in Corpus Linguistics has grown exponentially over the last three decades, and the quantitative tools it routinely uses have become more sophisticated.\nAn area of constant exploration is the role that a technical expertise in language structure -lexis, grammar, discourse -plays in relation to expertise in quantification.\nAs the discussion below will indicate there are various types of combination, from 'mainly language with some numbers thrown in' to 'mainly numbers with little regard for language'.\nThis is not simply a matter of finding a balance, but constantly exploring new ways of approaching language so that possibilities of finding new knowledge constantly come into view.\nWhatever Corpus Linguistics is, it is not static.\nIn this paper I shall present a view of Corpus Linguistics that conceptualises it in three phases, distinguished by the use made in each phase of quantitative data.\nI am going to be talking about corpora of English, though much of what I say will be applicable to other languages.\nLaying the foundation: quantifying language categories\nOne of the greatest contributions of corpus linguistics to theoretical linguistics is the opportunity it affords for quantifying the comparative frequency of various linguistic categories that are identified and tagged in corpora.\nThis activity has not been without criticism.\nChomsky, for example, has famously offered the view that simple quantity adds nothing to the explanatory function of theory.\nIt is true that establishing comparative frequencies does not change our knowledge of what can be said, but it does alter our understanding of what is typically said, and under what circumstances.\nIt also permits comparison between corpora and as a consequence geographical varieties can be compared, changes in language over time can be observed, and the stages of language development (in children or in learners) can be described.\nA key example of this kind of work is the Longman Grammar of Spoken and Written English\nAn example of comparative work in the same tradition is the paper by Annotating a corpus for the categories central to SFL (for example, distinguishing process types, or types of Theme) remains a largely manual enterprise, though assisted by mark-up and quantification software such as the UAM Corpus Tool (O'Donnell).\nOnce annotation has been carried out, however, quantification can be carried out.\nMatthiessen demonstrates that registers are indeed distinguished by the relative frequency of grammatical categories in them, and he also shows the variation of individual texts within the register.\n'Quantifying' phraseology: a lexical approach\nThe second 'phase' I draw attention to adopts a lexical view of language, and is often treated as qualitative rather than quantitative.\nThis approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence.\nThe job of the researcher is to identify patterns of use.\nThe attitude towards quantification is relatively casual and implicit, but is nonetheless of importance.\nTwo examples will be given here, both using the Bank of English corpus (HarperCollins Publishers and the University of Birmingham).\nThe first example uses the quintessentially British phrase 'cup of tea' (a phrase used as a demonstration by Sinclair and by Danielsson).\nThe Bank of English contains over 2,400 instances of this phrase, of which 100 random examples are selected for this illustration.\nPreceding 'cup of tea' are a number of different elements: • Indefinite article: 'a cup of tea' (65 instances) • Indefinite article + adjective: 'a nice/quick/calming cup of tea' (9 instances)\n• Possessive: 'my/your/Linda's cup of tea' (7 instances) •\nOther determiners: 'the cup of tea', 'another cup of tea', 'every cup of tea' (6 instances)\nThe obvious observation here is that 'cup of tea' is used with the indefinite article in nearly 75% of cases.\nBut a further observation that can be made is that 'a cup of tea' and 'my cup of tea' are dissimilar in meaning ('They planned to celebrate with a quiet cup of tea' as opposed to 'She's not my cup of tea').\nIn other words, 'a cup of tea' is a container with brown liquid in it, whereas 'my cup of tea' refers to one's preference or otherwise for an individual.\nTo what extent is it true that 'possessive + cup of tea' has this metaphoric use exclusively?\nTo test this, instances of 'my/his/her/their cup of tea' in the BoE are identified (160 in total), and 100 random lines selected.\nThe ones that do not indicate preference are then identified; there are 27 of them.\nThus, 73% of instances of 'possessive + cup of tea' have a non-literal interpretation.\nA further observation is that the non-literal instances tend to include either a negative or a comparator ('not my cup of tea' or 'more/just/entirely my cup of tea').\nA further count is then carried out on the 100 lines, identifying those including a negative or comparator and those with literal or non-literal meaning.\nLiteral Non-literal Total\nThe usual conclusion from this kind of study is that the non-literal meaning of 'cup of tea' is reliably associated with negative and comparative phraseology, while the literal meaning is rarely used with these words.\nMore significantly, information such as this forms the basis of Sinclair's concept of Idiom\nSinclair argues that much of English operates within this somewhat grey area between the absolute fixed phrase and the entire open choice.\nIt is often exploited in jokes, such 'Chocolate is not my cup of tea'.\nThe second point to make is that the precise numbers involved here are not of particular interest.\nThe difference between the relevant numbers have to be significant, not only in the statistical sense but sufficiently to give confidence that the generalisations drawn from them are accurate.\nMy second example is a study of verb complementation that whether decide 7 16\nFurthermore, the wordform 'decide' when followed by 'whether' is frequently preceded by indications of obligation, necessity or volition, such as 'will decide', 'has yet to decide', 'was forced to decide' and so on.\nThis was later formalised with a study that looked at 10 verbs, each occurring with both that-clauses and wh-clauses.\nThe conclusion was that non-finite verb-forms co-occur with wh-clauses that construe hypothetical actions whereas finite verbforms co-occur with that-clauses that construe actual situations.\nIn addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse.\nFurther examples are given in\nIn this section I have drawn on a tradition which, as noted above, is often described as qualitative rather than quantitative.\nWhat I have tried to point out, however, is that this qualitative work does rely on measures, sometimes intuitively rather than formally established, of relative and comparative frequency.\nWhat is valued in this kind of work is observation or noticing, that is, the identification of classes of object that may not exist as a class outside that context and which have a meaning-or function-related definition rather than a formal one.\nEnhancing innovation\nAll the kinds of quantitative approaches outlined above continue to be used in Corpus Linguistics.\nIf we try to combine the benefits of phases 1 and 2 we might arrive at the following desiderata: • The statistics should be robust and should be applicable to language data; • The method of working should rest on as few preconceptions about language as possible, and be as exploratory as possible; • The outcome should offer genuine insight into language and discourse.\nI am now going to give three examples of what might be considered to be phase 3 research.\nI would describe this work as quantity-led.\nLike the phase 2 work outlined above, there is an attempt to rely on information that emerges from the text ('trust the text', as Sinclair says), rather than on information that is presupposed.\nPhase 2 work is based on the human observation of the behaviour of large numbers of a single word or phrase ('decide' or 'cup of tea') and builds theory bottom-up from such observations.\nConcordancing software rearranges the data -the texts -to permit that observation.\nIn phase 3 work, statistical packages take over the role of rearranging the data.\nSo although the approach is quantitative rather than qualitative, and relies on numbers rather than observation, to my mind it has the same bottomup approach that moves from evidence to theory.\nOne example of this phase comes from many decades ago; the others are more recent.\nExample 1: Multi-Dimensional Analysis and subsequent publications)\nThe essence of Biber's MDA is the observation that language is different in different contexts.\nAcademic prose is different from newspaper prose; political speeches are different from casual conversation, and so on.\nThe differences are easily recognisable, but rest on a multiplicity of frequency variations.\nBiber is not the only linguist to observe this (Halliday does this in a more theoretically-informed way, and Matthiessen, as noted, has added statistical rigour to that model), but Biber has carried out more extensive investigations of this type than anyone else.\nUnlike Halliday, he begins with a 'common sense' notion of register and with an eclectic mix of language features.\nThe mix is intentionally eclectic because unlike Halliday he does not make presuppositions about which features will be significant in distinguishing between registers.\nThe various sub-corpora are then tagged with the language features, and the strength of co-occurrence of those features is calculated.\nThe result is a number of factors, each consisting of a set of features that either attract or repel each other.\nThe factors are then interpreted in terms of what they mean in terms of discourse.\n'Informational' versus 'involved' is one factor; 'narrative' versus 'non-narrative' is another.\nAt this point the factors are renamed 'dimensions'.\nCorpora of texts belonging to two different registers may be alike on one dimension and different on another.\nPlotting registers as being more or less like each other therefore requires multiple dimensions.\nThe original five dimensions proposed by Biber have been widely used in subsequent work by him and others, but in other work the dimensions have been worked out anew, permitting the approach to be applied to texts that may not be categorised in traditional register categories.\nFurther refinements to the set of language features have also been made.\nA project carried out 2013-15, led by Thompson and advised by Biber ('Interdisciplinary Research Discourse' or IDRD), used the current set of language features from Biber's studies, but attempted two innovations.\nThe first was to generate new dimensions from a corpus of articles from a single academic journal (Global Environmental Change).\nWe identified, for example, 'system-oriented' versus 'action-oriented'; 'explicit argumentation' versus 'implicit argumentation'; greater or lesser degrees of 'spoken-ness'.\nThe second was to avoid a prior division of our corpus into registers.\nWe had deliberately selected an interdisciplinary journal for our project, with the aim of investigating ID discourse.\nAlthough we hypothesised in advance that different disciplinary discourse styles would emerge from the journal, we were not able to, and indeed did not want to, divide the articles in the journal between those disciplines in order to arrive at sub-corpora that could then be compared.\nWhat we did instead was to assign each article -each text -a value on each of the identified dimensions.\nWe then derived clusters of texts (or 'constellations') that shared values on those dimensions.\nDepending on how the figures are interpreted -with greater or lesser granularity -we identify 3 or 6 constellations.\nWe are able then to identify the type of research being reported in each, basically on stylistic grounds.\nFigure Example 2: Collostructions (Stefanowitsch and Gries, 2003)\nMy second example uses quantitative information to enhance a linguistic theory.\nThat theory is the theory of Constructions (e.g. For example, they study the ditransitive construction ('give someone something', 'promise someone something', 'tell someone something'), arguing that the 'agent + recipient + theme' Stefanowitsch and Gries's work aligns with, though is not derived from, classic phase 2 approaches to language.\nFor example, constructions such as 'accident waiting to happen' look very similar to Sinclair's Units of Meaning.\nThose such as 'predicative as' or 'causative into' are identifiable as grammar patterns Where Stefanowitsch and Gries's work is different is that it offers a robust quantitative approach to the question of collostruction, rather than relying on impressions of frequency.\nThe numerical value given to collostructional strength is used to offer an insight into how constructions come to express meaning, as this is said to be derived from the meaning of the collostructions with the highest strength.\nExample 3: Co-occurrence measurements as exploratory mechanisms\nIn this third example I include two research projects carried out recently, both of which involve exploiting quantitative measures to explore corpora.\nThese are 'bottom-up' in the sense that there is no pre-emptive model of language at the outset, but unlike phase 2 studies they have numbers rather than words at their heart.\nFor me, they have the genuine sense of exploring the unknown and of encountering unexpected insights that phase 2 studies also have.\nThe studies are somewhat controversial, in that they treat language as a 'bag of words', that is, without adding linguistic knowledge about structure, meaning and so on.\nLinguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.\nIn introducing these examples I have used the term 'co-occurrence', meaning that two words frequently co-occur in the same text.\nCo-occurrence of words within a short span (i.e. 'collocation') is a traditional concept in Corpus Linguistics, as noted at the beginning of this paper.\nCollocation, as Firth famously almost said, gives us a lot of information about a word: its denotational and connotational meanings, for example.\nIt has been widely accepted with Corpus Linguistics that collocation needs to be measured within a fairly short span: + / -5 words from the node is common.\nBeyond this, the influence of a given word is negligible.\nIn the studies described in this section, a rather different view of co-occurrence is taken.\nThere is no node word and no directional influence, and the purpose is not to find out more about an individual word.\nRather one aim is to gain novel insights into a set of texts by observing the co-occurrence of words within them.\nThe second aim is to gain novel insights into those words by organising them into groups according to the strength of their co-occurrence in given texts.\nThe first is a project initiated by Neil Millar\nMy second example under this heading is another study undertaken as part of the IDRD project outlined above and used the same corpus.\nOne of our aims in that project was to find bottom-up and novel ways to explore a corpus whose contents were diverse but not in known ways.\nTo do this we used topic modelling\nThe output from topic modelling is a set of lists of words that are grouped according to the probability of their co-occurrence within the specified texts.\nWhat the lists give us, first of all, is a sense of the 'aboutness' of the corpus.\nHere are a few of the themes that can be identified (from • Kinds of natural environment e.g. There are, of course, other methods of tracking topics through time, or of identifying the focus of a given paper.\nSimply reading the paper is an obvious method!\nWhat is significant about using topic modelling is precisely that it does not rely on pre-existing ideas about what a topic might consist of.\nIt throws open the notion of 'aboutness' and uses a data-driven way of organising the content of a large corpus.\nConclusion\nIn this paper I have distinguished between a number of approaches to quantification, or phases, in Corpus Linguistics.\nI have suggested that there tends to be a tension between statistical rigour and a desired objective of using data-driven methods to drive theoretical innovation.\nHowever, methods of identifying word co-occurrence provide a way of organising a corpus to lead to new insights.\nOne of the key questions for Corpus Linguistics is how a corpus might satisfactorily be 'analysed'.\nIn general, the investigation is top-down, in the sense that a question is asked of the corpus and means devised to find the answer to the question.\nThe project relating to 'Rate My Professors', described above, is one such investigation, where the question: 'what categories of individual qualities are discernible from the comments made' is answered using the strength of co-occurrence of adjectives as the research method.\nA guiding principle in Corpus Linguistics, however, is that one should 'trust the text'\n",
        "entities": [
            [
                54,
                69,
                "TERMINO"
            ],
            [
                377,
                392,
                "TERMINO"
            ],
            [
                443,
                455,
                "TERMINO"
            ],
            [
                958,
                973,
                "TERMINO"
            ],
            [
                1158,
                1173,
                "TERMINO"
            ],
            [
                1342,
                1357,
                "TERMINO"
            ],
            [
                1493,
                1508,
                "TERMINO"
            ],
            [
                2186,
                2201,
                "TERMINO"
            ],
            [
                2267,
                2282,
                "TERMINO"
            ],
            [
                2600,
                2615,
                "TERMINO"
            ],
            [
                3781,
                3792,
                "TERMINO"
            ],
            [
                4286,
                4301,
                "TERMINO"
            ],
            [
                4347,
                4363,
                "TERMINO"
            ],
            [
                8130,
                8146,
                "TERMINO"
            ],
            [
                8934,
                8949,
                "TERMINO"
            ],
            [
                11017,
                11033,
                "TERMINO"
            ],
            [
                11254,
                11270,
                "TERMINO"
            ],
            [
                12203,
                12219,
                "TERMINO"
            ],
            [
                12384,
                12400,
                "TERMINO"
            ],
            [
                15733,
                15751,
                "TERMINO"
            ],
            [
                15822,
                15837,
                "TERMINO"
            ],
            [
                16062,
                16077,
                "TERMINO"
            ],
            [
                16535,
                16553,
                "TERMINO"
            ],
            [
                17939,
                17954,
                "TERMINO"
            ],
            [
                18264,
                18279,
                "TERMINO"
            ],
            [
                18784,
                18799,
                "TERMINO"
            ]
        ]
    },
    {
        "text": "Introduction\nSupervised learning is the machine learning task which consists of building a model capable of predicting output for a specific problem based on prior observation of a previously labeled data set\nThe main idea behind this is that supervised learning models can infer new knowledge by establishing associations between the examples provided and the expected tags.\nHowever, supervised learning requires a sufficient number of labeled examples that model the problem domain and, at the same time, the number of examples should be enough to cluster the examples in two subsets, one for model learning, and another for evaluating its accuracy based on samples that are not seen during the training stage.\nThe development of an annotated corpus is a very time-consuming process.\nTo facilitate this task, some researchers have used distant supervision as a method of getting automatically annotated data\nFurthermore, even though the annotation process occurs manually, the quality of the corpus cannot be guaranteed.\nIn this sense, Mozetič, Igor et al.\nThe remainder of this paper is organized as follows: Section 2 describes our proposal, and Section 3 contains information regarding studies based on corpus compiled with this tool, as well as the description of future lines of action.\nSystem architecture UMUCorpusClassifier is designed to work with the social network Twitter, which is a widely used network for compiling corpus in various branches of Natural Language Processing (NLP)\nIdentifying duplicate tweets is a complex task.\nAlthough Twitter provides a mechanism called retweet that allows the content of messages written by other users to be disseminated and the identification of these messages is trivial, many users in the social network use copy and paste mechanisms so it is possible to find duplicate or virtually the same tweets.\nAlso, hyperlinks in tweets are encoded differently with each new tweet due to Twitter's own hyperlink shortening mechanism.\nFor this reason, we have made the decision to replace URLs with a fixed token, which makes it easier to identify certain tweets.\nIn addition, we have added a mechanism to calculate the similarity of the texts.\nBeing an experimental technology, tweets are not removed, but administrators are given the opportunity to combine the responses of the tweets at their discretion.\nThe next step is to assign each corpus a set of independent labels.\nThey can be made using a set of predefined labels, such as outof-domain, positive, negative, neutral, do-notknow-do-not-answer or define a new set of tags for the corpus.\nEach label is identified with a color and a name.\nThe corpus labeling process can be carried out manually by the same user or allow access to the platform to a set of annotators and to supervise their work.\nDocuments are When an annotator enters the web application, a tweet appears randomly from tweets that they had not previously classified.\nFigure\nTo facilitate the labeling monitoring process, UMUCorpusClassifier provides a set of metrics and charts, such as the evolution of the annotations made by time, to evaluate that the work is being done constantly; the total rankings by tag; or (3) the average of annotations as well as their standard deviation.\nFigure\nFor each annotator, the degree of selfagreement is calculated, which measures how the same annotator classifies semantically similar documents.\nTo cluster similar documents, we obtain the sentence-embeddings from FastText in its Spanish version\nFor each tweet, one can see the degree of inter-agreement that measures how the same tweet is classified by different annotators by using the Krippendorff's alpha coefficient\nOnce the corpus has been compiled and annotated, it can be exported to text formats.\nThe export process is flexible and allows you to choose the number of classes to export.\nCombining classes is also allowed.\nThis is useful, for example, when a classification has been made on a scale of very negative, negative, neutral, positive, and very positive type values, but one may want to combine the results to return the corpus grouped into positive, neutral, and negative.\nCorpora can be exported balanced, that is to say, the system automatically searches for the class with the largest number of instances and cuts instances from the other classes.\nThese deleted instances are agreed by consensus.\nFinally, it is possible to export only the Twitter IDs in order to share them with the community as recommended by Twitter's privacy policies Furthermore, the number of instances to export can be selected and set.\nOne of the advantages of this approach is that corpus can be exported by consensus: since the same tweet can be classified by different annotators, the number of tweets to export can be limited and retrieve those tweets that have achieved strong consensus among annotators.\nThus, subsets of the corpus comprising the documents with common agreement can be retrieved, and the rest of the documents can be analyzed.\nFurthermore, the software is easily extensible.\nIn this respect, it is relatively easy to include new strategies to export the data or to improve the platform to include new data sources other than Twitter.\nFurther work\nIn this study, we have presented UMUCor-pusClassifier, a NLP tool that assists in the compilation and annotation of linguistic corpus.\nSo far, we have used this application on several domains.\nSpecifically, we have compiled tweets about different types of diseases to carry out infodemiology studies that involve measuring the population's perception of infectious diseases\nIn the current version of the platform it is only possible to assign a label to a document.\nWe are working to enable the multilabel classification.\nAnother line of research is the addition of a contextual feature extraction module, enabling the analysis of groups of Twitter accounts from which tweets are extracted.\nThese features may include information on the time of publication, number of followers, etc.\nLastly, with regard to semantic similarity, we are currently analyzing ways to distance the most different tweets from each other, so that we can export the tweets with strongest consensus and the most distant ones.\n",
        "entities": [
            [
                200,
                208,
                "TERMINO"
            ],
            [
                511,
                528,
                "TERMINO"
            ],
            [
                939,
                957,
                "TERMINO"
            ],
            [
                1462,
                1489,
                "TERMINO"
            ],
            [
                4267,
                4285,
                "TERMINO"
            ],
            [
                4538,
                4556,
                "TERMINO"
            ]
        ]
    },
    {
        "text": "Preface\nThis book has been a long time in the making (the first version of the first chapter has the date stamp 2005-08-04, 11:30) and as the field of corpus linguistics and my own perspective on this field developed over this time span, many thoughts accumulated, that I intended to put into the preface when the time would come to publish.\nNow that this time is finally there, I feel that there is not much left to say that I have not already said in the book itself.\nHowever, given that there is, by now, a large number of corpus-linguistic textbooks available, ranging from the very decent to the excellent, a few words seem in order to explain why I feel that it makes sense to publish another one.\nThe main reason is that I have found, in my many years of teaching corpus linguistics, that most available textbooks are either too general or too specific.\nOn the one hand, there are textbooks that provide excellent discussions of the history of corpus linguistics or the history of corpus design, or that discuss the epistemological status of corpus data in a field that has been dominated far too long by generative linguistic ideas about what does and does not constitute linguistic evidence.\nOn the other hand, there are textbooks that focus one or more specific corpus-based techniques, discussing very specific phenomena (often the research interests of the textbook authors themselves) using a narrow range of techniques (often involving specific software solutions).\nWhat I would have wanted and needed when I took my first steps into corpus linguistic research as a student is an introductory textbook that focuses on methodological issues -on how to approach the study of language based on usage data and what problems to expect and circumvent.\nA book that discusses the history and epistemology of corpus linguistics only to the extent necessary to grasp these methodological issues and that presents case studies of a broad range of linguistic phenomena from a coherent methodological perspective.\nThis book is my attempt to write such a textbook.\nThe first part of the book begins with an almost obligatory chapter on the need for corpus data (a left-over from a time when corpus linguistics was still somewhat of a fringe discipline).\nI then present what I take to be the methodological foundations that distinguish corpus linguistics from other, superficially Preface similar methodological frameworks, and discuss the steps necessary to build concrete research projects on these foundations -formulating the research question, operationalizing the relevant constructs and deriving quantitative predictions, extracting and annotating data, evaluating the results statistically and drawing conclusions.\nThe second part of the book presents a range of case studies from the domains of lexicology, grammar, text linguistics and metaphor, including variationist and diachronic perspectives.\nThese case studies are drawn from the vast body of corpus linguistic research literature published over the last thirty years, but they are all methodologically deconstructed and explicitly reconstructed in terms of the methodological framework developed in the first part of the book.\nWhile I refrain from introducing specific research tools (e.g. in the form of specific concordancing or statistics software), I have tried to base these case studies on publicly available corpora to allow readers to replicate them using whatever tools they have at their disposal.\nI also provide supplementary online material, including information about the corpora and corpus queries used as well as, in many cases, the full data sets on which the case studies are based.\nAt the time of publication, the supplementary online material is available as a zip file via\nI hope that the specific perspective taken in this book, along with the case studies and the possibility to study the full data sets, will help both beginning and seasoned researchers gain an understanding of the underlying logic of corpus linguistic research.\nIf not -the book is free as in beer, so at least you will not have wasted any money on it.\nIt is also free as in speech -the Creative Commons license under which it is published allows you to modify and build on the content, remixing it into the textbook you would have wanted and needed.\nBerlin, 8th March 2020 viii 1 The need for corpus data\nBroadly speaking, science is the study of some aspect of the (physical, natural or social) world by means of systematic observation and experimentation, and linguistics is the scientific study of those aspects of the world that we summarize under the label language.\nAgain very broadly, these encompass, first, language systems (sets of linguistic elements and rules for combining them) as well as mental representations of these systems, and second, expressions of these systems (spoken and written utterances) as well as mental and motorsensory processes involved in the production and perception of these expressions.\nSome linguists study only the linguistic system, others study only linguistic expressions.\nSome linguists study linguistic systems as formal entities, others study them as mental representations.\nSome linguists study linguistic expressions in their social and/ or cultural contexts, others study them in the context of production and comprehension processes.\nEveryone should agree that whatever aspect of language we study and from whatever perspective we do so, if we are doing so scientifically, observation and experimentation should have a role to play.\nLet us define a corpus somewhat crudely as a large collection of authentic text (i.e., samples of language produced in genuine communicative situations), and corpus linguistics as any form of linguistic inquiry based on data derived from such a corpus.\nWe will refine these definitions in the next chapter to a point where they can serve as the foundation for a methodological framework, but they will suffice for now.\nDefined in this way, corpora clearly constitute recorded observations of language behavior, so their place in linguistic research seems so obvious that anyone unfamiliar with the last sixty years of mainstream linguistic theorizing will wonder why their use would have to be justified at all.\nI cannot think of any other scientific discipline whose textbook authors would feel compelled to begin their exposition by defending the use of observational data, and yet corpus linguistics textbooks often do exactly that.\nThe reasons for this defensive stance can be found in the history of the field, which until relatively recently has been dominated by researchers interested mainly in language as a formal system and/or a mental representation of such a system.\nAmong these researchers, the role of corpus data, and the observation of linguistic behavior more generally is highly controversial.\nWhile there are formalists who have discovered (or are beginning to discover) the potential of corpus data for their research, much of the formalist literature has been, and continues to be, at best dismissive of corpus data, at worst openly hostile.\nCorpus data are attacked as being inherently flawed in ways and to an extent that leaves them with no conceivable use at all in linguistic inquiry.\nIn this literature, the method proposed instead is that of intuiting linguistic data.\nPut simply, intuiting data means inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality (roughly, whether the sentence is a possible sentence of the language in question).\nTo put it mildly, inventing one's own data is a rather subjective procedure, so, again, anyone unfamiliar with the last sixty years of linguistic theorizing might wonder why such a procedure was proposed in the first place and why anyone would consider it superior to the use of corpus data.\nReaders familiar with this discussion or readers already convinced of the need for corpus data may skip this chapter, as it will not be referenced extensively in the remainder of this book.\nFor all others, a discussion of both issues -the alleged uselessness of corpus data and the alleged superiority of intuited data -seems indispensable, if only to put them to rest in order to concentrate, throughout the rest of this book, on the vast potential of corpus linguistics and the exciting avenues of research that it opens up.\nSection 1.1 will discuss four major points of criticisms leveled at corpus data.\nAs arguments against corpus data, they are easily defused, but they do point to aspects of corpora and corpus linguistic methods that must be kept in mind when designing linguistic research projects.\nSection 1.2 will discuss intuited data in more detail and show that it does not solve any of the problems associated (rightly or wrongly) with corpus data.\nInstead, as Section 1.3 will show, intuited data actually creates a number of additional problems.\nStill, intuitions we have about our native language (or other languages we speak well) can nevertheless be useful in linguistic research -as long as we do not confuse them with \"data\".\nArguments against corpus data\nThe four major points of criticism leveled at the use of corpus data in linguistic research are the following:\n1. corpora are usage data and thus of no use in studying linguistic knowledge; 2. corpora and the data derived from them are necessarily incomplete; Corpus data as usage data The first point of criticism is the most fundamental one: if corpus data cannot tell us anything about our object of study, there is no reason to use them at all.\nIt is no coincidence that this argument is typically made by proponents of generative syntactic theories, who place much importance on the distinction between what they call performance (roughly, the production and perception of linguistic expressions) and competence (roughly, the mental representation of the linguistic system).\nNoam Chomsky, one of the first proponents of generative linguistics, argued early on that the exclusive goal of linguistics should be to model competence, and that, therefore, corpora have no place in serious linguistic analysis: The speaker has represented in his brain a grammar that gives an ideal account of the structure of the sentences of his language, but, when actually faced with the task of speaking or \"understanding\", many other factors act upon his underlying linguistic competence to produce actual performance.\nHe may be confused or have several things in mind, change his plans in midstream, etc.\nSince this is obviously the condition of most actual linguistic performance, a direct record -an actual corpus -is almost useless, as it stands, for linguistic analysis of any but the most superficial kind\nThe assumption of a barrier between competence and performance is a central axiom in generative linguistics, which famously assumes that language acquisition depends on input only minimally, with an innate \"universal grammar\" doing\nIn any case, the gap between linguistic usage and linguistic knowledge would be an argument against corpus data only if there were a way of accessing linguistic knowledge directly and without the interference of other factors.\nSometimes, intuited data is claimed to fit this description, but as I will discuss in Section 1.2.1, not even Chomsky himself subscribes to this position.\n1.1 Arguments against corpus data The incompleteness of corpora\nNext, let us look at the argument that corpora are necessarily incomplete, also a long-standing argument in Chomskyan linguistics: [I]t is obvious that the set of grammatical sentences cannot be identified with any particular corpus of utterances obtained by the linguist in field work.\nAny grammar of a language will project the finite and somewhat accidental corpus of observed utterances to a set (presumably infinite) of grammatical utterances\nLet us set aside for now the problems associated with the idea of grammaticality and simply replace the word grammatical with conventionally occurring (an equation that Chomsky explicitly rejects).\nEven the resulting, somewhat weaker statement is quite clearly true, and will remain true no matter how large a corpus we are dealing with.\nCorpora are incomplete in at least two ways.\nFirst, corpora -no matter how large -are obviously finite, and thus they can never contain examples of every linguistic phenomenon.\nAs an example, consider the construction [it doesn't matter the N] (as in the lines It doesn't matter the colour of the car /\nBut\nwhat goes on beneath the bonnet from the Billy Bragg song A Lover Sings).\n(1) a. It doesn't matter the reasons people go and see a film as long as they go and see it.\n(thenorthernecho.co.uk)\nb. Remember, it doesn't matter the size of your garden, or if you live in a flat, there are still lots of small changes you can make that will benefit wildlife.\n(avonwildlifetrust.org.uk)\nHowever, the largest currently publicly available linguistic corpus of British English, the one-hundred-million-word British National Corpus, does not contain a single instance of this construction.\nThis is unlikely to be due to the fact that the construction is limited to an informal style, as the BNC contains a reasonable amount of informal language.\nInstead, it seems more likely that the construction is simply too infrequent to occur in a sample of one hundred million words of text.\nThus, someone studying the construction might wrongly conclude that it does not exist in British English on the basis of the BNC.\nSecond, linguistic usage is not homogeneous but varies across situations (think of the kind of variation referred to by terms such as dialect, sociolect, genre, register, style etc., which I will discuss in more detail in Section 2.1 below).\nClearly, it is, for all intents and purposes, impossible to include this variation in its entirety in a given corpus.\nThis is a problem not only for studies that are interested in linguistic variation but also for studies in core areas such as lexis and grammar: many linguistic patterns are limited to certain varieties, and a corpus that does not contain a particular language variety cannot contain examples of a pattern limited to that variety.\nFor example, the verb croak in the sense 'die' is usually used intransitively, but there is one variety in which it also occurs transitively.\nConsider the following representative examples:\n(2) a. Because he was a skunk and a stool pigeon ... I croaked him just as he was goin' to call the bulls with a police whistle ... (Veiller, Within the Law) b.\n[Use] your bean.\nIf I had croaked the guy and frisked his wallet, would I have left my signature all over it?\n(Stout, Some Buried Cesar)\nc.\nI recall pointing to the loaded double-barreled shotgun on my wall and replying, with a smile, that I would croak at least two of them before they got away.\nThe absence of meaning in corpora\nFinally, let us turn to the argument that corpora do not contain information about the semantics, pragmatics, etc. of the linguistic expressions they contain.\nLest anyone get the impression that it is only Chomskyan linguists who reject corpus data, consider the following statement of this argument by George Lakoff, an avowed anti-Chomskyan:\nCorpus linguistics can only provide you with utterances (or written letter sequences or character sequences or sign assemblages).\nTo do cognitive linguistics with corpus data, you need to interpret the data -to give it meaning.\nThe meaning doesn't occur in the corpus data.\nThus, introspection is always used in any cognitive analysis of language [...] Lakoff (and others putting forward this argument) are certainly right: if the corpus itself was all we had, corpus linguistics would be reduced to the detection of formal patterns (such as recurring combinations) in otherwise meaningless strings of symbols.\nThere are cases where this is the best we can do, namely, when dealing with documents in an unknown or unidentifiable language.\nAn example is the Phaistos disc, a clay disk discovered in 1908 in Crete.\nThe disc contains a series of symbols that appear to be pictographs (but may, of course, have purely phonological value), arranged in an inward spiral.\nThese pictographs may or may not\nIt hardly seems desirable to put ourselves in the position of a Phaistos disc scholar artificially, by excluding from our research designs our knowledge of English (or whatever other language our corpus contains); it is quite obvious that we should, as Intuition Intuited data would not be the only alternative to corpus data, but it is the one proposed and used by critics of the latter, so let us look more closely at this 1.2 Intuition practice.\nGiven the importance of grammaticality judgments, one might expect them to have been studied extensively to determine exactly what it is that people are doing when they are making such judgments.\nSurprisingly, this is not the case, and the few studies that do exist are hardly ever acknowledged as potentially problematic by those linguists that routinely rely on them, let alone discussed with respect to their place in scientific methodology.\nOne of the few explicit discussions is found in [A]mong the kinds of experiments that can be done on language, one kind is very simple, reliable, and cheap: simply present native speakers of a language with a sentence or phrase, and ask them to judge whether or not it is grammatical in their language or whether it can have some particular meaning.\nThis statement is representative of the general assumptions underlying the practice of grammaticality judgments in generative linguistics (and many other frameworks) in two ways: first, in that it presents individual grammaticality judgments as a kind of scientific experiment on a par with more sophisticated experiments, and second, in that it presents grammaticality judgments as a direct reflection of a speaker's mental representation of the language in question.\nJackendoff briefly touches upon a crucial problem of the first assumption: Ideally, we might want to check these experiments out by asking large numbers of people under controlled circumstances, and so forth.\nBut in fact the method is so reliable that, for a very good first approximation, linguists tend to trust their own judgments and those of their colleagues\nIt is certainly true that linguists trust their own judgments, but that does not mean, of course, that this trust is justified.\nThere is little evidence that individual grammaticality judgments are reliable: in the linguistic literature, grammaticality judgments of the same sentences by different authors often differ considerably and the few studies that have investigated the reliability of grammaticality judgments have consistently shown that such judgments display too much variation within and across speakers to use them as linguistic data (cf., e.g., The attraction of grammaticality judgments lies not so much in their reliability, then, but in the ease with which they can be collected, and Jackendoff is very explicit about this when he says that other kinds of experiments can be used to explore properties of the mental grammar\n[...] Their disadvantage is their relative inefficiency: it takes a great deal of time to set up the experiment.\nBy contrast, when the experiment consists of making judgments of grammaticality, there is nothing simpler than devising and judging some more sentences\nHowever, the fact that something can be done quickly and effortlessly does not make it a good scientific method.\nIf one is serious about using grammaticality judgments -and there are research questions that are not easily addressed without them -, then these judgments must be made as reliable as possible; among other things, this involves the two aspects mentioned by Jackendoff in passing: first, asking large numbers of speakers (or at least more than one) and, second, controlling the circumstances under which they are asked (cf.\nIn sum, there are serious problems with the reliability of linguistic intuition in general, a point I will briefly return to in Section 1.3.\nIn the case of isolated judgments by the researchers themselves, these problems are compounded by two additional ones: first, the researchers are language experts, whose judgments will hardly be representative of the average native speaker -as Ronald Langacker has quipped (in an example sentence meant to illustrate syntactic complexity): \"Linguists are no different from any other people who spend nineteen hours a day pondering the complexity of grammar [...]\"\nLet us return to the second assumption in the passage quoted above -that grammaticality judgments are transparently related to the mental grammar of the speaker producing them.\nIn particular, let us discuss whether intuited \"data\" fare better than corpus data in terms of the three major points of criticism discussed in the preceding section: Intuition as performance\nThe most fundamental point of criticism leveled against corpus data concerns the claim that since corpora are samples of language use (\"performance\"), they are useless in the study of linguistic knowledge (\"competence\").\nI argued in Section 1.1.1 above that this claim makes sense only in the context of rather implausible assumptions concerning linguistic knowledge and linguistic usage, but even if we accept these assumptions, the question remains whether intuited judgments are different from corpus data in this respect.\nIt seems obvious that both inventing sentences and judging their grammaticality are kinds of behavior and, as such, performance in the generative linguistics sense.\nIn fact, Chomsky himself admits this: [W]hen we study competence -the speaker-hearer's knowledge of his language -we may make use of his reports and his behavior as evidence, but we must be careful not to confuse \"evidence\" with the abstract constructs that we develop on the basis of evidence and try to justify in terms of evidence.\n, emphasis mine).\nThere is little to add to this statement, other than to emphasize that if it is possible to construct a model of linguistic competence on the basis of intuited judgments that involve factors other than competence, it should also be possible to do so on the basis of corpus data that involve factors other than competence, and the competence/performance argument against corpus data collapses.\nThe incompleteness of intuition\nNext, let us turn to the issue of incompleteness.\nAs discussed in Section 1.1.2, corpus data are necessarily incomplete, both in a quantitative sense (since every\nTo my knowledge, this issue has never been empirically addressed, and it would be difficult to do so, since there is no complete data set against which intuited judgments could be compared.\nHowever, it seems implausible to assume that such judgments are more complete than corpus data.\nFirst, just like a corpus, the linguistic experience of a speaker is finite and any mental generalizations based on this experience will be partial in the same way that generalizations based on corpus data must be partial (although it must be admitted that the linguistic experience a native speaker gathers over a lifetime exceeds even a large corpus like the BNC in terms of quantity).\nSecond, just like a corpus, a speaker's linguistic experience is limited to certain language varieties: most English speakers have never been to confession or planned an illegal activity, for example, which means they will lack knowledge of certain linguistic structures typical of these situations.\nTo exemplify this point, consider that many speakers of English are unaware of the fact that there is a use of the verb bring that has the valency pattern (or subcategorization frame) [bring NP LIQUID [ PP to the boil]] (in British English) or [bring NP LIQUID [ PP to a boil]] (in American English).\nThis use is essentially limited to a single genre, -recipes: of the 145 matches in the BNC, 142 occur in recipes and the remaining three in narrative descriptions of someone following a recipe.\nThus, a native speaker of English who never reads cookbooks or cookingrelated journals and websites and never watches cooking shows on television can go through their whole life without encountering the verb bring used in this way.\nWhen describing the grammatical behavior of the verb bring based on their intuition, this use would not occur to them, and if they were asked to judge the grammaticality of a sentence like Half-fill a large pan with water and bring to the boil [BNC A7D]\n, they would judge it ungrammatical.\nThus, this valency pattern would be absent from their description in the same way that transitive croak 'die' or [it doesn't matter the N] would be absent from a grammatical description based on the BNC (where, as we saw in Section 1.1.2, these patterns do not occur).\nIf this example seems too speculative, consider Culicover's analysis of the phrase no matter Intuitions about form and meaning\nFinally, let us turn to the question whether intuited \"data\" contain information about meaning.\nAt first glance, the answer to this question would appear to be an obvious \"yes\": if I make up a sentence, of course I know what that sentence means.\nHowever, a closer look shows that matters are more complex and the answer is less obvious.\nConstructing a sentence and interpreting a sentence are two separate activities.\nAs a consequence, I do not actually know what my constructed sentence means, but only what I think it means.\nWhile I may rightly consider myself the final authority on the intended meaning of a sentence that I myself have produced, my interpretation ceases to be privileged in this way once the issue is no longer my intention, but the interpretation that my constructed sentence would conventionally receive in a particular speech community.\nIn other words, the interpretation of a constructed sentence is subjective in the same way that the interpretation of a sentence found in a corpus is subjective.\nIn fact, interpreting other people's utterances, as we must do in corpus linguistic research, may actually lead to more intersubjectively stable results, as interpreting other people's utterances is a more natural activity than interpreting our own: the former is what we routinely engage in in communicative situations, the latter, while not exactly unnatural, is a rather exceptional activity.\nOn the other hand, it is very difficult not to interpret a sentence, but that is exactly what I would have to do in intuiting grammaticality judgments -judging a sentence to be grammatical or ungrammatical\nis supposed to be a judgment purely about form, dependent on meaning only insofar as that meaning is relevant to the grammatical structure.\nConsider the examples in (3): (3) a. When she'd first moved in she hadn't cared about anything, certainly not her surroundings -they had been the least of her problems -and if the villagers hadn't so kindly donated her furnishings she'd probably still be existing in empty rooms\n.\n(BNC H9V)\nThe structure in (3b) is a ditransitive, which is widely agreed to be impossible with donate (but see But the semantic considerations that increase or decrease our willingness to judge an utterance as grammatical are frequently more subtle than the difference between the readings in (3b) and (3c).\nConsider the example in (3d), which contains a clear example of donate with the supposedly ungrammatical ditransitive valency pattern.\nSince this is an authentic example, we cannot simply declare it ungrammatical; instead, we must look for properties that distinguish this example from more typical uses of donate and try to arrive an an explanation for such exceptional, but possible uses.\nIn Intuition data vs. corpus data As the preceding section has shown, intuited judgments are just as vulnerable as corpus data as far as the major points of criticism leveled at the latter are concerned.\nIn fact, I have tried to argue that they are, in some respects, more vulnerable to these criticisms.\nFor those readers who are not yet convinced of the need for corpus data, let me compare the quality of intuited \"data\" and corpus data in terms of two aspects that are considered much more crucial in methodological discussions outside of linguistics than those discussed above: 1. data reliability (roughly, how sure can we be that other people will arrive at the same set of data using the same procedures); 2. data validity or epistemological status of the data (roughly, how well do we understand what real world phenomenon the data correspond to);\nAs to the first criterion, note that the problem is not that intuition \"data\" are necessarily wrong.\nVery often, intuitive judgments turn out to agree very well with more objective kinds of evidence, and this should not come as a surprise.\nAfter all, as native speakers of a language, or even as advanced foreign-language speakers, we have considerable experience with using that language actively (speaking and writing) and passively (listening and reading).\nIt would thus be surprising if we were categorically unable to make statements about the probability of occurrence of a particular expression.\nInstead, the problem is that we have no way of determining introspectively whether a particular piece of intuited \"data\" is correct or not.\nTo decide this, we need objective evidence, obtained either by serious experiments (including elicitation experiments) or by corpus-linguistic methods.\nBut if that is the case, the question is why we need intuition \"data\" in the first place.\nIn other words, intuition \"data\" are simply not reliable.\nThe second criterion provides an even more important argument, perhaps the most important argument, against the practice of intuiting.\nNote that even if we manage to solve the problem of reliability (as systematic elicitation from a representative sample of speakers does to some extent), the epistemological status of intuitive data remains completely unclear.\nThis is particularly evident in the\nIn contrast, the epistemological status of a corpus datum is crystal clear: it is (a graphemic representation of) something that a specific speaker has said or written on a specific occasion in a specific situation.\nStatements that go beyond a specific speaker, a specific occasion or a specific situation must, of course, be inferred from these data; this is difficult and there is a constant risk that we get it wrong.\nHowever, inferring general principles from specific cases is one of the central tasks of all scientific research and the history of any discipline is full of inferences that turned out to be wrong.\nIntuited data may create the illusion that we can jump to generalizations directly and without the risk of errors.\nThe fact that corpus data do not allow us to maintain this illusion does not make them inferior to intuition, it makes them superior.\nMore importantly, it makes them normal observational data, no different from observational data in any other discipline.\nTo put it bluntly, then, intuition \"data\" are less reliable and less valid than corpus data, and they are just as incomplete and in need of interpretation.\nDoes this mean that intuition \"data\" should be banned completely from linguistics?\nThe answer is no, but not straightforwardly.\nOn the one hand, we would deprive ourselves of a potentially very rich source of information by dogmatically abandoning the use of our linguistic intuition (native-speaker or not).\nOn the other hand, given the unreliability and questionable epistemological status of intuition data, we cannot simply use them, as some corpus linguists suggest (e.g.\nThe solution to this problem is quite simple.\nWhile intuited information about linguistic patterns fails to meet even the most basic requirements for scientific Since there are no standards of purity for hypotheses, it is also unproblematic to mix intuition and corpus data in order to come up with more fine-grained hypotheses (cf. in this context Corpus data in other sub-disciplines of linguistics\nBefore we conclude our discussion of the supposed weaknesses of corpus data and the supposed strengths of intuited judgments, it should be pointed out that this discussion is limited largely to the field of grammatical theory.\nThis in itself would be surprising if intuited judgments were indeed superior to corpus evidence: after all, the distinction between linguistic behavior and linguistic knowledge is potentially relevant in other areas of linguistic inquiry, too.\nYet, no other sub-discipline of linguistics has attempted to make a strong case against observation and for intuited \"data\".\nIn some cases, we could argue that this is due to the fact that intuited judgments are simply not available.\nIn language acquisition or in historical linguistics, for example, researchers could not use their intuition even if they wanted to, since not even the most fervent defendants of intuited judgments would want to argue that speakers have meaningful intuitions about earlier stages of their own linguistic competence or their native language as a whole.\nFor language acquisition research, corpus data and, to a certain extent, psycholinguistic experiments are the only sources of data available, and historical linguists must rely completely on textual evidence.\nIn dialectology and sociolinguistics, however, the situation is slightly different: at least those researchers whose linguistic repertoire encompasses more than one dialect or sociolect (which is not at all unusual), could, in principle, attempt to use intuition data to investigate regional or social variation.\nTo my knowledge, however, nobody has attempted to do this.\nThere are, of course, descriptions of\nThe same is true of conversation and discourse analysis.\nOne could theoretically argue that our knowledge of our native language encompasses knowledge about the structure of discourse and that this knowledge should be accessible to introspection in the same way as our knowledge of grammar.\nHowever, again, no conversation or discourse analyst has ever actually taken this line of argumentation, relying instead on authentic usage data.\n6 Even lexicographers, who could theoretically base their descriptions of the meaning and grammatical behavior of words entirely on the introspectively accessed knowledge of their native language have not generally done so.\nBeginning with the Oxford English Dictionary (OED), dictionary entries have been based at least in part on citations -authentic usage examples of the word in question (see If the incompleteness of linguistic corpora or the fact that corpus data have to be interpreted were serious arguments against their use, these sub-disciplines of linguistics should not exist, or at least, they should not have yielded any useful insights into the nature of language change, language acquisition, language variation, the structure of linguistic interactions or the lexicon.\nYet all of these disciplines have, in fact, yielded insightful descriptive and explanatory models of their respective research objects.\nThe question remains, then, why grammatical theory is the only sub-discipline of linguistics whose practitioners have rejected the common practice of building models of underlying principles on careful analyses of observable phenomena.\nIf I were willing to speculate, I would consider the possibility that the rejection of corpora and corpus-linguistic methods in (some schools of) grammatical theorizing are based mostly on a desire to avoid having to deal with actual data, which are messy, incomplete and often frustrating, and that the arguments against the use of such data are, essentially, post-hoc rationalizations.\nBut\nwhatever\nthe case 6\nPerhaps Speech Act Theory could be seen as an attempt at discourse analysis on the basis of intuition data: its claims are often based on short snippets of invented conversations.\nThe difference between intuition data and authentic usage data is nicely demonstrated by the contrast between the relatively broad but superficial view of linguistic interaction found in philosophical pragmatics and the rich and detailed view of linguistic interaction found in Conversation Analysis (e.g. 2 What is corpus linguistics?\nAlthough corpus-based studies of language structure can look back on a tradition of at least a hundred years, there is no general agreement as to what exactly constitutes corpus linguistics.\nThis is due in part to the fact that the hundred-year tradition is not an unbroken one.\nAs we saw in the preceding chapter, corpora fell out of favor just as linguistics grew into an academic discipline in its own right and as a result, corpus-based studies of language were relegated to the margins of the field.\nWhile the work on corpora and corpus-linguistic methods never ceased, it has returned to a more central place in linguistic methodology only relatively recently.\nIt should therefore come as no surprise that it has not, so far, consolidated into a homogeneous methodological framework.\nMore generally, linguistics itself, with a tradition that reaches back to antiquity, has remained notoriously heterogeneous discipline with little agreement among researchers even with respect to fundamental questions such as what aspects of language constitute their object of study (recall the brief remarks at the beginning of the preceding chapter).\nIt is not surprising, then, that they do not agree how their object of study should be approached methodologically and how it might be modeled theoretically.\nGiven this lack of agreement, it is highly unlikely that a unified methodology will emerge in the field any time soon.\nOn the one hand, this heterogeneity is a good thing.\nThe dogmatism that comes with monolithic theoretical and methodological frameworks can be stifling to the curiosity that drives scientific progress, especially in the humanities and social sciences which are, by and large, less mature descriptively and theoretically than the natural sciences.\nOn the other hand, after more than a century of scientific inquiry in the modern sense, there should no longer be any serious disagreement as to its fundamental procedures, and there is no reason not to apply these procedures within the language sciences.\nThus, I will attempt in this chapter to sketch out a broad, and, I believe, ultimately uncontroversial characterization of corpus linguistics as an instance of the scientific method.\nI will develop this proposal by successively considering and dismissing alternative characterizations of corpus linguistics.\nMy aim in doing so is not to delegitimize these alternative characterizations, but to point out ways in which they are incomplete unless they are embedded in a principled set of ideas as to what it means to study language scientifically.\n2 What is corpus linguistics?\nLet us begin by considering a characterization of corpus linguistics from a classic textbook: Corpus linguistics is perhaps best described for the moment in simple terms as the study of language based on examples of \"real life\" language use.\nThe first chapter of this book started with a similar definition, characterizing corpus linguistics as \"as any form of linguistic inquiry based on data derived from [...] a corpus\", where corpus was defined as \"a large collection of authentic text\".\nIn order to distinguish corpus linguistics proper from other observational methods in linguistics, we must first refine this definition of a linguistic corpus; this will be our concern in Section 2.1.\nWe must then take a closer look at what it means to study language on the basis of a corpus; this will be our concern in Section 2.2.\nThe linguistic corpus\nThe term corpus has slightly different meanings in different academic disciplines.\nIt generally refers to a collection of texts; in literature studies, this collection may consist of the works of a particular author (e.g. all plays by William Shakespeare) or a particular genre and period (e.g. all 18th century novels); in theology, it may be (a particular translation of) the Bible.\nIn field linguistics, it refers to any collection of data (whether narrative texts or individual sentences) elicited for the purpose of linguistic research, frequently with a particular research question in mind (cf. In corpus linguistics, the term is used differently -it refers to a collection of samples of language use with the following properties: • the instances of language use contained in it are authentic; The linguistic corpus • the collection is representative of the language or language variety under investigation; • the collection is large.\nIn addition, the texts in such a collection are often (but not always) annotated in order to enhance their potential for linguistic analysis.\nIn particular, they may contain information about paralinguistic aspects of the original data (intonation, font style, etc.), linguistic properties of the utterances (parts of speech, syntactic structure), and demographic information about the speakers/writers.\nTo distinguish this type of collection from other collections of texts, we will refer to it as a linguistic corpus, and the term corpus will always refer to a linguistic corpus in this book unless specified otherwise.\nLet us now discuss each of these criteria in turn, beginning with authenticity.\nAuthenticity\nThe word authenticity has a range of meanings that could be applied to language -it can mean that a speaker or writer speaks true to their character (He has found his authentic voice), or to the character of the group they belong to (She is the authentic voice of her generation), that a particular piece of language is correctly attributed (This is not an authentic Lincoln quote), or that speech is direct and truthful (the authentic language of ordinary people).\nIn the context of corpus linguistics (and often of linguistics in general), authenticity refers much more broadly to what\nIn the case of written language, the criterion of authenticity is easy to satisfy.\nWriting samples can be collected after the fact, so that there is no way for the speakers to know that their language will come under scientific observation.\nIn the case of spoken language, the \"minimum disruption\" that Sinclair mentions becomes relevant.\nWe will return to this issue and its consequences for authenticity presently, but first let us discuss some general problems with the corpus linguist's broad notion of authenticity.\nThe texts which are collected in a corpus have a reflected reality: they are only real because of the presupposed reality of the discourses of which they are a trace.\nThis is decontexualized language, which is why it is only partially real.\nIf the language is to be realized as use, it has to be recontextualized.\nThis rather abstract point has very practical consequences, however.\nFirst, any text, spoken or written, will lose not only its communicative context (the discourse of which it was originally a part), but also some of its linguistic and paralinguistic properties when it becomes part of a corpus.\nThis is most obvious in the case of transcribed spoken data, where the very act of transcription means that aspects like tone of voice, intonation, subtle aspects of pronunciation, facial expressions, gestures, etc. are replaced by simplified descriptions or omitted altogether.\nIt is also true for written texts, where, for example, visual information about the font, its color and size, the position of the text on the page, and the tactile properties of the paper are removed or replaced by descriptions (see further Section 2.1.4 below).\nThe linguistic corpus\nThe corpus linguist can attempt to supply the missing information introspectively, \"recontextualizing\" the text, as Widdowson puts it.\nBut since they are not in an authentic setting (and often not a member of the same cultural and demographic group as the original or originally intended hearer/reader)\n, this recontextualization can approximate authenticity at best.\nSecond, texts, whether written or spoken, may contain errors that were present in the original production or that were introduced by editing before publication or by the process of preparing them for inclusion in the corpus (cf.\nalso This does not mean that corpora cannot be used.\nIt simply means that limits of authenticity have to be kept in mind.\nWith respect to spoken language, however, there is a more serious problem -Sinclair's \"minimum disruption\".\nThe problem is that in observational studies no disruption is ever\nminimalas soon as the investigator is present in person or in the minds of the observed, we get what is known as the \"observer's paradox\": we want to observe people (or other animate beings) behaving as they would if they were not observed -in the case of gathering spoken language data, we want to observe speakers interacting linguistically as they would if no linguist was in sight.\nIn some areas of study, it is possible to circumvent this problem by hiding (or installing hidden recording devices), but in the case of human language users this is impossible: it is unethical as well as illegal in most jurisdictions to record people without their knowledge.\nSpeakers must typically give written consent before the data collection can begin, and there is usually a recording device in plain view that will constantly remind them that they are being recorded.\nThis knowledge will invariably introduce a degree of inauthenticity into the data.\nTake the following excerpts from the Bergen Corpus of London Teenage Language (COLT).\nIn the excerpt in 2.1 The linguistic corpus the radio), and the researcher must rely, again, on an attempt to recontextualize the data based on their own experience as a language user in order to identify possible distortions.\nThere is no objective way of judging the degree of distortion introduced by the presence of an observer, since we do not have a sufficiently broad range of surreptitiously recorded data for comparison.\nThere is one famous exception to the observer's paradox in spoken language data: the so-called Nixon Tapes -illegal surreptitious recordings of conversation in the executive offices of the White House and the headquarters of the opposing Democratic Party produced at the request of the Republican President Richard Nixon between February 1971 and July 1973.\nMany of these tapes are now available as digitized sound files and/or transcripts (see, for example, However, even these recordings are too limited in size and topic area as well as in the diversity of speakers recorded (mainly older white American males), to serve as a standard against which to compare other collections of spoken data.\nThe ethical and legal problems in recording unobserved spoken language cannot be circumvented, but their impact on the authenticity of the recorded language can be lessened in various ways -for example, by getting general consent from speakers, but not telling them when precisely they will be recorded.\nResearchers may sometimes deliberately choose to depart from authenticity in the corpus-linguistic sense if their research design or the phenomenon under investigation requires it.\nA researcher may be interested in a phenomenon that is so rare in most situations that even the largest available corpora do not contain a sufficient number of cases.\nThese may be structural phenomena (like the pattern [It doesn't matter the N] or transitive croak, discussed in the previous chapter), or unusual communicative situations (for example, human-machine interaction).\nIn such cases, it may be necessary to switch methods and use some kind of grammaticality judgments after all, but it may also be possible to elicit these phenomena in what we could call semi-authentic settings.\nFor example, researchers interested in motion verbs often do not have the means (or the patience) to collect these verbs from general corpora, or corpora may not contain a sufficiently broad range of descriptions of motion events with particular properties.\nSuch descriptions are sometimes elicited by asking speakers to describe movie snippets or narrate a story from a picture book (cf. e.g. 2 What is corpus linguistics?\nSuch semi-structured elicitation techniques may also be used where a phenomenon is frequent enough in a typical corpus, but where the researcher wants to vary certain aspects systematically, or where the researcher wants to achieve comparability across speakers or even across languages.\nThese are sometimes good reasons for eliciting a special-purpose corpus rather than collecting naturally occurring text.\nStill, the stimulus-response design of elicitation is obviously influenced by experimental paradigms used in psychology.\nThus, studies based on such corpora must be regarded as falling somewhere between corpus linguistics and psycholinguistics and they must therefore meet the design criteria of both corpus linguistic and psycholinguistic research designs.\nRepresentativeness\nPut simply, a representative sample is a subset of a population that is identical to the population as a whole with respect to the distribution of the phenomenon under investigation.\nThus, for a corpus (a sample of language use) to be representative of a particular language, the distribution of linguistic phenomena (words, grammatical structures, etc.) would have to be identical to their distribution in the language as a whole (or in the variety under investigation, see further below).\nThe way that corpus creators typically aim to achieve this is by including in the corpus different manifestations of the language it is meant to represent in proportions that reflect their incidence in the speech community in question.\nSuch a corpus is sometimes referred to as a balanced corpus.\nBefore we can discuss this in more detail, a terminological note is in order.\nYou may have noted that in the preceding discussion I have repeatedly used terms like language variety, genre, register and style for different manifestations of language.\nThe precise usage of these terms notoriously vary across subdisciplines of linguistics and individual researchers, including the creators of corpora.\nIn this book, I use language variety to refer to any form of language delineable from other forms along cultural, linguistic or demographic criteria.\nIn other words, I use it as a superordinate term for text-linguistic terms like genre, register, style, and medium as well as sociolinguistic terms like dialect, sociolect, etc.\nWith respect to what I am calling text-linguistic terms here, I follow the usage suggestions synthesized by For a corpus to be representative (or \"balanced\"), its text categories should accurately reflect both quantitatively and qualitatively the language varieties found in the speech community whose language is represented in the corpus.\nHowever, it is clear that this is an ideal that is impossible to achieve in reality for at least four reasons.\nFirst, for most potentially relevant parameters we simply do not know how they are distributed in the population.\nWe may know the distribution of some of the most important demographic variables (e.g. sex, age, education), but we simply do not know the overall distribution of spoken vs. written language, press language vs. literary language, texts and conversations about particular topics, etc.\nSecond, even if we did know, it is not clear that all manifestations of language use shape and/or represent the linguistic system in the same way, simply because we do not know how widely they are received.\nFor example, emails may be responsible for a larger share of written language produced in a given time span than news sites, but each email is typically read by a handful of people at the most, while some news texts may be read by millions of people (and others not at all).\nThird, in a related point, speech communities are not homogeneous, so defining balance based on the proportion of language varieties in the speech community may not yield a realistic representation of the language even if it were possible: every member of the speech community takes part in different communicative situations involving different language varieties.\nSome people read more than others, and among these some read mostly newspapers, others mostly novels; some people watch parliamentary debates on TV all day, others mainly talk to customers in the bakery where they work.\nIn other words, the proportion of language varieties that speakers encounter varies, requiring a notion of balance based on the incidence of language varieties in the linguistic experience of a typical speaker.\nThis, in turn, requires a definition of what constitutes a typical speaker in a given speech community.\nSuch a definition may be possible, but to my knowledge, does not exist so far.\nFinally, there are language varieties that are impossible to sample for practical reasons -for example, pillow talk (which speakers will be unwilling to share because they consider it too private), religious confessions or lawyer-client conver-sations (which speakers are prevented from sharing because they are privileged), and the planning of illegal activities (which speakers will want to keep secret in order to avoid lengthy prison terms).\nRepresentativeness or balancedness also plays a role if we do not aim at investigating a language as a whole, but are instead interested in a particular variety.\nIn this case, the corpus will be deliberately skewed so as to contain only samples of the variety under investigation.\nHowever, if we plan to generalize our results to that variety as a whole, the corpus must be representative of that variety.\nThis is sometimes overlooked.\nFor example, there are studies of political rhetoric that are based on speeches by just a handful of political leaders (cf., e.g., Given the problems discussed above, it seems impossible to create a linguistic corpus meeting the criterion of representativeness.\nIn fact, while there are very well-thought out approaches to approximating representativeness (cf., e.g., The first linguistic corpus in our sense was the Brown University Standard Corpus of Present-Day American English (generally referred to as BROWN).\nIt is made up exclusively of edited prose published in the year 1961, so it clearly does not attempt to be representative of American English in general, but only of a particular kind of written American English in a narrow time span.\nThis is legitimate if the goal is to investigate that particular variety, but if the corpus were meant to represent the standard language in general (which the corpus creators explicitly deny), it would force us to accept a very narrow understanding of standard.\nThe BROWN corpus consists of 500 samples of approximately 2000 words each, drawn from a number of different language varieties, as shown in Table The first level of sampling is, roughly, by genre: there are 286 samples of nonfiction, 126 samples of fiction and 88 samples of press texts.\nThere is no reason to believe that this corresponds proportionally to the total number of words produced in these language varieties in the USA in 1961.\nThere is also no reason to believe that the distribution corresponds proportionally to the incidence of these language varieties in the linguistic experience of a typical speaker.\nThis is true all the more so when we take into account the second level of sampling within these 2.1 The linguistic corpus genres, which uses a mixture of sub-genres (such as reportage or editorial in the press category or novels and short stories in the fiction category), and topic areas (such as Romance, Natural Science or Sports).\nClearly the number of samples included for these categories is not based on statistics of their proportion in the language as a whole.\nIntuitively, there may be a rough correlation in some cases: newspapers publish more reportage than editorials, people (or at least academics like those that built the corpus) generally read more mystery fiction than science fiction, etc.\nThe creators of the BROWN corpus are quite open about the fact that their corpus design is not a representative sample of (written) American English.\nThey describe the collection procedure as follows: The selection procedure was in two phases: an initial subjective classification and decision as to how many samples of each category would be used, followed by a random selection of the actual samples within each category.\nIn most categories the holding of the Brown University Library and the Providence Athenaeum were treated as the universe from which the random selections were made.\nBut for certain categories it was necessary to go beyond these two collections.\nFor the daily press, for example, the list of American newspapers of which the New York Public Library keeps microfilms files was used (with the addition of the Providence Journal).\nCertain categories of chiefly ephemeral material necessitated rather arbitrary decisions; some periodical materials in the categories Skills and Hobbies and Popular Lore were chosen from the contents of one of the largest second-hand magazine stores in New York City.\nAlthough this sampling procedure is explicitly acknowledged to be \"subjective\" by the creators of the BROWN corpus, their description suggests that their design was guided by a general desire for balance: The list of main categories and their subdivisions was drawn up at a conference held at Brown University in February 1963.\nThe participants in the conference also independently gave their opinions as to the number of samples there should be in each category.\nThese figures were averaged to obtain the preliminary set of figures used.\nA few changes were later made on the basis of experience gained in making the selections.\nFiner subdivision was based on proportional amounts of actual publication during 1961.\nSecond, the procedure involves an attempt to capture the proportion of language varieties in actual publication -this proportion was determined on the basis of the American Book Publishing Record, a reference work containing publication information on all books published in the USA in a given year.\nWhether this is, in fact, a comprehensive source is unclear, and anyway, it can only be used in the selection of excerpts from books.\nBasing the estimation of the proportion of language varieties on a different source would, again, have yielded a very different corpus design.\nFor example, the copyright registrations for 1961 suggest that the category of periodicals is severely underrepresented relative to the category of books -there are roughly the same number of copyright registrations for the two language varieties, but there are one-and-a-half times as many excerpts from books as from periodicals in the BROWN corpus.\nDespite these shortcomings, the BROWN corpus set standards, inspiring a host of corpora of different varieties of English using the same design -for example, the Lancaster-Oslo/Bergen Corpus (LOB) containing British English from 1961, the Freiburg Brown (FROWN) and Freiburg LOB (FLOB) corpora of American and British English respectively from 1991, the Wellington Corpus of Written New Zealand English, and the Kolhapur Corpus (Indian English).\nThe success of the BROWN design was partly due to the fact that being able to study strictly comparable corpora of different varieties is useful regardless of their design.\nHowever, if the design had been widely felt to be completely off-target, 2 What is corpus linguistics? researchers would not have used it as a basis for the substantial effort involved in corpus creation.\nMore recent corpora at first glance appear to take a more principled approach to representativeness or balance.\nMost importantly, they typically include not just written language, but also spoken language.\nHowever, a closer look reveals that this is the only real change.\nFor example, the BNC Baby, a four-millionword subset of the 100-million-word British National Corpus (BNC), includes approximately one million words each from the text categories spoken conversation, written academic language, written prose fiction and written newspaper language (Table\nEven what I would consider the most serious approach to date to creating a balanced corpus design, the sampling schema of the International Corpus of English (ICE), is unlikely to be substantially closer to constituting a representative sample of English language use (see It puts a stronger emphasis on spoken language -sixty percent of the corpus are spoken text categories, although two thirds of these are public language use, while for most of us private language use is likely to account for more of our linguistic experience.\nIt also includes a much broader range of written text categories than previous corpora, including not just edited writing but also student writing and letters.\nLinguists would probably agree that the design of the ICE corpora is \"more representative\" than that of the BNC Baby, which is in turn \"more representative\" than that of the BROWN corpus and its offspring.\nHowever, in light of the above discussion of representativeness, there is little reason to believe that any of these corpora, or the many others that fall somewhere between BROWN and ICE, even come close to approximating a random sample of (a given variety of) English in terms of the text categories they contain and the proportions with which they are represented.\nThis raises the question as to why corpus creators go to the trouble of attempting to create representative corpora at all, and why some corpora seem to be more successful attempts than others.\nIt seems to me that, in fact, corpus creators are not striving for representativeness at all.\nThe impossibility of this task is widely acknowledged in corpus linguistics.\nInstead, they seem to interpret balance in terms of the related but distinct property diversity.\nWhile corpora will always be skewed relative to the overall population of texts and language varieties in a speech community, the undesirable effects of this skew can be alleviated by including in the corpus as broad a range of varieties as is realistic, either in general or in the context of a given research project.\nThe linguistic corpus Unless language structure and language use are infinitely variable (which, at a given point in time, they are clearly not), increasing the diversity of the sample will increase representativeness even if the corpus design is not strictly proportional to the incidence of text varieties or types of speakers found in the speech community.\nIt is important to acknowledge that this does not mean that diversity and representativeness are the same thing, but given that representative corpora are practically (and perhaps theoretically) impossible to create, diversity is a workable and justifiable proxy.\n2 What is corpus linguistics?\nSize\nLike diversity, corpus size is also assumed, more or less explicitly, to contribute to representativeness (e.g. A search for cooking in the main catalogue yields 7638 items that presumably include all cookbooks in the collection.\nThis means that cookbooks make up no more than 0.04 percent of printed English ( 7638 /19000000 = 0.000402).\nThus, they could quickly be lost in their entirety when the sample size drops substantially below the size of the population as a whole.\nAnd when a genre (or a language variety in general) goes missing from our sample, at least some linguistic phenomena will disappear along with it -such as the expression [bring NP LIQUID [ PP to the/a boil]], which, as discussed in Chapter 1, is exclusive to cookbooks.\nThere are several projects gathering very large corpora on a broader range of web-accessible text.\nThese corpora are certainly impressive in terms of their size, even though they typically contain mere billions rather than trillions of 2 What is corpus linguistics? words.\nHowever, their size is the only argument in their favor, as their creators and their users must not only give up any pretense that they are dealing with a representative corpus, but must contend with a situation in which they have no idea what texts and language varieties the corpus contains and how much of it was produced by speakers of English (or by human beings rather than bots).\nThese corpora certainly have their uses, but they push the definition of a linguistic corpus in the sense discussed above to their limit.\nTo what extent they are representative cannot be determined.\nOn the one hand, corpus size correlates with representativeness only to the extent that we take corpus diversity into account.\nOn the other hand, assuming (as we did above) that language structure and use are not infinitely variable, size will correlate with the representativeness of a corpus at least to some extent with respect to particular linguistic phenomena (especially frequent phenomena, such as general vocabulary, and/or highly productive processes such as derivational morphology and major grammatical structures).\nThere is no principled answer to the question \"How large must a linguistic corpus be?\", except, perhaps, an honest \"It is impossible to say\"\nCurrent corpora that at least make an honest attempt at diversity currently range from one million (e.g. the ICE corpora mentioned above) to about half a billion (e.g. the COCA mentioned in the preceding chapter).\nLooking at the published corpus-linguistic literature, my impression is that for most linguistic phenomena that researchers are likely to want to investigate, these corpus sizes seem sufficient.\nLet us take this broad range as characterizing a linguistic corpus for practical purposes.\nAnnotations\nMinimally, a linguistic corpus consists simply of a large, diverse collection of files containing authentic language samples as raw text, but more often than not, corpus creators add one or more of three broad types of annotation: 2.1 The linguistic corpus 1. information about paralinguistic features of the text such as font style, size and color, capitalization, special characters, etc. (for written texts), and intonation, overlapping speech, length of pauses, etc. (for spoken text); 2. information about linguistic features, such as parts of speech, lemmas or grammatical structure; 3. information about the producers of the text (speaker demographics like age, sex, education) or the circumstances of its production (genre, medium, situation).\nIn this section, we will illustrate these types of annotation and discuss their practical implications as well as their relation to the criterion of authenticity, beginning with paralinguistic features, whose omission was already hinted at as a problem for authenticity in Section 2.1.1 above.\nFor example, Figure\nIn contrast, consider the London-Lund Corpus of Spoken English (LLC), an excerpt from which is shown in Figure\nIn addition, however, intonation contours are recorded in detail preceding the vowel of the prosodically most prominent syllable using the equals sign and rightward and leftward slashes: = stands for \"level tone\", / for \"rise\", \\ for \"fall\", \\/ for \"(rise-)fall-rise\" and /\\ for \"(fall-)rise-fall\".\nA colon indicates that the following syllable is higher than the preceding one, an exclamation mark indicates that it is very high.\nOccasionally, the LLC uses phonetic transcription to indicate an unexpected pronunciation or vocalizations that have no standard spelling (like the [@:] in line 1410 which stands for a long schwa).\nThe two corpora differ in their use of symbols to annotate certain features, for example: • the LLC indicates overlap by asterisks and plus signs, the SBCSAE by square brackets, which, in turn, are used in the LLC to mark \"subordinate tone units\" or phonetic transcriptions; • the LLC uses periods and hyphens to indicate pauses, the SBCSAE uses only periods, with hyphens used to indicate that an intonation unit is truncated; • intonation units are enclosed by the symbols ^and # in the LLC and by line breaks in the SBCSAE; • lengthening is shown by an equals sign in the SBCSAE and by a colon following a vowel in the LLC.\nThus, even where the two corpora annotate the same features of speech in the transcriptions, they code these features differently.\nSuch differences are important to understand for anyone working with the these corpora, as they will influence the way in which we have to search the corpus (see further Section 4.1.1 below) -before working with a corpus, one should always read the full manual.\nMore importantly, such differences reflect different, sometimes incompatible theories of what features of spoken language are relevant, and at what level of detail.\nThe SBCSAE and the LLC cannot easily be combined into a larger corpus, since they mark prosodic features at very different levels of detail.\nThe LLC gives detailed information about pitch and intonation contours absent from the SBCSAE; in contrast, the SBCSAE contains information about volume and audible breathing that is absent from the LLC.\nWritten language, too, has paralinguistic features that are potentially relevant to linguistic research.\nConsider the excerpt from the LOB corpus in Figure\nAdditionally, the corpus contains markup pertaining not to the appearance of the text but to its linguistic properties.\nFor example, the word Mme in line 94 is an abbreviation, indicated in the corpus by the sequence \\0 preceding it.\nThis may not seem to contribute important information in this particular case, but it is useful where abbreviations end in a period (as they often do), because it serves to disambiguate such periods from sentence-final ones.\nSentence boundaries are also marked explicitly: each sentence begins with a caret symbol ^.\nThe linguistic corpus Other corpora (and other versions of the LOB corpus)\ncontain more detailed linguistic markup.\nMost commonly, they contain information about the word class of each word, represented in the form of a so-called \"part-of-speech (or POS) tags\".\nFigure Annotations of paralinguistic or linguistic features in a corpus impact its authenticity in complex ways.\nOn the one hand, including information concerning paralinguistic features makes a corpus more authentic than it would be if this information was simply discarded.\nAfter all, this information represents aspects of the original speech events from which the corpus is derived and is necessary to ensure a reconceptualization of the data that approximates these events as closely as possible.\nOn the other hand, this information is necessarily biased by the interests and theoretical perspectives of the corpus creators.\nBy splitting the spoken corpora into intonation units, for example, the creators assume that there are such units 2.1 The linguistic corpus and that they are a relevant category in the study of spoken language.\nThey will also identify these units based on particular theoretical and methodological assumptions, which means that different creators will come to different decisions.\nThe same is true of other aspects of spoken and written language.\nResearchers using these corpora are then forced to accept the assumptions and decisions of the corpus creators (or they must try to work around them).\nThis problem is even more obvious in the case of linguistic annotation.\nThere may be disagreements as to how and at what level of detail intonation should be described, for example, but it is relatively uncontroversial that it consists of changes in pitch.\nIn contrast, it is highly controversial how many parts of speech there are and how they should be identified, or how the structure even of simple sentences is best described and represented.\nAccepting (or working around) the corpus creators' assumptions and decisions concerning POS tags and annotations of syntactic structure may seriously limit or distort researcher's use of corpora.\nAlso, while it is clear that speakers are at some level aware of intonation, pauses, indentation, roman vs. italic fonts, etc., it is much less clear that they are aware of parts of speech and grammatical structures.\nThus, the former play a legitimate role in reconceptualizing authentic speech situations, while the latter arguably do not.\nNote also that while linguistic markup is often a precondition for an efficient retrieval of data, error in markup may hide certain phenomena systematically (see further\nFinally, corpora typically give some information about the texts they contain -so-called metadata.\nThese may be recorded in a manual, a separate computerreadable document or directly in the corpus files to which they pertain.\nTypical metadata are language variety (in terms of genre, medium topic area, etc., as described in Section 2.1.2 above), the origin of the text (for example, speaker/writer, year of production and or publication), and demographic information about the speaker/writer (sex, age, social class, geographical origin, sometimes also level of education, profession, religious affiliation, etc.).\nMetadata may also pertain to the structure of the corpus itself, like the file names, line numbers and sentence or utterance ids in the examples cited above.\nMetadata are also crucial in recontextualizing corpus data and in designing certain kinds of research projects, but they, too, depend on assumptions and choices made by corpus creators and should not be uncritically accepted by researchers using a given corpus.\nTowards a definition of corpus linguistics Having characterized the linguistic corpus in its ideal form, we can now reformulate the definition of corpus linguistics cited at the beginning of this chapter as follows:\nDefinition (First attempt) Corpus linguistics is the investigation of linguistic phenomena on the basis of linguistic corpora.\nThis definition is more specific with respect to the data used in corpus linguistics and will exclude certain variants of discourse analysis, text linguistics, and other fields working with authentic language data (whether such a strict exclusion is a good thing is a question we will briefly return to at the end of this chapter).\nHowever, the definition says nothing about the way in which these data are to be investigated.\nCrucially, it would cover a procedure in which the linguistic corpus essentially serves as a giant citation file, that the researcher scours, more or less systematically, for examples of a given linguistic phenomenon.\nThis procedure of basing linguistic analyses on citations has a long tradition in descriptive English linguistics, going back at least to Otto Jespersen's sevenvolume Modern English Grammar on Historical Principles\nA fairly stringent implementation of this method is described in the following passage from the FAQ web page of the Merriam-Webster Online Dictionary:\nEach day most Merriam-Webster editors devote an hour or two to reading a cross section of published material, including books, newspapers, magazines, and electronic publications; in our office this activity is called \"reading and marking. \"\nThe editors scour the texts in search of Towards a definition of corpus linguistics\nThe \"cross-section of published material\" referred to in this passage is heavily skewed towards particular varieties of formal written language.\nGiven that people will typically consult dictionaries to look up unfamiliar words they encounter in writing, this may be a reasonable choice to make, although it should be pointed out that modern dictionaries are often based on more diverse linguistic corpora.\nBut let us assume, for the moment, that the cross-section of published material read by the editors of Merriam Webster's dictionary counts as a linguistic corpus.\nGiven this assumption, the procedure described here clearly falls under our definition of corpus linguistics.\nInterestingly, the publishers of Merriam Webster's even refer to their procedure as \"study Collecting citations is perfectly legitimate.\nIt may serve to show that a particular linguistic phenomenon existed at a particular point in time -one reason for basing the OED on citations was and is to identify the first recorded use of each word.\nIt may also serve to show that a particular linguistic phenomenon exists at all, for example, if that phenomenon is considered ungrammatical (as in the case of [it doesn't matter the N], discussed in the previous chapter).\nHowever, the method of collecting citations cannot be regarded as a scientific method except for the purpose of proving the existence of a phenomenon, and hence does not constitute corpus linguistics proper.\nWhile the procedure described by the makers of Merriam Webster's sounds relatively methodical and organized, it is obvious that the editors will be guided in their selection by many factors that would be hard to control even if one were fully aware of them, such as their personal interests, their sense of esthetics, the intensity with which they have thought about some uses of a word as opposed to others, etc.\nThis can result in a substantial bias in the resulting data base even if the method is applied systematically, a bias that will be reflected in the results of the linguistic analysis, i.e. the definitions and example sentences in the dictionary.\nTo pick a random example: The word of the day on Merriam-Webster's website at the time of writing is implacable, defined as \"not capable of being appeased, significantly changed, or mitigated\" (Merriam-Webster, sv. implacable).\nThe entry gives two examples for the use of this word (cf. 4a, b), and the wordof-the-day message gives two more (shown in 4c, d in abbreviated form): (4) a. He has an implacable hatred for his political opponents.\nb.\nan implacable judge who knew in his bones that the cover-up extended to the highest levels of government\nExcept for hatred, the nouns modified by implacable in these examples are not at all representative of actual usage.\nThe lemmas most frequently modified by implacable in the 450-million-word Corpus of Contemporary American English (COCA) are enemy and foe, followed at some distance by force, hostility, opposition, will, and the hatred found in (4a).\nThus, it seems that implacable is used most frequently in contexts describing adversarial human relationships, while the examples that the editors of the Merriam-Websters selected as typical deal mostly with adversarial abstract forces.\nPerhaps this distortion is due to the materials the editors searched, perhaps the examples struck the editors as citation-worthy precisely because they are slightly unusual, or because they appealed to them esthetically (they all have a certain kind of rhetorical flourish).\n(5) a. implacable enemies b.\nThe government faces implacable opposition on the issue of nuclear waste.\n(LDCE, s.v. implacable)\nObviously, the method of citation collection becomes worse the more opportunistically the examples are collected: the researcher will not only focus on examples that they happen to notice, they may also selectively focus on examples that they intuitively deem particularly relevant or representative.\nIn the worst case, they will consciously perform an introspection-based analysis of a phenomenon and then scour the corpus for examples that support this analysis; we could call this method corpus-illustrated linguistics (cf. Towards a definition of corpus linguistics\nThe use of corpus examples for illustrative purposes has become somewhat fashionable among researchers who largely depend on introspective \"data\" otherwise.\nWhile it is probably an improvement over the practice of simply inventing data, it has a fundamental weakness: it does not ensure that the data selected by the researcher are actually representative of the phenomenon under investigation.\nIn other words, corpus-illustrated linguistics simply replaces introspectively invented data with introspectively selected data and thus inherits the fallibility of the introspective method discussed in the previous chapter.\nSince overcoming the fallibility of introspective data is one of the central motivations for using corpora in the first place, the analysis of a given phenomenon must not be based on a haphazard sample of instances that the researcher happened to notice while reading or, even worse, by searching the corpus for specific examples.\nThe whole point of constructing corpora as representative samples of a language or variety is that they will yield representative samples of particular linguistic phenomena in that language or variety.\nThe best way to achieve this is to draw a complete sample of the phenomenon in question, i.e. to retrieve all instances of it from the corpus (issues of retrieval are discussed in detail in Chapter 4).\nThese instances must then be analyzed systematically, i.e., according to a single set of criteria.\nThis leads to the following definition (cf.\nAs was mentioned in the preceding section, linguistic corpora are currently between one million and half a billion words in size, while web-based corpora can contain up to a trillion words.\nAs a consequence, it is usually impossible to extract a complete sample of a given phenomenon manually, and this has lead to a widespread use of computers and corpus linguistic software applications in the field.\nHowever, the usefulness of this approach is limited.\nIt is true that there are scientific disciplines that are so heavily dependent upon a particular technology that they could not exist without it -for example, radio astronomy (which requires a radio telescope) or radiology (which requires an x-ray machine).\nHowever, even in such cases we would hardly want to claim that the technology in question can serve as a defining criterion: one can use the same technology in ways that do not qualify as belonging to the respective discipline.\nFor example, a spy might use a radio telescope to intercept enemy transmissions, and an engineer may use an x-ray machine to detect fractures in a steel girder, but that does not make the spy a radio astronomer or the engineer a radiologist.\nClearly, even a discipline that relies crucially on a particular technology cannot be defined by the technology itself but by the uses to which it puts that technology.\nIf anything, we must thus replace the reference to corpus analysis software by a reference to what that software typically does.\nSoftware packages for corpus analysis vary in capability, but they all allow us to search a corpus for a particular (set of) linguistic expression(s) (typically word forms), by formulating a query using query languages of various degrees of abstractness and complexity, and they all display the results (or hits) of that query.\nSpecifically, most of these software packages have the following functions: 1. they produce KWIC (Key Word In Context) concordances, i.e. they display the hits for our query in their immediate context, defined in terms of a particular number of words or characters to the left and the right (see Figure 2.\nthey identify collocates of a given expression, i.e. word forms that occur in a certain position relative to the hits; these words are typically listed in the 2.2\nTowards a definition of corpus linguistics order of frequency with which they occur in the position in question (see Table 3.\nthey produce frequency lists, i.e. lists of all character strings in a given corpus listed in the order of their frequency of occurrence (see Table\nNote that concordancers differ with respect to their ability to deal with annotation -there are few standards in annotation, especially in older corpora and even the emerging XML-based standards, or wide-spread conventions like the column format shown in Figure\nLet us briefly look at why the three functions listed above might be useful in corpus linguistic research (we will discuss them in more detail in later chapters).\nA concordance provides a quick overview of the typical usage of a particular (set of) word forms or more complex linguistic expressions.\nThe occurrences are presented in random order in Figure Collocate lists are a useful way of summarizing the contexts of a linguistic expression.\nFor example, the collocate list in the column marked L1 in Table Finally, frequency lists provide useful information about the distribution of word forms (and, in the case of written language, punctuation marks) in a particular corpus.\nThis can be useful, for example, in comparing the structural properties or typical contents of different language varieties (see further\nGiven the widespread implementation of these three techniques, they are obviously central to corpus linguistics research, so we might amend the definition above as follows (a similar definition is implied by\nTwo problems remain with this definition.\nThe first problem is that the requirements of systematicity and completeness that were introduced in the second definition are missing.\nThis can be remedied by combining the second and third definition as follows:\nDefinition (Combined second and third attempt)\nCorpus linguistics is the complete and systematic investigation of linguistic phenomena on the basis of linguistic corpora using concordances, collocations, and frequency lists.\nThe second problem is that including a list of specific techniques in the definition of a discipline seems undesirable, no matter how central these techniques are.\nFirst, such a list will necessarily be finite and will thus limit the imagination of future researchers.\nSecond, and more importantly, it presents the techniques in question as an arbitrary set, while it would clearly be desirable to characterize them in terms that capture the reasons for their central role in the discipline.\nWhat concordances, collocate lists and frequency lists have in common is that they are all ways of studying the distribution of linguistic elements in a corpus.\nThus, we could define corpus linguistics as follows: Definition (Fourth attempt) Corpus linguistics is the complete and systematic investigation of the distribution of linguistic phenomena in a linguistic corpus.\nOn the one hand, this definition subsumes the previous two definitions:\nIf we assume that corpus linguistics is essentially the study of the distribution of linguistic phenomena in a linguistic corpus, we immediately understand the central role of the techniques described above: (i) KWIC concordances are a way of displaying the distribution of an expression across different syntagmatic contexts; (ii) collocation tables summarize the distribution of lexical items with respect to other lexical items in quantitative terms, and (iii) frequency lists summarize the overall quantitative distribution of lexical items in a given corpus.\nOn the other hand, the definition is not limited to these techniques but can be applied open-endedly on all levels of language and to all kinds of distributions.\nThis definition is close to the understanding of corpus linguistics that this book will advance, but it must still be narrowed down somewhat.\nFirst, it must not be misunderstood to suggest that studying the distribution of linguistic phenomena is an end in itself in corpus linguistics.\nHowever, while the statistical properties of language are a worthwhile and actively researched area, they are not the primary object of research in corpus linguistics.\nInstead, the definition just given captures an important aspect of a discipline referred to as statistical or stochastic natural language processing (Manning & Schütze 1999 is a good, if somewhat dense introduction to this field).\nStochastic natural language processing and corpus linguistics are closely related fields that have frequently profited from each other (see, e.g., Corpus linguistics, as its name suggests, is part of linguistics and thus focuses on linguistic research questions that may include, but are in no way limited to the stochastic properties of language.\nAdding this perspective to our definition, we get the following:\nDefinition (Fourth attempt, linguistic interpretation) Corpus linguistics is the investigation of linguistic research questions based on the complete and systematic analysis of the distribution of linguistic phenomena in a linguistic corpus.\nThis is a fairly accurate definition, in the sense that it describes the actual practice of a large body of corpus-linguistic research in a way that distinguishes it from similar kinds of research.\nIt is not suitable as a final characterization of corpus linguistics yet, as the phrase \"distribution of linguistic phenomena\" is still somewhat vague.\nThe next section will explicate this phrase.\nCorpus linguistics as a scientific method Say we have noticed that English speakers use two different words for the forward-facing window of a car: some say windscreen, some say windshield.\nIt is a genuinely linguistic question, what factor or factors explain this variation.\nIn line with the definition above, we would now try to determine their distribution in a corpus.\nSince the word is not very frequent, assume that we combine four corpora that we happen to have available, namely the BROWN, FROWN, LOB and FLOB corpora mentioned in Section 2.1.2 above.\nWe find that windscreen occurs 12 times and windshield occurs 13 times.\nThat the two words have roughly the same frequency in our corpus, while undeniably a fact about their distribution, is not very enlightening.\nIf our combined corpus were representative, we could at least conclude that neither of the two words is dominant.\nLooking at the grammatical contexts also does not tell us much: both words are almost always preceded by the definite article the, sometimes by a possessive pronoun or the indefinite article a.\nBoth words occur frequently in the PP [through NP], sometimes preceded by a verb of seeing, which is not surprising given that they refer to a type of window.\nThe distributional fact that the two words occur in very similar grammatical contexts is more enlightening: it suggests that we are, indeed, dealing with synonyms.\nHowever, it does not provide an answer to the question why there should be two words for the same thing.\nIt is only when we look at the distribution across the four corpora, that we find a possible answer: windscreen occurs exclusively in the LOB and FLOB corpora, while windshield occurs exclusively in the BROWN and FROWN corpora.\nThe first two are corpora of British English, the second two are corpora of American English; thus, we can hypothesize that we are dealing with dialectal variation.\nIn other words: we had to investigate differences in the distribution of linguistic phenomena under different conditions in order to arrive at a potential answer to our research question.\nTaking this into account, we can now posit the following final definition of corpus linguistics:\nDefinition (Final Version) Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.\nThe remainder of Part I of this book will expand this definition into a guideline for conducting corpus linguistic research.\nThe following is a brief overview.\nCorpus linguistics as a scientific method Any scientific research project begins, obviously, with the choice of an object of research -some fragment of reality that we wish to investigate -, and a research question -something about this fragment of reality that we would like to know.\nSince reality does not come pre-packaged and labeled, the first step in formulating the research question involves describing the object of research in terms of constructs -theoretical concepts corresponding to those aspects of reality that we plan to include.\nThese concepts will be provided in part by the state of the art in our field of research, including, but not limited to, the specific model(s) that we may choose to work with.\nMore often than not, however, our models will not provide fully explicated constructs for the description of every aspect of the object of research.\nIn this case, we must provide such explications.\nIn corpus linguistics, the object of research will usually involve one or more aspects of language structure or language use, but it may also involve aspects of our psychological, social or cultural reality that are merely reflected in language (a point we will return to in some of the case studies presented in Part II of this book).\nIn addition, the object of research may involve one or more aspects of extralinguistic reality, most importantly demographic properties of the speaker(s) such as geographical location, sex, age, ethnicity, social status, financial background, education, knowledge of other languages, etc.\nNone of these phenomena are difficult to characterize meaningfully as long as we are doing so in very broad terms, but none of them have generally agreed-upon definitions either, and no single theoretical framework will provide a coherent model encompassing all of them.\nIt is up to the researcher to provide such definitions and to justify them in the context of a specific research question.\nOnce the object of research is properly delineated and explicated, the second step is to state our research question in terms of our constructs.\nThis always involves a relationship between at least two theoretical constructs: one construct, whose properties we want to explain (the explicandum), and one construct that we believe might provide the explanation (the explicans).\nIn corpus linguistics, the explicandum is typically some aspect of language structure and/or use, while the explicans may be some other aspect of language structure or use (such as the presence or absence of a particular linguistic element, a particular position in a discourse, etc.), or some language external factor (such as the speaker's sex or age, the relationship between speaker and hearer, etc.).\nIn empirical research, the explicandum is referred to as the dependent variable and the explicans as the independent variable -note that these terms are actually quite transparent: if we want to explain X in terms of Y, then X must be (potentially) dependent on Y.\nEach of the variables must have at least two possible values.\nIn the simplest case, these values could be the presence vs. the absence of instances of the construct, in more complex cases, the values would correspond to different (classes of) instances of the construct.\nIn the example above, the dependent variable is Word for the Forward-Facing Window of a Car with the values windshield and windscreen; the independent variable is Variety of English with the values british and american (from now on, variables will be typographically represented by small caps with capitalization, their values will be represented by all small caps).\nThe third step in a research project is to derive a testable prediction from the hypothesis.\nCrucially, this involves defining our constructs in a way that allows us to measure them, i.e., to identify them reliably in our data.\nThis process, which is referred to as operationalization, is far from trivial, since even well-defined and agreed-upon aspects of language structure or use cannot be straightforwardly read off the data.\nWe will return to operationalization in detail in Chapter 3, Section 3.2.\nThe fourth step consists in collecting data -in the case of corpus linguistics, in retrieving them from a corpus.\nThus, we must formulate one or more queries that will retrieve all (or a representative sample of) cases of the phenomenon under investigation.\nOnce retrieved, the data must, in a fifth step, be categorized according to the values of the variables involved.\nIn the context of corpus linguistics, this means annotating them according to an annotation scheme containing the operational definitions.\nRetrieval and annotation are discussed in detail in Chapter 4.\nThe fifth and final step of a research project consists in evaluating the data with respect to our prediction.\nNote that in the simple example presented here, the conditional distribution is a matter of all-or-nothing: all instances of windscreen occur in the British part of the corpus and all instances of windshield occur in the American part.\nThere is a categorical difference between the two words with respect to the conditions under which they occur (at least in our corpora).\nIn contrast, the two words do not differ at all with respect to the grammatical contexts in which they occur.\nThe evaluation of such cases is discussed in\nNote also that many of the aspects that were proposed as defining criteria in previous definitions need no longer be included once we adopt our final version, since they are presupposed by this definition: conditional distributions (whether they differ in relative or absolute terms) are only meaningful if they are based on the complete data base (hence the criterion of completeness); conditional distributions can only be assessed if the data are carefully categorized according to the relevant conditions (hence the criterion of systematicity); distributions (especially relative ones) are more reliable if they are based on a large data set (hence the preference for large electronically stored corpora that are accessed via appropriate software applications); and often -but not always -the standard procedures for accessing corpora (concordances, collocate lists, frequency lists)\nare a natural step towards identifying the relevant distributions in the first place.\nHowever, these preconditions are not self-serving, and hence they cannot themselves form the defining basis of a methodological framework: they are only motivated by the definition just given.\nFinally, note that our final definition does distinguish corpus linguistics from other kinds of observational methods, such as text linguistics, discourse analysis, variationist sociolinguistics, etc., but it does so in a way that allows us to recognize the overlaps between these methods.\nThis is highly desirable given that these methods are fundamentally based on the same assumptions as to how language can and should be studied (namely on the basis of authentic instances of language use), and that they are likely to face similar methodological problems.\nCorpus linguistics as a scientific method\nAt the end of the previous chapter, we defined corpus linguistics as \"the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus\" and briefly discussed the individual steps necessary to conduct research on the basis of this discussion.\nIn this chapter, we will look in more detail at the logic and practice of formulating and testing research questions The scientific hypothesis\nBroadly speaking, there are two ways in which we can state our research question: first, in the form of an actual question such as \"Is there a relationship between X and Y?\" or \"What is the relationship between X and Y?\"; second, in the form of a specific hypothesis concerning the relationship between two variables, such as \"all X are Y\" or \"X leads to Y\".\nThe first way entails a relatively open-minded approach to our data.\nWe might have some general expectation of what we will find, but we would put them aside and simply start collecting observations and look for patterns.\nIf we find such patterns, we might use them to propose a provisional generalization, which we successively confirm, modify or replace on the basis of additional observations until we are satisfied that we have found the broadest generalization that our data will allow -this will then be the answer to our research question.\nThis so-called inductive approach was famously rejected by the Austrian-British philosopher Karl Popper for reasons that will become clear below, but after a period of disrepute it has been making a strong comeback in many disciplines in recent years due to the increasing availability of massive amounts of data and of tools that can search for correlations in these data within a reasonable time frame (think of the current buzz word \"big data\").\nSuch massive amounts of data allow us to take an extremely inductive approach -essentially just asking \"What relationships exist in my data?\" -and still arrive at reliable generalizations.\nOf course, matters are somewhat more complex, since, as discussed at the end of the previous chapter, theoretical constructs cannot directly be read off our data.\nBut the fact remains that, used in the right way, inductive research designs have their applications.\nIn corpus linguistics, large amounts of data have been available for some time (as mentioned in the previous chapter, the size even of corpora striving for some kind of balance is approaching half-a-billion words), and inductive approaches are used routinely and with insightful consequences\nThe second way of stating research questions entails a more focused way of approaching our data.\nWe state our hypothesis before looking at any data, and then limit our observations just to those that will help us determine the truth of this hypothesis (which is far from trivial, as we will see presently).\nThis so-called deductive approach is generally seen as the standard way of conducting research (at least ideally -actual research by actual people tends to be a bit messier even conceptually).\nWe will generally take a deductive approach in this book, but it will frequently include inductive (exploratory) excursions, as induction is a often useful in itself (for example, in situations where we do not know enough to state a useful working hypothesis or where our aim is mainly descriptive) or in the context of deductive research (where a first exploratory phase might involve inductive research as a way of generating hypotheses).\nWe will see elements of inductive research in some of the case studies in Part II of this book.\nStating hypotheses\nAs indicated above, scientific hypotheses are typically statements relating two variables, but in order to understand what makes such statements special, let us take a step back and look at the simpler statement in ( (1) The English language has a word for the forward-facing window of a car.\nLet us assume, for the moment, that we agree on the existence of something called car that has something accurately and unambiguously described by 'forward-facing window', and that we agree on the meaning of \"English\" and \"language X has a word for Y\".\nHow could we prove the statement in (1) to be true?\nThe scientific hypothesis There is only one way: we have to find the word in question.\nWe could, for example, describe the concept Forward-Facing Window of Car to a native speaker or show them a picture of one, and ask them what it is called (a method used in traditional dialectology and field linguistics).\nOr we could search a corpus for all passages mentioning cars and hope that one of them mentions the forward-facing window; alternatively, we could search for grammatical contexts in which we might expect the word to be used, such as ⟨ through the NOUN of POSS.PRON car ⟩ (see Section 4.1 in Chapter 4 on how such a query would have to be constructed).\nOr we could check whether other people have already found the word, for example by searching the definitions of an electronic dictionary.\nIf we find a word referring to the forward-facing window of a car, we have thereby proven its existence -we have verified the statement in But how could we falsify the statment, i.e., how could we prove that English does not have a word for the forward-facing window of a car?\nThe answer is simple: we can't.\nAs discussed extensively in Chapter 1, both native-speaker knowledge and corpora are necessarily finite.\nThus, if we ask a speaker to tell us what the forward-facing window of car is called and they don't know, this may be because there is no such word, or because they do not know this word (for example, because they are deeply uninterested in cars).\nIf we do not find a word in our corpus, this may be because there is no such word in English, or because the word just happens to be absent from our corpus, or because it does occur in the corpus but we missed it.\nIf we do not find a word in our dictionary, this may be because there is no such word, or because the dictionary-makers failed to include it, or because we missed it (for example, because the definition is phrased so oddly that we did not think to look for it -as in the Oxford English Dictionary, which defines windscreen somewhat quaintly as \"a screen for protection from the wind, now esp. in front of the driver's seat on a motor-car\" (OED, sv. windscreen)).\nNo matter how extensively we have searched for something (e.g. a word for a particular concept), the fact that we have not found it does not mean that it does not exist.\nThe statement in (\nThere are research questions that take the form of existential statements.\nFor example, in 2016 the astronomers Konstantin Batygin and Michael E. Brown proposed the existence of a ninth planet (tenth, if you cannot let go of Pluto) in our solar system\nBut if scientific hypotheses are not (or only rarely) existential statements, what are they instead?\nAs indicated at the end of the previous and the beginning of the current chapter, they are statements postulating relationships between constructs, rather than their existence.\nThe minimal model within which such a hypothesis can be stated is visualized schematically in the cross table (or contingency table) in Table\nThere must be (at least) two constructs, one of which we want to explain (the dependent variable), and one which we believe provides an explanation (the independent variable).\nEach variable has (at least) two values.\nThe dimensions of 3.1\nThe scientific hypothesis\nOur speculation concerning the distribution of the words windscreen and windshield, discussed in the previous chapter, essentially consists of the two universal statements, given in ( (2) All occurrences of the word windscreen are British English.\n(or, more formally, \"For all x, if x is the word windscreen then x is (a word of) British English\")\n(3) All occurrences of the word windshield are American English.\n(or, more formally, \"For all x, if x is the word windshield then x is (a word of) American English\")\nNote that the statements in ( How would we test (either one or both of) these hypotheses?\nNaively, we might attempt to verify them, as we would in the case of existential statements.\nThis attempt would be doomed, however, as If we treat the statements in ( If we were dealing with existential statements, this would be a plausible strategy and the results would tell us, that the respective words exist in the respective variety.\nHowever, with respect to the universal statements in ( Variety british american\nWhat we would have looked for in our naive attempt to verify our hypotheses are only those cases that should exist (i.e., the intersections indicated by checkmarks in Table\nObviously, we also have to look for those cases that should not exist (i.e., the intersections indicated by crosses in Table\nEven if we approach our data less naively and find that our data conform fully to the hypothesized distribution in Table First, the distribution could be due to some difference between the corpora other than the dialectal varieties they represent -it could, for example, be due to stylistic preferences of the authors, or the house styles of the publishing houses whose texts are included in the corpora.\nThere are, after all, only a handful of texts in LOB and BROWN that mention either of the two words at all (three in each corpus).\nThe scientific hypothesis Second, and more importantly, even if such confounding variables could be ruled out, no amount of data following the distribution in Table\nIn other words, we cannot verify the hypotheses in (\nThus, to test a scientific hypothesis, we have to specify cases that should not exist if the hypothesis were true, and then do our best to find such cases.\nAs Popper puts it: \"Every 'good' scientific theory is a prohibition: it forbids certain things to happen\", and \"[e]very genuine test of a theory is an attempt to falsify it, or to refute it\"\nThe harder we try to find such cases but fail to do so, the more certain we can be that our hypothesis is correct.\nBut no matter how hard we look, we must learn to accept that we can never be absolutely certain: in science, a \"fact\" is simply a hypothesis that has not yet been falsified.\nThis may seem disappointing, but science has made substantial advances despite (or perhaps because) scientists accept that there is no certainty when it comes to truth.\nIn contrast, a single counterexample will give us the certainty that our hypothesis is false.\nIncidentally, our attempts to falsify a hypothesis will often turn up evidence that appears to confirm it -for example, the more data we search in an attempt to find examples of the word windshield in British English, the more cases of windscreen we will come across.\nIt would be strange to disregard this confirming evidence, and even Popper does not ask us to: however, he insists that in order to count as confirming evidence (or \"corroborating evidence\", as he calls it), it must be the 3 Corpus linguistics as a scientific method result of \"a serious but unsuccessful attempt to falsify the theory\"\nIn our example, we would have to take the largest corpora of British and American English we can find and search them for counterexamples to our hypothesis (i.e., the intersections marked by crosses in Table Testing hypotheses: From counterexamples to probabilities\nWe have limited the discussion of scientific hypotheses to the simple case of universal statements so far, and in the traditional Popperian philosophy of science, these are the only statements that truly qualify as scientific hypotheses.\nIn corpus linguistics (and the social sciences more generally), hypotheses of this type are the exception rather than the norm -we are more likely to deal with statements about tendencies (think Most swans are white or Most examples of windscreen are British English), where the search for counterexamples is not a viable research strategy.\nThey may, however, inform corpus-based syntactic argumentation (cf. In the case of windscreen and windshield, we actually find counterexamples once we increase the sample size sufficiently, but there is still an overwhelming number of cases that follow our predictions.\nWhat do we make of such a situation?\nTake another well-known lexical difference between British and American English: the distilled petroleum used to fuel cars is referred to as petrol in British 3.1\nThe scientific hypothesis English and gasoline in American English.\nA search in the four corpora used above yields the frequencies of occurrence shown in Table\nNote that there are two problems with the strategy of checking counterexamples individually to determine whether they are genuine counterexample or not.\nFirst, we only checked the example that looked like a counterexample -we did not check all the examples that fit our hypothesis.\nHowever, these examples could, of course, also contain cases of misclassified data, which would lead to additional counterexamples.\nOf course, we could theoretically check all examples, 3 Corpus linguistics as a scientific method as there are only 42 examples overall.\nHowever, the larger our corpus is (and most corpus-linguistic research requires corpora that are much larger than the four million words used here), the less feasible it becomes to do so.\nThe second problem is that we were lucky, in this case, that the counterexample came from a novel by a well-known author, whose biographical information is easily available.\nBut linguistic corpora do not (and cannot) contain only well-known authors, and so checking the individual demographic data for every speaker in a corpus may be difficult to impossible.\nFinally, some language varieties cannot be attributed to a single speaker at all -political speeches are often written by a team of speech writers that may or may not include the person delivering the speech, newspaper articles may include text from a number of journalists and press agencies, published texts in general are typically proof-read by people other than the author, and so forth.\nLet us look at a more complex example, the words for the (typically elevated) paved path at the side of a road provided for pedestrians.\nDictionaries typically tell us, that this is called pavement in British English and sidewalk in American English, for example, the OALD: (5) a. pavement noun [...] 1 [countable] (British English) (North American English sidewalk) a flat part at the side of a road for people to walk on [OALD] b. sidewalk noun\nA query for the two words (in all their potential morphological and orthographic variants) against the LOB and FLOB corpora (British English) and BROWN and FROWN corpora (American English) yields the results shown in Table The scientific hypothesis\nIn the case of sidewalk, it seems at least possible that a closer inspection of the four cases in British English would show them to be only apparent counterexamples, due, for example, to misclassified texts.\nIn the case of the 22 cases of pavement in American English, this is less likely.\nLet us look at both cases in turn.\nHere are all four examples of sidewalk in British English, along with their author and title of the original source as quoted in the manuals of the corresponding corpora: (6) a.\nOne persistent taxi follows him through the street, crawling by the sidewalk...\n(LOB E09: Wilfrid T. F. Castle, Stamps of Lebanon's Dog River)\nb. \"Keep that black devil away from Rusty or you'll have a sick horse on your hands, \" he warned, and leaped to the wooden sidewalk.\n(LOB N07: Bert Cloos, Drury)\nc. There was a small boy on the sidewalk selling melons.\n(FLOB K24: Linda Waterman, Bad Connection.)\nd. Joe, my love, the snowflakes fell on the sidewalk.\n(FLOB K25: Christine McNeill, The Lesson.)\nNot much can be found about Wilfrid T.F. (Thomas Froggatt) Castle, other than that he wrote several books about postal stamps and about history, including the history of English parish churches, all published by British publishers.\nThere is a deceased estate notice under the name Wilfrid Thomas Froggatt Castle that gives his last address in Somerset (The Stationery Office 1999).\nIf this is the same person, it seems likely that he was British and that (6a) is a genuinely British English use of sidewalk.\nBert Cloos is the author of a handful of western novels with titles like Sangre India, Skirmish and Injun Blood.\nAgain, very little can be found out about him, but he is mentioned in the Los Angeles Times from\nFor the authors of (6c, d), Linda Waterman and Christine McNeill, no biographical information can be found at all.\nWaterman's story was published in a British student magazine, but this in itself is no evidence of anything.\nThe story is set in Latin America, so there may be a conscious effort to evoke American English.\nIn McNeill's case there is some evidence that she is British: she uses some words that are typically British, such as dressing gown (AmE (bath)robe) and breadbin (AmE breadbox), so it is plausible that she is British.\nLike Waterman's story, hers was published in a British magazine.\nInterestingly, however, the scene in which the word is used is set in the United States, so she, too, might be consciously evoking American English.\nTo sum up, we have one example that was likely produced by an American speaker, and three that were likely produced by British speakers, although two of these were probably evoking American English.\nWhich of these examples we may safely discount, however, remains difficult to say.\nTurning to pavement in American English, it would be possible to check the origin of the speakers of all 22 cases with the same attention to detail, but it is questionable that the results would be worth the time invested: as pointed out, it is unlikely that there are so many misclassified examples in the American corpora.\nOn closer inspection, however, it becomes apparent that we may be dealing with a different type of exception here: the word pavement has additional senses to the one cited in (5a) above, one of which does exist in American English.\nHere is the remainder of the relevant dictionary entry: (7) a. 2 [countable, uncountable] (British English) any area of flat stones on the ground b.\nEven with quite a large context, this example is compatible with a reading of pavement as 'road surface' or as 'pedestrian path'.\nIf it came from a British text, we would not hesitate to assign the latter reading, but since it comes from an American text (the novel Error of Judgment by the American author George Harmon Coxe), we might lean towards erring on the side of caution and annotate 3.1\nThe scientific hypothesis it as 'road surface'\n.\nAlas, the side of \"caution\" here is the side suggested by the very hypothesis we are trying to falsify -we would be basing our categorization circularly on what we are expecting to find in the data.\nA more intensive search of novels by American authors in the Google Books archive (which is larger than the BROWN corpus by many orders of magnitude), turns up clear cases of the word pavement with the meaning of sidewalk, for example, this passage from a novel by American author Mary Roberts Rinehart:\n(9) He had fallen asleep in his buggy, and had wakened to find old Nettie drawing him slowly down the main street of the town, pursuing an erratic but homeward course, while the people on the pavements watched and smiled.\n(Mary Roberts Rinehart, The Breaking Point, Ch. 10)\nSince this reading exists, then, we have found a counterexample to our hypothesis and can reject it.\nBut what does this mean for our data from the BROWN corpus -is there really nothing to be learned from this sample concerning our hypothesis?\nLet us say we truly wanted to err on the side of caution, i.e. on the side that goes against our hypothesis, and assign the meaning of sidewalk to Coxe's novel too.\nLet us further assume that we can assign all other uses of pavement in the sample to the reading 'paved surface', and that two of the four examples of sidewalk in the British English corpus are genuine counterexamples.\nThis would give us the distribution shown in Table\nThere are several reasons why the answer to this question must be \"no\".\nFirst, we can rarely say with any certainty whether we are dealing with true counterexamples or whether the apparent counterexamples are due to errors in the construction of the corpus or in our classification.\nThis turned out to be surprisingly difficult even with respect to a comparatively straightforward issue like the distribution of vocabulary across major dialectal boundaries.\nImagine how much more difficult it would have been with grammatical phenomena.\nFor example, the LOB corpus contains (10a): (10) a. We must not be rattled into surrender, but we must not -and I am not -be afraid of negotiation.\n(LOB A05) b. We must not be rattled into surrender, but we must not be -and I am not -afraid of negotiation.\n(11) It is, however, reported that the tariff on textiles and cars imported from the Common Market are to be reduced by 10 percent.\n(LOB A15)\nHere, the auxiliary be should agree with its singular subject tarrif, but instead, the plural form occurs.\nThere is no way to find out who wrote it and whether they intended to use the singular form but were confused by the embedded plural NP textiles and cars (a likely explanation).\nThus, we would have to discard it based on our intuition that it constitutes an error (the LOB creators actually mark it as such, but I have argued at length in Chapter 1 why this would defeat the point of using a corpus in the first place), or we would have to accept it as a counterexample to the generalization that singular subjects take singular verbs (which we are unlikely to want to give up based on a single example).\nIn theoretical terms, this may not be a definitive argument against the idea of falsification by counterexample.\nWe could argue that we simply have to make sure that there are no errors in the construction of our corpus and that we have to classify all hits correctly as constituting a genuine counterexample or not.\nHowever, in actual practice this is impossible.\nWe can (and must) try to minimize errors in our data and our classification, but we can never get rid of them completely (this is true not only in corpus-linguistics but in any discipline).\nThe scientific hypothesis\nSecond, even if our data and our classification were error-free, human behavior is less deterministic than the physical processes Popper had in mind when he elevated counterexamples to the sole acceptable evidence in science.\nEven in a simple case like word choice, there may be many reasons why a speaker may produce an exceptional utterance -evoking a variety other than their own (as in the examples above), unintentionally or intentionally using a word that they would not normally use because their interlocutor has used it, temporarily slipping into a variety that they used to speak as a child but no longer do, etc.\nWith more complex linguistic behavior, such as producing particular grammatical structures, there will be additional reasons for exceptional behavior: planning errors, choosing a different formulation in mid-sentence, tiredness, etc. -all the kinds of things classified as performance errors in traditional grammatical theory.\nIn other words, our measurements will never be perfect and speakers will never behave perfectly consistently.\nThis means that we cannot use a single counterexample (or even a handful of counterexamples) as a basis for rejecting a hypothesis, even if that hypothesis is stated in terms of a universal statement.\nHowever, as pointed out above, many (if not most) hypotheses in corpus linguistics do not take the form of universal statements (\"All X's are Y\", \"Z's always do Y\", etc.), but in terms of tendencies or preferences (\"X's tend to be Y\", \"Z's prefer Y\", etc.).\nFor example, there are a number of prepositions and/or adverbs in English that contain the morpheme -ward or -wards, such as afterward(s), backward(s), downward(s), inward(s), outward(s) and toward(s).\nThese two morphemes are essentially allomorphs of a single suffix that are in free variation: they have the same etymology (-wards simply includes a lexicalized genitive ending), they have both existed throughout the recorded history of English and there is no discernible difference in meaning between them\n.\nHowever, many dictionaries claim that the forms ending in -s are preferred in British English and the ones without the -s are preferred in American English.\nWe can turn this claim into a hypothesis involving two variables (Variety and Suffix Variant), but not one of the type \"All x are y\".\nInstead, we would have to state it along the lines of ( (12) Most occurrences of the suffix -wards are British English.\n(13) Most occurrences of the suffix -ward are American English.\nClearly, counterexamples are irrelevant to these statements.\nFinding an example like (14a) in a corpus of American English does not disprove the hypothesis that the use in (14b) would be preferred or more typical: 3 Corpus linguistics as a scientific method\nInstead, we have to state our prediction in relative terms.\nGenerally speaking, we should expect to find more cases of -wards than of -ward in British English and more of -ward than of -wards in American English, as visualized in Table\nThere is another issue that we must turn to first, though -that of defining our variables and their values in such a way that we can identify them in our data.\nWe saw even in the simple cases discussed above that this is not a trivial matter.\nFor example, we defined American English as \"the language occurring in the BROWN and FROWN corpora\", but we saw that the FROWN corpus contains at least one misclassified text by a British author, and we also saw that it is questionable to assume that all and only speakers of American English produce the language we would want to call \"American English\" (recall the uses of sidewalk by British speakers).\nThus, nobody would want to claim that our definition accurately reflects linguistic reality.\nSimilarly, we assumed that it was possible, in principle, to recognize which of several senses of a word (such as pavement) we are dealing with in a given instance from the corpus; we saw that this assumption runs into difficulties very quickly, raising the more general question of how 3.2 Operationalization to categorize instances of linguistic phenomena in corpora.\nThese are just two examples of the larger problem of operationalization, to which we will turn in the next section.\nOperationalization\nThe discussion so far has shown some of the practical challenges posed even by a simple construct like Variety with seemingly obvious values such as british and american.\nHowever, there is a more fundamental, and more challenging issue to consider: As hinted at in Section 3.1.1, we are essentially making (sets of) existential statements when we postulate such constructs.\nAll examples discussed above simply assumed the existence of something called \"British English\" and \"American English\", concepts that in turn presuppose the existence of something called \"English\" and of the properties \"British\" and \"American\".\nBut if we claim the existence of these constructs, we must define them; what is more, we must define them in a way that enables us (and others) to find them in the real world (in our case, in samples of language use).\nWe must provide what is referred to as operational definitions.\nOperational definitions\nPut simply, an operational definition of a construct is an explicit and unambiguous description of a set of operations that are performed to identify and measure that construct.\nThis makes operational definitions fundamentally different from our every-day understanding of what a definition is.\nTake an example from physics, the property Hardness.\nA typical dictionary definition of the word hard is the following (the abbreviations refer to dictionaries, see Study Notes to the current chapter): (15) 1 firm to touch firm, stiff, and difficult to press down, break, or cut [≠ soft] (LDCE, s.v. hard, cf. also the virtually identical definitions in CALD, MW and OALD)\nThis definition corresponds quite closely to our experiential understanding of what it means to be hard.\nHowever, for a physicist or an engineer interested in the hardness of different materials, it is not immediately useful: firm and stiff are simply loose synonyms of hard, and soft is an antonym -they do not help in understanding hardness, let alone in finding hardness in the real world.\nThe remainder of the definition is more promising: it should be possible to determine the hardness of a material by pressing it down, breaking or cutting it and noting how difficult this is.\nHowever, before, say, \"pressing down\" can be used as an operational definition, at least three questions need to be asked: first, what type of object is to be used for pressing (what material it is made of and what shape it has); second, how much pressure is to be applied; and third, how the \"difficulty\" of pressing down is to be determined.\nThere are a number of hardness tests that differ mainly along the answers they provide to these questions (cf. (16) HV = 0.102 × 𝐹 𝐴 F is the load in newtons, A is the surface of the indentation, and 0.102 is a constant that converts newtons into kilopond\n(this is necessary because the Vickers Hardness Test used to measure the test force in kilopond before the newton became the internationally recognized unit of force).\nUnlike the dictionary definition quoted above, Vickers Hardness (HV) is an operational definition of hardness: it specifies a procedure that leads to a number representing the hardness of a material.\nThis operational definition is partly motivated by our experiential understanding of hardness in the same way as (part of) the dictionary definition (\"difficult to press down\"), but in other aspects, it is arbitrary.\nFor example, one could use indenters that differ in shape or material, and indeed there are other widely used tests that do this: the Brinell Hardness Test uses a hardmetal ball with a diameter that may differ depending on the material to be tested, and the Knoop Hardness Test uses a diamond indenter with a rhombic-based pyramid shape\n.\nOne could also use a different measure of \"difficulty of pressing down\": for example, some tests use the rebound energy of an object dropped onto the material from a particular height.\nObviously, each of these tests will give a different result when applied to the same material, and some of them cannot be applied to particular materials (for example, materials that are too flexible for the indenter to leave an indentation, or materials that are so brittle that they will fracture during testing).\nMore crucially, none of them attempt to capture the \"nature\" of hardness; instead, they are meant Although matters are actually substantially more complicated\n, let us assume that this definition captures the essence of schizophrenia.\nAs a basis for diagnosis, it is useless.\nThe main problem is that \"understanding what is real\" is a mental process that cannot be observed or measured directly (a second problem is that everyone may be momentarily confused on occasion with regard to whether something is real or not, for example, when we are tired or drunk).\nIn psychiatry, mental disorders are therefore operationally defined in terms of certain behaviors.\nFor example, the fourth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV), used by psychiatrists and psychologists in the United States to diagnose schizophrenia, classifies an individual as schizophrenic if they (i) display at least two of the following symptoms: \"delusions\", \"hallucinations\", \"disorganized speech\", \"grossly disorganized or catatonic behavior\" and \"affective flattening\", \"poverty of speech\" or \"lack of motivation\"; and if they (ii) function \"markedly below the level achieved prior to the onset\" in areas \"such as work, interpersonal relations, or self-care\"; and if (iii) these symptoms can be observed over a period of at least one month and show effects over a period of at least six months; and if (iv) similar diagnoses (such as schizoaffective disorder) and substance abuse and medication can be ruled out\nThis definition of schizophrenia is much less objective than that of physical hardness, which is partly due to the fact that human behavior is more complex and less comprehensively understood than the mechanical properties of materials, and partly due to the fact that psychology and psychiatry are less mature disciplines than physics.\nHowever, it is an operational definition in the sense that it effectively presents a check-list of observable phenomena that is used to determine the presence of an unobservable phenomenon.\nAs in the case of hardness tests, there is no single operational definition -the International Statistical Classification of Diseases and Related Health Problems used by European psychologists and psychiatrists offers a different definition that overlaps with that of the DSM-IV but places more emphasis on (and is more specific with respect to) mental symptoms and less emphasis on social behaviors.\nAs should have become clear, operational definitions do not (and do not attempt to) capture the \"essence\" of the things or phenomena they define.\nWe cannot say that the Vickers Hardness number \"is\" hardness or that the DSM-IV list of symptoms \"is\" schizophrenia.\nThey are simply ways of measuring or diagnosing these phenomena.\nConsequently, it is pointless to ask whether operational definitions are \"correct\" or \"incorrect\" -they are simply useful in a particular context.\nHowever, this does not mean that any operational definition is as good as any other.\nA good operational definition must have two properties: it must be reliable and valid.\nA definition is reliable to the degree that different researchers can use it at different times and all get the same results; this objectivity (or at least intersubjectivity) is one of the primary motivations for operationalization in the first place.\nObviously, the reliability of operational definitions will vary depending on the degree of subjective judgment involved: while Vickers Hardness is extremely reliable, depending only on whether the apparatus is in good working order and the procedure is followed correctly, the DSM-IV definition of schizophrenia is much less reliable, depending, to some extent irreducibly, on the opinions and experience of the person applying it.\nEspecially in the latter case it is important to test the reliability of an operational definition empirically, i.e. to let different people apply it and see to what extent they get the same results (see further A definition is valid to the degree that it actually measures what it is supposed to measure.\nThus, we assume that there are such phenomena as \"hardness\" or \"schizophrenia\" and that they may be more or less accurately captured by an operational definition.\nValidity is clearly a very problematic concept: since phenomena can only be measured by operational definitions, it would be circular to assess the quality of the same definitions on the basis of these measures.\nOne indirect indication of validity is consistency (e.g., the phenomena identified by the definition share a number of additional properties not mentioned in the definition), but to a large extent, the validity of operationalizations is likely to be assessed on the basis of plausibility arguments.\nThe more complex and the less directly accessible a construct is, the more problematic the concept of validity becomes: While everyone would agree that there is such a thing as Hardness, this is much less clear in the case of Schizophrenia: it is not unusual for psychiatric diagnoses to be reclassified (for example, what was Asperger's syndrome in the DSM-IV became part of autism spectrum disorder in the DSM-V) or to be dropped altogether (as was the case with homosexuality, which was treated as a mental disorder by the DSM-II until 1974)\n.\nThus, operational definitions may 3.2 Operationalization create the construct they are merely meant to measure; it is therefore important to keep in mind that even a construct that has been operationally defined is still just a construct, i.e. part of a theory of reality rather than part of reality itself.\nExamples of operationalization in corpus linguistics Corpus linguistics is no different from other scientific disciplines: it is impossible to conduct any corpus-based research without operational definitions.\nHowever, this does not mean that researchers are necessarily aware that this is what they are doing.\nIn corpus-based research, we find roughly three different situations: 1. operational definitions may already be part of the corpus and be accepted (more or less implicitly) by the researcher, as is frequently the case with tokenization (which constitutes an operational definition of token that presupposes a particular theory of what constitutes a word), or with partof-speech tagging (which constitutes an operational definition of word classes), but also with metadata, including the corpus design itself (which typically constitutes a series of operational definitions of language varieties); 2. operational definitions may remain completely implicit, i.e. the researcher simply identifies and categorizes phenomena on the basis of their (professional but unspoken) understanding of the subject matter without any indication as to how they proceeded; 3. operational definitions and the procedure by which they have been applied may be explicitly stated.\nThere may be linguistic phenomena, whose definition is so uncontroversial that it seems justified to simply assume and/or apply it without any discussion at all -for example, when identifying occurrences of a specific word like sidewalk.\nBut even here, it is important to state explicitly which orthographic strings were searched for and why.\nAs soon as matters get a little more complex, implicitly applied definitions are unacceptable because unless we state exactly how we identified and categorized a particular phenomenon, nobody will be able to interpret our results correctly, let alone reproduce or replicate them or transfer them to a different set of data.\nFor example, the English possessive construction is a fairly simple and uncontroversial grammatical structure.\nIn written English it consists either of the sequence [NOUN 1 + ' + s + zero or more ADJECTIVEs + NOUN 2 ] (where the entire noun phrase that includes NOUN 1 is part of the construction) , or [NOUN 1 + ' + zero or more ADJECTIVEs + NOUN 2 ] (if the noun ends in s and is not a surname), or [POSS. PRONOUN + zero or more ADJECTIVEs + NOUN].\nThese sequences seem easy enough to identify in a corpus (or in a list of hits for appropriately constructed queries), so a researcher studying the possessive may not even mention how they defined this construction.\nWhile all of these cases have the form of the possessive construction and match the strings above, opinions may differ on whether they should be included in a sample of English possessive constructions.\nExample (18a) is a so-called possessive compound, a lexicalized possessive construction that functions like a conventional compound and could be treated as a single word.\nIn examples (18b and c), the possessive construction is a proper name.\nConcerning the latter: if we want to include it, we would have to decide whether also to include proper names where possessive pronoun and noun are spelled as a single word, as in MySpace (the name of an online social network now lost in history).\nExample (18d) is similar in that my God is used almost like a proper name; in addition, it is part of a fixed phrase.\nExample (18e) is a geographical name; here, the problem is that such names are increasingly spelled without an apostrophe, often by conscious decisions by government institutions (see These are just some of the problems we face even with a very simple grammatical structure.\nThus, if we were to study the possessive construction (or any other structure), we would have to state precisely which potential instances of a structure we include.\nIn other words, our operational definition needs to include a list of cases that may occur in the data together with a statement of whetherand why -to include them or not.\nLikewise, it may be plausible in certain contexts to use operational definitions already present in the data without further discussion.\nIf we accept graphemic or even orthographic representations of language (which corpus linguists do, most of the time), then we also accept some of the definitions that come along with orthography, for example concerning the question what constitutes a word.\nFor many research questions, it may be irrelevant whether the orthographic word correlates with a linguistic word in all cases (whether it does depends to a large extent on the specific linguistic model we adopt), so we may simply accept this correspondence as a pre-theoretical fact.\nBut there are research questions, for example concerning the mean length of clauses, utterances, etc., where this becomes relevant and we may have to define the notion of word in a different way.\nAt the very least, we should acknowledge that we are accepting a graphemic or orthographic definition despite the fact that it may not have a linguistic basis.\nSimilarly, there may be situations where we simply accept the part-of-speech tagging or the syntactic annotation in a corpus, but given that there is no agreedupon theory of word classes, let alone of syntactic structures, this can be problematic in some situations.\nAt the very least, it is crucial to understand that tagging and other kinds of annotation are the result of applying operational definitions by other researchers and if we use tags or other forms of annotation, we must familiarize ourselves with these definitions by reading the fine manuals that typically accompany the corpus.\nThese manuals and other literature provided by corpus creators must be read and cited like all other literature, and we must clarify in the description of our research design why and to what extent we rely on the operationalizations described in these materials.\nLet us look at five examples of frequently used corpus linguistic operationalizations that demonstrate various aspects of the issues sketched out above.\nParts of speech\nLet us begin with a brief discussion of tokenization and part-of-speech (POS) tagging, two phenomena whose operational definitions are typically decided on and applied by the corpus makers and implicitly accepted by the researchers using a corpus.\nWe saw an example of POS tagging in\nIn other cases, the categories themselves differ.\nFor example, in BROWN, all prepositions are labeled IN, while the BNC distinguishes of from other prepositions by labeling the former PRF and the latter PRP; FLOB has a special tag for the preposition for, IF; LOB labels all coordinating conjunctions CC, FLOB has a special tag for BUT, CCB.\nMore drastically, LOB and FLOB treat some sequences of orthographic words as multi-word tokens belonging to a single word class: in front of is treated as a preposition in LOB and FLOB, indicated by labeling all three words IN (LOB) and II (FLOB), with an additional indication that they are part of a sequence: LOB attaches straight double quotes to the second and third word, FLOB adds a 3 to indicate that they are part of a three word sequence and then a number indicating their position in the sequence.\nSuch tag sequences, called ditto tags make sense only if you believe that the individual parts in a multiword expression lose their independent word-class membership.\nEven then, we have to check very carefully, which particular multi-word sequences are treated like this and decide whether we agree.\nThe makers of BROWN and the BNC obviously had a more traditional view of word classes, simply treating in front of as a sequence of a preposition, a noun, and another preposition (BROWN) or specifically the subcategory of (BNC).\nDitto tags are a way of tokenizing the corpus at orthographic word boundaries while allowing words to span more than one token.\nBut tokenization itself also differs across corpora.\nFor example, BROWN tokenizes only at orthographic word boundaries (white space or punctuation), while the other three corpora also tokenize at clitic boundaries.\nThey all treat the n't in words like don't, doesn't, etc. as separate tokens, labeling it XNOT (LOB), XX (FLOB) and XX0 (BNC), while BROWN simply indicates that a word contains this clitic by attaching an asterisk to the end of the POS tag (other clitics, like 'll, 's, etc. are treated similarly).\nIt is clear, then, that tokenization and part-of-speech tagging are not inherent in the text itself, but are the result of decisions by the corpus makers.\nBut in what sense can these decisions be said to constitute operational definitions?\nThere are two different answers to this question.\nThe first answer is that the theories of tokenization and word classes are (usually) explicitly described in the corpus manual itself or in a guide as to how to apply the tag set.\nA good example of the latter is 3 Corpus linguistics as a scientific method\nAs an example, consider the instructions for the POS tags DT and JJ, beginning with the former: Determiner -DT This category includes the articles a(n), every, no and the, the indefinite determiners another, any and some, each, either (as in either way), neither (as in neither decision), that, these, this and those, and instances of all and both when they do not precede a determiner or possessive pronoun (as in all roads or both times).\n(Instances of all or both that do precede a determiner or possessive pronoun are tagged as predeterminers (PDT).)\nSince any noun phrase can contain at most one determiner, the fact that such can occur together with a determiner (as in the only such case) means that it should be tagged as an adjective (JJ), unless it precedes a determiner, as in such a good time, in which case it is a predeterminer (PDT).\nAdjective -JJ Hyphenated compounds that are used as modifiers are tagged as adjectives (JJ).\nEXAMPLES: happy-go-lucky/JJ one-of-a-kind/JJ run-of-the-mill/JJ Ordinal numbers are tagged as adjectives (JJ), as are compounds of the form n-th X-est, like fourth-largest.\nCC or DT When they are the first members of the double conjunctions both .\nOperationalization EXAMPLES:\nEither/DT child could sing.\nBut: Either/CC a boy could sing or/CC a girl could dance\n.\nEither/CC a boy or/CC a girl could sing.\nEither/CC a boy or/CC girl could sing.\nBe aware that either or neither can sometimes function as determiners (DT) even in the presence of or or nor.\nEXAMPLE:\nEither/DT boy or/CC girl could sing.\nHowever, POS tagging is not usually done by skilled, experienced annotators, bringing us to the second, completely different way in which POS tags are based on operational definitions.\nThe usual way in which corpora are annotated for parts of speech is by processing them using a specialized software application called tagger (a good example is the Tree Tagger\nPut simply, these taggers work as follows: For each word, they take into account the probabilities with which the word is tagged as A, B, C, etc., and the probability that a word tagged as A, B, C should occur at this point given the tag assigned to the preceding word.\nThe tagger essentially multiplies both probabilities and then chooses the tag with the highest joint probability.\nAs an example, consider the word cost in (20b), the beginning of which I repeat here: The wordform cost has a probability of 0.73 (73 percent) of representing a noun and a probability of 0.27 (27 percent) of representing a verb.\nIf the tagger simply went by these probabilities, it would assign the tag NN.\nHowever, the probability that modal verb is followed by a noun is 0.01 (1 percent), while the probability that it is followed by a verb is 0.8 (80 percent).\nThe tagger now multiplies the probabilities for noun (0.73 × 0.01 = 0.0072) and for verb (0.27 × 0.8 = 0.216).\nSince the latter is much higher, the tagger will tag the word (correctly, in this case, as a verb).\nBut how does the tagger know these probabilities?\nIt has to \"learn\" them from a corpus that has been annotated by hand by skilled, experienced annotators based on a reliable, valid annotation scheme.\nObviously, the larger this corpus, the more accurate the probabilities, the more likely that the tagger will be correct.\nI will return to this point presently, but first, note that in corpora which have been POS tagged automatically, the tagger itself and the probabilities it uses are the operational definition.\nIn terms of reliability, this is a good thing: If we apply the same tagger to the same text several times, it will give us the same result every time.\nIn terms of validity, this is a bad thing in two ways: first, because the tagger assigns tags based on learned probabilities rather than definitions.\nThis is likely to work better in some situations than in others, which means that incorrectly assigned tags will not be distributed randomly across parts of speech.\nFor example, the is unlikely to be tagged incorrectly, as it is always a determiner, but that is more likely to be tagged incorrectly, as it is a conjunction about two thirds of the time and a determiner about one third of the time.\nLikewise, horse is unlikely to be tagged incorrectly as it is a noun 99 percent of the time, but riding is more likely to be tagged incorrectly, as it is a noun about 15 percent of the time and a verb about 85 percent of the time.\nA sequence like the horse is almost certain to be tagged correctly, but a sequence like that riding much less so.\nWhat is worse, in the latter case, whether riding will be tagged correctly depends on whether that has been tagged correctly.\nIf that has been tagged as a determiner, riding will be (correctly) tagged as a noun, as verbs never follow determiners and the joint probability that it is a verb will be zero.\nIn contrast, if that has been tagged as a conjunction, the tagger will tag riding as a verb: conjunctions are followed by verbs with a probability of 0.16 and by nouns with a probability of 0.11, and so the joint probability that it is a verb (0.16 × 0.85 = 0.136) is higher than the joint probability that it is a noun (0.11 × 0.67 = 0.0165).\nThis will not always be the right decision, as (22) shows: In short, some classes of word forms (like ing-forms of verbs) are more difficult to tag correctly than others, so incorrectly assigned tags will cluster around such cases.\nThis can lead to considerable distortions in the tagging of specific words 3.2 Operationalization and grammatical constructions.\nFor example, in the BNC, the word form regard is systematically tagged incorrectly as a verb in the complex prepositions with regard to and in regard to, but is correctly tagged as a noun in most instances of the phrase in high regard.\nIn other words, particular linguistic phenomena will be severely misrepresented in the results of corpus queries based on automatically assigned tags or parse trees.\nSometimes the probabilities of two possible tags are very close.\nIn these cases, some taggers will stoically assign the more probable tag even if the difference in probabilities is small.\nOther taggers will assign so-called ambiguity or portmanteau tags, as in the following example from the BNC:\n(23) Ford/NP0-NN1 faces/NN2-VVZ strike/VVB-NN1 over/AVP-PRP pay/NN1-VVB deal/NN1-VVB ./PUN (BNC AAC)\nFirst, such cases must obviously be kept in mind when constructing queries: the query ⟨ VBB ⟩ will miss the word strike in this sentence (as will the query ⟨ NN1 ⟩).\nIn order to find words with ambiguity tags, we have to indicate that the ag we are interested in may be preceded or followed by another tag (one such way is provided by regular expressions, see Section 4.1 below).\nSecond, such cases demonstrate vividly why the two operational definitions of parts of speechby tagging guide line and by tagger -are fundamentally different: no human annotator, even one with a very sketchy tagging guideline, would produce the annotation in (23).\nOn the other hand, it is simply not feasible to annotate a 100-million-word corpus using human annotators (though advances in crowdsourcing technology may change this), so we are stuck with a choice between using a tagger or having no POS annotation at all.\nExisting taggers tend to have an accuracy of around 95 to 97 percent.\nFor example, it has been estimated This leaves 95 percent of the words in the corpus tagged correctly and unambiguously.\nAs impressive as this sounds at first, a closer look reveals two problems.\nFirst, an accuracy of 95 percent means that roughly one word in 20 is tagged incorrectly.\nAssuming a mean sentence length of 20 words (actual estimates range from 16 to 22), every sentence contains on average one incorrectly or ambiguously tagged word.\nSecond, as pointed out above, some (classes of) word forms are more difficult to tag correctly, so the five percent mistagged or ambiguously tagged words will not be spread randomly through the data.\nLength\nThere is a wide range of phenomena that has been claimed and/or shown to be related to the weight of linguistic units (syllables, words or phrases) -word-order phenomena following the principle \"light before heavy\", such as the dative alternation\nLet us begin with words.\nClearly, words differ in length -everyone would agree that the word stun is shorter than the word flabbergast.\nThere are a number of ways in which we could operationalize Word Length, all of which would allow us to confirm this difference in length: • as \"number of letters\" (cf., e.g., • as \"number of phonemes\" (cf., e.g., • as \"number of syllables\" (cf., e.g., While all three operationalizations give us comparable results in the case of these two words, they will diverge in other cases.\nTake disconcert, which has the same length as flabbergast when measured in terms of phonemes (it has nine; BrE /dɪskənsɜt/ and AmE /dɪskənsɝːt/) or syllables (three), but it is shorter when measured in terms of letters (ten).\nOr take shock, which has the same length as stun when measured in syllables (one), but is longer when measured in letters (5 vs. 4) and shorter when measured in phonemes Operationalization\nClearly, none of these three definitions is \"correct\" -they simply measure different ways in which a word may have (phonological or graphemic) length.\nWhich one to use depends on a number of factors, including first, what aspect of word length is relevant in the context of a particular research project (this is the question of validity), and second, to what extent are they practical to apply (this is the question of reliability).\nThe question of reliability is a simple one: \"number of letters\" is the most reliably measurable factor assuming that we are dealing with written language or with spoken language transcribed using standard orthography; \"number of phonemes\" can be measured less reliably, as it requires that data be transcribed phonemically (which leaves more room for interpretation than orthography) or, in the case of written data, converted from an orthographic to a phonemic representation (which requires assumptions about which the language variety and level of formality the writer in question would have used if they had been speaking the text); \"number of syllables\" also requires such assumptions.\nThe question of validity is less easy to answer: if we are dealing with language that was produced in the written medium, \"number of letters\" may seem like a valid measure, but writers may be \"speaking internally\" as they write, in which case orthographic length would play a marginal role in stylistic and/or processing-based choices.\nWhether phonemic length or syllabic length are the more valid measure may depend on particular research questions (if rhythmic considerations are potentially relevant, syllables are the more valid measure), but also on particular languages; for example, 3 Corpus linguistics as a scientific method glish (and other stress-timed languages) process them phonemically (in which case it depends on the phenomenon, which of the measures are more valid).\nWhen we want to measure the length of linguistic units above word level, e.g. phrases, we can choose all of the above methods, but additionally or instead we can (and more typically do) count the number of words and/or constituents (cf. e.g.\nAs mentioned at the beginning of this subsection, weight is sometimes understood to refer to structural complexity rather than length.\nThe question how to measure structural complexity has been addressed in some detail in the case of phrases, where it has been suggested that Complexity could be operationalized as \"number of nodes\" in the tree diagram modeling the structure of the phrase (cf. Structural complexity can also be operationalized at various levels for words.\nThe number of nodes could be counted in a phonological description of a word.\nFor example, two words with the same number of syllables may differ in the complexity of those syllables: amaze and astound both have two syllables, but the second syllable of amaze follows a simple CVC pattern, while that of astound has the much more complex CCVCC pattern.\nThe number of nodes could also be counted in the morphological structure of a word.\nIn this case, all of the words mentioned above would have a length of one, except disconcert, which has a length of 2 (dis + concert).\nOperationalization\nDue to the practical and theoretical difficulties of defining and measuring complexity, the vast majority of corpus-based studies operationalize Weight in terms of some measure of Word Length even if they theoretically conceptualize it in terms of complexity.\nSince complexity and length correlate to some extent, this is a justifiable simplification in most cases.\nIn any case, it is a good example of how a phenomenon and its operational definition may be more or less closely related.\nDiscourse status\nThe notion of \"topical\", \"old\", or \"given\" information plays an important role in many areas of grammar, such as pronominal reference, voice, and constituent order in general.\nDefinitions of this construct vary quite drastically across researchers and frameworks, but there is a simple basis for operational definitions of Topicality in terms of \"referential distance\", proposed by Talmy Givón: (24) Referential Distance\nWith respect to coding devices, it has to be specified whether only overt references (by lexical nouns, proper names and pronouns) are counted, or whether covert references (by structural and/or semantic positions in the clause that are not phonologically realized) are included, and if so, which kinds of covert references.\nWith respect to clauses, it has to be specified what counts as a clause, and it has to be specified how complex clauses are to be counted.\nA concrete example may demonstrate the complexity of these decisions.\nLet us assume that we are interested in determining the referential distance of the pronouns in the following example, all of which refer to the person named Joan (verbs and other elements potentially forming the core of a clause have been indexed with numbers for ease of reference in the subsequent discussion): (25) Joan, though Anne's junior 1 by a year and not yet fully accustomed 2 to the ways of the nobility, was 3 by far the more worldly-wise of the two.\nShe watched 4 , listened 5 , learned 6 and assessed 7 , speaking 8 only when spoken 9 to in general -whilst all the while making 10 her plans and looking 11 to the future...\nEnchanted 12 at first by her good fortune in becoming 13 Anne Mowbray's companion, grateful 14 for the benefits showered 15 upon her, Joan rapidly became 16 accustomed to her new role.\n(BNC CCD)\nLet us assume the traditional definition of a clause as a finite verb and its dependents and let us assume that only overt references are counted.\nIf we apply these definitions very narrowly, we would put the referential distance between the initial mention of Joan and the first pronominal reference at 1, as Joan is a dependent of was in clause (25 3 ) and there are no other finite verbs between this mention and the pronoun she.\nA broader definition of clause along the lines of \"a unit expressing a complete proposition\" however, might include the structures (25 1 ) (though Anne's junior by a year) and ( In fact, the structures (25 1 ) and ( The differences that decisions about covert mentions can make are even more obvious when calculating the referential distance of the second pronoun, her (in her plans).\nAgain, assuming that every finite verb and its dependents form a clause the distance between her and the previous use she is six clauses (25 4 to 25 9 ).\nHowever, in all six clauses, the logical subject is also Joan.\nIf we include these as mentions, the referential distance is 1 again (her good fortune is part of the clause (25 12 ) and the previous mention would be the covert reference by the logical subject of clause (25 11 )).\nFinally, note that I have assumed a very flat, sequential understanding of \"number of clauses\" counting every finite verb separately.\nHowever, one could argue that the sequence She watched 4 , listened 5 , learned 6 and assessed 7 is actually a single clause with four coordinated verb phrases sharing the subject she, that speaking 8 only when spoken 9 to in general is a single clause consisting of a matrix clause and an embedded adverbial clause, and that this clause itself is dependent 3.2 Operationalization on the clause with the four verb phrases.\nThus, the sequence from (25 4 ) to (25 9 ) can be seen as consisting of six, two or even just one clause, depending on how we decide to count clauses in the context of referential distance.\nObviously, there is no \"right\" or \"wrong\" way to count clauses; what matters is that we specify a way of counting clauses that can be reliably applied and that is valid with respect to what we are trying to measure.\nWith respect to reliability, obviously the simpler our specification, the better (simply counting every verb, whether finite or not, might be a good compromise between the two definitions mentioned above).\nWith respect to validity, things are more complicated: referential distance is meant to measure the degree of activation of a referent, and different assumptions about the hierarchical structure of the clauses in question are going to have an impact on our assumptions concerning the activation of the entities referred to by them.\nSince specifying what counts as a clause and what does not is fairly complex, it might be worth thinking about more objective, less theory-dependent measures of distance, such as the number of (orthographic) words between two mentions (I am not aware of studies that do this, but finding out to what extent the results correlate with clause-based measures of various kinds seems worthwhile).\nFor practical as well as for theoretical reasons, it is plausible to introduce a cutoff point for the number of clauses we search for a previous mention of a referent: practically, it will become too time consuming to search beyond a certain point, theoretically, it is arguable to what extent a distant previous occurrence of a referent contributes to the current information status.\nNote that, as an operational definition of \"topicality\" or \"givenness\", it will miss a range of referents that are \"topical\" or \"given\".\nFor example, there are referents that are present in the minds of speakers because they are physically present in the speech situation, or because they constitute salient shared knowledge for them, or because they talked about them at a previous occasion, or because they were mentioned prior to the cut-off point.\nSuch referents may already be \"given\" at the point that they are first mentioned in the discourse.\nConversely, the definition may wrongly classify referents as discourse-active.\nFor example, in conversational data an entity may be referred to by one speaker but be missed or misunderstood by the hearer, in which case it will not constitute given information to the hearer (Givón originally intended the measure for narrative data only, where this problem will not occur).\n3 Corpus linguistics as a scientific method Both Word Length and Discourse Status are phenomena that can be defined in relatively objective, quantifiable terms -not quite as objectively as physical Hardness, perhaps, but with a comparable degree of rigor.\nLike Hardness measures, they do not access reality directly and are dependent on a number of assumptions and decisions, but providing that these are stated sufficiently explicitly, they can be applied almost automatically.\nWhile Word Length and Discourse Status are not the only such phenomena, they are not typical either.\nMost phenomena that are of interest to linguists (and thus, to corpus linguists) require operational definitions that are more heavily dependent on interpretation.\nLet us look at two such phenomena, Word Sense and Animacy.\nWord senses\nAlthough we often pretend that corpora contain words, they actually contain graphemic strings.\nSometimes, such a string is in a relatively unique relationship with a particular word.\nFor example, sidewalk is normally spelled as an uninterrupted sequence of the character S or s followed by the characters i, d, e, w, a, l and k, or as an uninterrupted sequence of the characters S, I, D, E, W, A, L and K, so (assuming that the corpus does not contain hyphens inserted at the end of a line when breaking the word across lines), there are just three orthographic forms; also, the word always has the same meaning.\nThis is not the case for pavement, which, as we saw, has several meanings that (while clearly etymologically related), must be distinguished.\nIn these cases, the most common operationalization strategy found in corpus linguistics is reference to a dictionary or lexical database.\nIn other words, the researcher will go through the concordance and assign every instance of the orthographic string in question to one word-sense category posited in the corresponding lexical entry.\nA resource frequently used in such cases is the WordNet database (cf. (26) a. S: (n) pavement#1, paving#2 (the paved surface of a thoroughfare) b. S: (n) paving#1, pavement#2, paving material#1 (material used to pave an area) c. S: (n) sidewalk#1, pavement#3 (walk consisting of a paved area for pedestrians; usually beside a street or roadway) Operationalization\nThere are three senses of pavement, as shown by the numbers attached, and in each case there are synonyms.\nOf course, in order to turn this into an operational definition, we need to specify a procedure that allows us to assign the hits in our corpus to these categories.\nFor example, we could try to replace the word pavement by a unique synonym and see whether this changes the meaning.\nBut even this, as we saw in Section 3.1.2 above, may be quite difficult.\nThere is an additional problem: We are relying on someone else's decisions about which uses of a word constitute different senses.\nIn the case of pavement, this is fairly uncontroversial, but consider the entry for the noun bank: (27) a. bank#1 (sloping land (especially the slope beside a body of water)) b. bank#2, depository financial institution#1, bank#2, banking concern#1, banking company#1 (a financial institution that accepts deposits and channels the money into lending activities) c. bank#3 (a long ridge or pile)\nd. bank#4 (an arrangement of similar objects in a row or in tiers)\ne. bank#5 (a supply or stock held in reserve for future use (especially in emergencies))\nf. bank#6 (the funds held by a gambling house or the dealer in some gambling games)\ng. bank#7, cant#2, camber#2 (a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force)\nh. savings bank#2, coin bank#1, money box#1, bank#8 (a container (usually with a slot in the top) for keeping money at home) i. bank#9, bank building#1 (a building in which the business of banking transacted)\nj. bank#10 (a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning))\nWhile everyone will presumably agree that (27a) and (27b) are separate senses (or even separate words, i.e. homonyms), it is less clear whether everyone would distinguish (27b) from (27i) and/or (27f); or (27e) and (27f), or even (27a) and (27g).\nIn these cases, one could argue that we are just dealing with contextual variants of a single underlying meaning.\nThus, we have the choice of coming up with our own set of senses (which has the advantage that it will fit more precisely into the general theoretical framework we are working in and that we might find it easier to apply), or we can stick with an established set of senses such as that proposed by WordNet, which has the advantage that it is maximally transparent to other researchers and that we cannot subconsciously make it fit our own preconceptions, thus distorting our results in the direction of our hypothesis.\nIn either case, we must make the set of senses and the criteria for applying them transparent, and in either case we are dealing with an operational definition that does not correspond directly with reality (if only because word senses tend to form a continuum rather than a set of discrete categories in actual language use).\nAnimacy\nThe animacy of the referents of noun phrases plays a role in a range of grammatical processes in many languages.\nIn English, for example, it has been argued (and shown) to be involved in the grammatical alternations already discussed above, in other languages it is involved in grammatical gender, in alignment systems, etc.\nThe simplest distinction in the domain of Animacy would be the following: (28) animate vs. inanimate Dictionary definitions typically treat animate as a rough synonym of alive (OALD and CALD define it as \"having life\"), and inanimate as a rough synonym of not alive, normally in the sense of not being capable of having life, like, for example, a rock (\"having none of the characteristics of life that an animal or plant has\", CALD, see also OALD), but sometimes additionally in the sense of being no longer alive (\"dead or appearing to be dead\", OALD).\nThe basic distinction in (28) looks simple, so that any competent speaker of a language should be able to categorize the referents of nouns in a text accordingly.\nOn second thought, however, it is more complex than it seems.\nFor example, what about dead bodies or carcasses?\nThe fact that dictionaries disagree as to whether these are inanimate shows that this is not a straightforward question that calls for a decision before the nouns in a given corpus could be categorized reliably.\nLet us assume for the moment that animate is defined as \"potentially having life\" and thus includes dead bodies and carcasses.\nThis does not solve all problems: For example, how should body parts, organs or individual cells be categorized?\nThey \"have life\" in the sense that they are part of something alive, but they are not, in themselves, living beings.\nIn fact, in order to count as an animate being in a communicatively relevant sense, an entity has to display some degree of intentional agency.\nThis raises the question of whether, for example, plants, jellyfish, bacteria, viruses or prions should be categorized as animate.\nOperationalization\nSometimes, the dimension of intentionality/agency is implicitly recognized as playing a crucial role, leading to a three-way categorization such as that in (29): (29) human vs. other animate vs. inanimate\nIf Animacy is treated as a matter of degree, we might want to introduce further distinctions in the domain of animates, such as higher animals, lower animals, plants, micro-organisms.\nHowever, the distinction between humans and other animates introduces additional problems.\nFor example, how should we categorize animals that are linguistically represented as quasi-human, like the bonobo Kanzi, or a dog or a cat that is treated by their owner as though it has human intelligence?\nIf we categorize them as other animate, what about fictional talking animals like the Big Bad Wolf and the Three Little Pigs?\nAnd what about fully fictional entities, such as gods, ghosts, dwarves, dragons or unicorns?\nAre they, respectively, humans and animals, even though they do not, in fact exist?\nClearly, we treat them conceptually as such, so unless we follow an extremely objectivist semantics, they should be categorized accordingly -but this is not something we can simply assume implicitly.\nA slightly different problem is posed by robots (fictional ones that have quasihuman or quasi-animal capacities and real ones, that do not).\nShould these be treated as humans/animate?\nIf so, what about other kinds of \"intelligent\" machines (again, fictional ones with quasi-human capacities, like HAL 9000 from Arthur C. Clarke's Space Odyssey series, or real ones without such capacities, like the laptop on which I am writing this book)?\nAnd what about organizations (when they are metonymically treated as agents, and when they are not)?\nWe might want to categorize robots, machines and organizations as human/animate in contexts where they are treated as having human or animal intelligence and agency, and as inanimate where they are not.\nIn other words, our categorization of a referent may change depending on context.\nSometimes studies involving animacy introduce additional categories in the inanimate domain.\nOne distinction that is often made is that between concrete and abstract, yielding the four-way categorization in (30): (30) human vs. animate vs. concrete inanimate vs. abstract inanimate\nThe distinction between concrete and abstract raises the practical issue where to draw the line (for example, is electricity concrete?).\nIt also raises a deeper issue that we will return to: are we still dealing with a single dimension?\nAre abstract inanimate entities (say, marriage or Wednesday) really less \"animate\" than concrete entities like a wedding ring or a calendar?\nAnd are animate and abstract 3 Corpus linguistics as a scientific method incompatible, or would it not make sense to treat the referents of words like god, demon, unicorn, etc. as abstract animate?\nInterim summary\nWe have seen that operational definitions in corpus linguistics may differ substantially in terms of their objectivity.\nSome operational definitions, like those for length and discourse status, are almost comparable to physical measures like Vickers Hardness in terms of objectivity and quantitativeness.\nOthers, like those for word senses or animacy are more like the definitions in the DSM or the ICD in that they leave room for interpretation, and thus for subjective choices, no matter how precise the instructions for the identification of individual categories are.\nUnfortunately, the latter type of operational definition is more common in linguistics (and the social sciences in general), but there are procedures to deal with the problem of subjectiveness at least to some extent.\nWe will return to these procedures in detail in the next chapter.\nHypotheses in context:\nThe research cycle\nLet us conclude this chapter with a brief discussion of the role that hypothesis testing plays within a given strand of research, i.e., within the context of a set of research projects dealing with a particular research question (or a set of such questions), starting, again, with Karl Popper.\nPopper is sometimes portrayed as advocating an almost mindless version of falsificationism, where researchers randomly pull hypotheses out of thin air and test them until they are falsified, then start again with a new randomly invented hypothesis.\nPopper's actual discussions are closer to actual scientific practice.\nIt is true that in terms of scientific logic, the only requirement of a hypothesis is that it is testable (i.e., falsifiable), but in scientific practice, it must typically meet two additional criteria: first, it must be a potentially insightful explanation of a particular research problem, and second, it must take into account previous research (if such research exists).\nIt is also true that falsification is central to Popperian research logic, but not as a mindless slashing of ideas, but as a process of error elimination.\nPopper describes the entire process using the following schematic representation\nIn this schema, P 1 stands for a research question (a \"problem\"), TT stands for a hypothesis (a \"tentative theory\"), EE for the attempt to falsify the hypothesis\nHe explicitly acknowledges that falsification, while central, is not the only criterion by which science proceeds: if there are several unfalsified hypotheses, we may also assess them based on which promises the most insightful explanation or which produces the most interesting additional hypotheses\nCrucially, ( Fleshing out Popper's basic schema in (31) above, drawing together the points discussed in this and the previous chapter, we can represent this cycle as shown in Figure Research begins with a general question -something that intrigues an individual or a group of researchers.\nThe part of reality to which this question pertains is then modeled, i.e., described in terms of theoretical constructs, enabling us to formulate, first, a more specific research question, and often, second, a hypothesis.\nThere is nothing automatic about these steps -they are typically characterized by lengthy critical discussion, false starts or wild speculation, until testable hypotheses emerge (in some disciplines, this stage has not yet been, and in some cases probably never will be reached).\nNext, predictions must be derived, requiring operational definitions of the constructs posited previously.\nThis may require some back and forth between formulating predictions and providing sufficiently precise operationalizations.\nNext, the predictions must be tested -in the case of corpus linguistics, corpora must be selected and data must be retrieved and annotated, something we will discuss in detail in the next chapter.\nThen the data are analyzed with respect to the hypothesis.\nIf they corroborate the hypothesis (or at least fail to falsify it), this is not the end of the process: with Popper, we should only begin to accept evidence as corroborating when it emerges from repeated attempts to falsify the hypothesis.\nThus, additional tests must be, and typically are, devised.\nIf the results of any test falsify the hypothesis, this does not, of course, lead to its immediate rejection.\nAfter all, we have typically arrived at our hypothesis based on good arguments, and so researchers will typically perform what we could call a \"design check\" on their experiment, looking closely at their predictions to see if they really follow from the hypothesis, the operational definitions to see whether they are reliable and valid with respect to the constructs they represent, and the test itself to determine whether there are errors or confounding variables in the data selection and analysis.\nIf potential problems are found, they will be fixed\nThe repeated testing, and especially the modification of a hypothesis is inherently dangerous, as we might be so attached to our hypothesis that we will keep testing it long after we should have given it up, or that we will try to save it by changing it just enough that our test will no longer falsify it, or by making it completely untestable (cf.\nFinally, note that the scientific research cycle is not only incremental, with each new hypothesis and each new test building on previous research, but that it is also collaborative, with one researcher or group of researchers picking up where another left off.\nThis collaborative nature of research requires researchers to be maximally transparent with respect to their research designs, laying open their data and methods in sufficient detail for other researchers to understand exactly what prediction was tested, how the constructs in question were operationalized, how data were retrieved and analyzed.\nAgain, this is the norm in disciplines like experimental physics and psychology, but not so much so in the more humanities-leaning disciplines, which tend to put the focus on ideas and arguments rather than methods.\nWe will deal with data retrieval and annotation in the next chapter and return to the issue of methodological transparency at the end of it.\nData retrieval and annotation\nTraditionally, many corpus-linguistic studies use the (orthographic) word form as their starting point.\nThis is at least in part due to the fact that corpora consist of text that is represented as a sequence of word forms, and that, consequently, word forms are easy to retrieve.\nAs briefly discussed in Chapter 2, concordancing software allows us to query the corpus for a string of characters and displays the result as a list of hits in context.\nAs we saw when discussing the case of pavement in Chapter 3, a corpus query for a string of characters like ⟨ pavement ⟩ may give us more than we want -it will return not only hits corresponding to the word sense 'pedestrian footpath', which we could contrast with the synonym sidewalk, but also those corresponding to the word sense 'hard surface' (which we could contrast with the synonym paving).\nThe query may, at the same time, give us less than we want, because it would only return the singular form of the word and only if it is spelled entirely in lower case.\nA study of the word (in either or both of its senses) would obviously require that we look at the lemma PAVEMENT, comprising at least the word forms pavement (singular), pavements (plural) and, depending on how the corpus is prepared, pavement's (possessive).\nIt also requires that we include in our query all possible graphemic variants, comprising at least cases in lower case, with an initial capital (Pavement, Pavements, Pavement's, e.g. at the beginning of a sentence), or in all caps (PAVEMENT, PAVEMENTS, PAVEMENT'S), but, depending on the corpus, also hyphenated cases occurring at a line break (e.g. pave- ¶ment, with ¶ standing for the line break).\nIn Chapter 3, we implicitly treated the second issue as a problem of retrieval, noting in passing that we queried our corpus in such a way as to capture all variants of the lemma PAVEMENT.\nWe treated the first issue as a problem of categorization -we went through the results of our query one by one, determining from the context, which of the senses of pavement we were likely dealing with.\nIn the context of a research project, our decisions would be recorded together with the data in some way -we would annotate the data, using an agreed-upon code for each of the categories (e.g., word senses).\nRetrieval is a non-trivial issue even when dealing with individual lexical items whose orthographic representations are not ambiguous.\nThe more complex the phenomena under investigation are, the more complex these issues become, requiring careful thought and a number of decisions concerning an almost inevitable trade-off between the quality of the results and the time needed to retrieve them.\nThis issue will be dealt with in Section 4.1.\nWe already saw that the issue of data annotation is extremely complex even in the case of individual lexical items, and the preceding chapter discussed some more complicated examples.\nThis issue will be dealt with in more detail in Section 4.2.\nRetrieval\nBroadly speaking, there are two ways of searching a corpus for a particular linguistic phenomenon: manually (i.e., by reading the texts contained in it, noting down each instance of the phenomenon in question) or automatically (i.e., by using a computer program to run a query on a machine-readable version of the texts).\nAs discussed in Chapter 2, there may be cases where there is no readily apparent alternative to a fully manual search, and we will come back to such cases below.\nHowever, as also discussed in Chapter 2, software-aided queries are the default in modern corpus linguistics, and so we take these as a starting point of our discussion.\nCorpus queries\nThere is a range of more or less specialized commercial and non-commercial concordancing programs designed specifically for corpus linguistic research, and there are many other software packages that may be repurposed to the task of searching text corpora even though they are not specifically designed for corpuslinguistic research.\nFinally, there are scripting languages like Perl, Python and R, with a learning curve that is not forbiddingly steep, that can be used to write programs capable of searching text corpora (ranging from very simple two-liners to very complex solutions).\nWhich of these solutions are available to you and suitable to your research project is not for me to say, so the following discussion will largely abstract away from such specifics.\nThe power of software-aided searches depends on two things: first, on the annotation contained in the corpus itself and second, on the pattern-matching capacities of the software used to access them.\nIn the simplest case (which we 4.1 Retrieval assumed to hold in the examples discussed in the previous chapter), a corpus will contain plain text in a standard orthography and the software will be able to find passages matching a specific string of characters.\nEssentially, this is something every word processor is capable of.\nMost concordancing programs can do more than this, however.\nFor example, they typically allow the researcher to formulate queries that match not just one string, but a class of strings.\nOne fairly standardized way of achieving this is by using so-called regular expressions -strings that may contain not just simple characters, but also symbols referring to classes of characters or symbols affecting the interpretation of characters.\nFor example, the lexeme sidewalk, has (at least) six possible orthographic representations: sidewalk, side-walk, Sidewalk, Side-walk, sidewalks, side-walks, Sidewalks and Side-walks (in older texts, it is sometimes spelled as two separate words, which means that we have to add at least side walk, side walks, Side walk and Side walks when investigating such texts).\nIn order to retrieve all occurrences of the lexeme, we could perform a separate query for each of these strings, but I actually queried the string in (1a); a second example of regular expressions is (1b), which represents one way of searching for all inflected forms and spelling variants of the verb synthesize (as long as they are in lower case): (1) a.\nAny group of characters in square brackets is treated as a class, which means that any one of them will be treated as a match, and the question mark means \"zero or one of the preceding characters\".\nThis means that the pattern in (1a) will match an upper-or lower-case S, followed by i, d, and e, followed by zero or one occurrence of a hyphen or a blank space, followed by w, a, l, and k, followed by zero or one occurrence of s.\nThis matches all the variants of the word.\nFor (1b), the [sz] ensures that both the British spelling (with an s) and the American spelling (with a z) are found.\nThe question mark after e ensures that both the forms with an e (synthesize, synthesizes, synthesized) and that without one (synthesizing) are matched.\nNext, the string matches zero to one occurrence of a d or an s followed by zero or one occurrence of the string ing (because it is enclosed in parentheses, it is treated as a unit for the purposes of the following question mark.\nRegular expressions allow us to formulate the kind of complex and abstract queries that we are likely to need when searching for words (rather than individual word forms) and even more so when searching for more complex expressions.\nBut even the simple example in ( 4 Data retrieval and annotation they quickly overgeneralize.\nThe pattern would also, for example, match some non-existing forms, like synthesizding, and, more crucially, it will match existing forms that we may not want to include in our search results, like the noun synthesis (see further\nThe benefits of being able to define complex queries become even more obvious if our corpus contains annotations in addition to the original text, as discussed in Section 2.1.4 of Chapter 2.\nIf the corpus contains part-of-speech tags, for example, this will allow us to search (within limits) for grammatical structures.\nFor example, assume that there is a part-of-speech tag attached to the end of every word by an underscore (as in the BROWN corpus, see Figure An asterisk means \"zero or more\", a plus means \"one or more\", and \\S means \"any non-whitespace character\", the meaning of the other symbols is as before.\nThe pattern in (3) matches the following sequence: 1. any word (i.e., sequence of non-whitespace characters) tagged as a preposition, followed by 2. zero or one occurrence of a word tagged as an article that is preceded by a whitespace, followed by 3. zero or more occurrences of a word tagged as an adjective (again preceded by a whitespace), including comparatives and superlatives -note that the JJ tag may be followed by zero or one occurrence of a T or an R), followed by 4.1 Retrieval 4. one or more words (again, preceded by a whitespace) that are tagged as a noun or proper name (note the square bracket containing the N for common nouns and the P for proper nouns), including plural forms and possessive forms\n(note that NN or NP tags may be followed by zero or one occurrence of an S or a $).\nThe query in (3) makes use of the annotation in the corpus (in this case, the part-of-speech tagging), but it does so in a somewhat cumbersome way by treating word forms and the tags attached to them as strings.\nAs shown in Figure The two programs just mentioned share a query syntax called CQP (for \"Corpus Query Processor\") in the Corpus Workbench and CQL (for \"Corpus Query Language\") in the (No)Sketch Engine\n.\nThis syntax is very powerful, allowing us to query the corpus for tokens or sequences of tokens at any level of annotation.\nIt is also very transparent: each token is represented as a value-attribute pair in square brackets, as shown in (\nThe attribute refers to the level of annotation (e.g. word, pos, lemma or whatever else the makers of a corpus have called their annotations), the value refers to what we are looking for.\nFor example, a query for the different forms of the word synthesize (cf. ( (5) a.\nAs you can see, we can use regular expressions inside the values for the attributes, and we can use the asterisk, question mark and plus outside the token to indicate that the query should match \"zero or more\", \"zero or one\" and \"one or 4 Data retrieval and annotation more\" tokens with the specified properties.\nNote that CQP syntax is case sensitive, so for example (5a) would only return hits that are in lower case.\nIf we want the query to be case-insensitive, we have to attach %c to the relevant values.\nWe can also combine two or more attribute-value pairs inside a pair of square brackets to search for tokens satisfying particular conditions at different levels of annotation.\nFor example, (6a) will find all instances of the word form walk tagged as a verb, while (6b) will find all instances tagged as a noun.\nWe can also address different levels of annotation at different positions in a query.\nFor example, (6c) will find all instances of the word form walk followed by a word tagged as a preposition, and (6d) corresponds to the query ⟨ through the NOUN of POSS.PRON car ⟩ mentioned in Section 3.1.1 of Chapter 3 (note the %c that makes all queries for words case-insensitive\n): This query syntax is so transparent and widely used that we will treat it as a standard in the remainder of the book and use it to describe queries.\nThis is obviously useful if you are using one of the systems mentioned above, but if not, the transparency of the syntax should allow you to translate the query into whatever possibilities your concordancer offers you.\nWhen talking about a query in a particular corpus, I will use the annotation (e.g., the part-of-speech tags) used in that corpus, when talking about queries in general, I will use generic values like noun or prep., shown in lower case to indicate that they do not correspond to a particular corpus annotation.\nOf course, even the most powerful query syntax can only work with what is there.\nRetrieving instances of phrasal categories based on part-of-speech annotation is only possible to a certain extent: even a complex query like that in (5c) or in Retrieval\nOther problems are impossible to fix; for example, if the noun phrase inside the PP contains another PP, the pattern will not recognize it as belonging to the NP but will treat it as a new match and there is nothing we can do about this, since there is no difference between the sequence of POS tags in a structures like (7a), where the PP off the kitchen is a complement of the noun room and as such is part of the NP inside the first PP, and (7b), where the PP at a party is an adjunct of the verb standing and as such is not part of the NP preceding it\n: In order to distinguish these cases in a query, we need a corpus annotated not just for parts of speech but also for syntactic structure (sometimes referred to as a treebank), like the SUSANNE corpus briefly discussed in Section 2.1.4 of Chapter 2 above.\nPrecision and recall\nIn arriving at the definition of corpus linguistics adopted in this book, we stressed the need to investigate linguistic phenomena exhaustively, which we took to mean \"taking into account all examples of the phenomenon in question\" (cf. Chapter 2).\nIn order to take into account all examples of a phenomenon, we have to retrieve them first.\nHowever, as we saw in the preceding section and in Chapter 3, it is not always possible to define a corpus query in a way that will retrieve all and only the occurrences of a particular phenomenon.\nInstead, a query can have four possible outcomes: it may 1. include hits that are instances of the phenomenon we are looking for (these are referred to as a true positives or hits, but note that we are using the word hit in a broader sense to mean \"anything returned as a result of a query\"); 2. include hits that are not instances of our phenomenon (these are referred to as a false positives); 3. fail to include instances of our phenomenon (these are referred to as a false negatives or misses); or 4. fail to include strings that are not instances of our phenomenon (this is referred to as a true negative).\n4\nData retrieval and annotation Table\nWe can describe the quality of a data set that we have retrieved from a corpus in terms of two measures.\nFirst, the proportion of positives (i.e., strings returned by our query) that are true positives; this is referred to as precision, or as the positive predictive value, cf. (8a).\nSecond, the proportion of all instances of our phenomenon that are true positives (i.e., that were actually returned by our query; this is referred to as recall, cf. (8b): 1 (8) a. Precision = True Positives True Positives + False Positives 1\nThere are two additional measures that are important in other areas of empirical research but do not play a central role in corpus-linguistic data retrieval.\nFirst, the specificity or true negative rate -the proportion of negatives that are incorrectly included in our data (i.e. false negatives); second, negative predictive value -the proportion of negatives (i.e., cases not included in our search) that are true negatives (i.e., that are correctly rejected).\nThese measures play a role in situations where a negative outcome of a test is relevant (for example, with medical diagnoses); in corpus linguistics, this is generally not the case.\nThere are also various scores that combine individual measures to give us an overall idea of the accuracy of a test, for example, the F1 score, defined as the harmonic mean of precision and recall.\nSuch scores are useful in information retrieval or machine learning, but less so in corpus-linguistic research projects, where precision and recall must typically be assessed independently of, and weighed against, each other.\nIdeally, the value of both measures should be 1, i.e., our data should include all cases of the phenomenon under investigation (a recall rate of 100 percent) and it should include nothing that is not a case of this phenomenon (a precision of 100 percent).\nHowever, unless we carefully search our corpus manually (a possibility I will return to below), there is typically a trade-off between the two.\nEither we devise a query that matches only clear cases of the phenomenon we are interested in (high precision) but that will miss many less clear cases (low recall).\nOr we devise a query that matches as many potential cases as possible (high recall), but that will include many cases that are not instances of our phenomenon (low precision).\nLet us look at a specific example, the English ditransitive construction, and let us assume that we have an untagged and unparsed corpus.\nHow could we retrieve instances of the ditransitive?\nAs the first object of a ditransitive is usually a pronoun (in the objective case) and the second a lexical NP (see, for example, (9) a. me, Let us apply this query (which is actually used in\nThere are 36 ditransitive clauses in the sample, thirteen of which are returned by our query.\nThere are also 2838 clauses that are not ditransitive, 14 of which are also returned by our query.\nTable 4\nData retrieval and annotation\nWe can now calculate precision and recall rate of our query: Clearly, neither precision nor recall are particularly impressive.\nLet us look at the reasons for this, beginning with precision.\nWhile the sequence of a pronoun and a determiner is typical for (one type of) ditransitive clause, it is not unique to the ditransitive, as the following false positives of our query show: (11) a. ... one of the experiences that went towards making me a Christian...\nb.\nI still ring her a lot.\nc. I told her that I'd had to take these tablets d.\nIt seems to me that they they tend to come from e. Do you need your caffeine fix before you this\nTwo other typical structures characterized by the sequence pronoun-determiner are object-complement clauses (cf. 11a) and clauses with quantifying noun phrases (cf. 11b).\nIn addition, some of the strings in (9b) above are ambiguous, i.e., they can represent parts of speech other than determiner; for example, that can be a conjunction, as in (9c), which otherwise fits the description of a ditransitive, and in (11d), which does not.\nFinally, especially in spoken corpora, there may be fragments that match particular search criteria only accidentally (cf. 11e).\nObviously, a corpus tagged for parts of speech could improve the precision of our search results somewhat, by excluding cases like (9c-d), but others, like (9a), 4.1 Retrieval could never be excluded, since they are identical to the ditransitive as far as the sequence of parts-of-speech is concerned.\nOf course, it is relatively trivial, in principle, to increase the precision of our search results: we can manually discard all false positives, which would increase precision to the maximum value of 1.\nTypically, our data will have to be manually annotated for various criteria anyway, allowing us to discard false positives in the process.\nHowever, the larger our data set, the more time consuming this will become, so precision should always be a consideration even at the stage of data retrieval.\nLet us now look at the reasons for the recall rate, which is even worse than the precision.\nThere are, roughly speaking, four types of ditransitive structures that our query misses, exemplified in (12a-e):\nThe first group of cases are those where the second object does not appear in its canonical position, for example in interrogatives and other cases of leftdislocation (cf. 12a), or passives (12b).\nThe second group of cases are those where word order is canonical, but either the first object (12c) or the second object (12d) or both (12e) do not correspond to the query.\nNote that, unlike precision, the recall rate of a query cannot be increased after the data have been extracted from the corpus.\nThus, an important aspect in constructing a query is to annotate a random sample of our corpus manual for the phenomenon we are interested in, and then to check our query against this manual annotation.\nThis will not only tell us how good or bad the recall of our query is, it will also provide information about the most frequent cases we are missing.\nOnce we know this, we can try to revise our query to take these cases into account.\nIn a POS-tagged corpus, we could, for example, search for a sequence of a pronoun and a noun in addition to the sequence pronoun-determiner that we used above, which would give us cases like (12d), or we could search for forms of be followed by a past participle followed by a determiner or noun, which would give us passives like those in (12b).\nIn some cases, however, there may not be any additional patterns that we can reasonably search for.\nIn the present example with an untagged corpus, for example, there is no additional pattern that seems in any way promising.\nIn such cases, we have two options for dealing with low recall: First, we can check (in our manually annotated subcorpus) whether the data that were recalled differ from the data that were not recalled in any way that is relevant for our research question.\nIf this is not the case, we might decide to continue working with a low recall and hope that our results are still generalizable -\nIf our data do differ along one or more of the dimensions relevant to our research project, we might have to increase the recall at the expense of precision and spend more time weeding out false positives.\nIn the most extreme case, this might entail extracting the data manually, so let us return to this possibility in light of the current example.\nManual, semi-manual and automatic searches\nIn theory, the highest quality search results would always be achieved by a kind of close reading, i.e. a careful word-by-word (or phrase-by-phrase, clause-byclause) inspection of the corpus.\nAs already discussed in Chapter 2, this may sometimes be the only feasible option, either because automatic retrieval is difficult (as in the case of searching for ditransitives in an untagged corpus), or because an automatic retrieval is impossible (e.g., because the phenomenon we are interested in does not have any consistent formal properties, a point we will return to presently).\nAs discussed above, in the case of words and in at least some cases of grammatical structures, the quality of automatic searches may be increased by using a corpus annotated automatically with part-of-speech tags, phrase tags or even grammatical structures.\nAs discussed in Section 3.2.2.1 of Chapter 3, this brings with it its own problems, as automatic tagging and grammatical parsing are far from perfect.\nStill, an automatically annotated corpus will frequently allow us to define searches whose precision and recall are higher than in the example above.\nIn the case of many other phenomena, however, automatic annotation is simply not possible, or yields a quality so low that it simply does not make sense to base queries on it.\nFor example, linguistic metaphors are almost impossible to identify automatically, as they have little or no properties that systematically set them apart from literal language.\nConsider the following examples of the metaphors anger is heat and anger is a (hot) liquid (from\nIf we are interested in metaphors of anger in general, however, this approach will not work, since we have no way of knowing beforehand which semantic fields to include in our query.\nThis is precisely the situation where exhaustive retrieval can only be achieved by a manual corpus search, i.e., by reading the entire corpus and deciding for each word, phrase or clause, whether it constitutes an example of the phenomenon we are looking for.\nThus, it is not surprising that many corpus-linguistic studies on metaphor are based on manual searches (see, for example, However, as mentioned in Chapter 2, manual searches are very time-consuming and this limits their practical applicability: either we search large corpora, in which case manual searching is going to take more time and human resources than are realistically available, or we perform the search in a realistic time-frame and with the human resources realistically available, in which case we have to limit the size of our corpus so severely that the search results can no longer be considered representative of the language as a whole.\nThus, manual searches 4\nData retrieval and annotation are useful mainly in the context of research projects looking at a linguistic phenomenon in some clearly defined subtype of language (for example, metaphor in political speeches, see When searching corpora for such hard-to-retrieve phenomena, it may sometimes be possible to limit the analysis usefully to a subset of the available data, as shown in the previous subsection, where limiting the query for the ditransitive to active declarative clauses with canonical word order still yielded potentially useful results.\nIt depends on the phenomenon and the imagination of the researcher to find such easier-to-retrieve subsets.\nTo take up the example of metaphors introduced above, consider the examples in ( (15) a. He was consumed by his anger.\nb.\nHe was filled with anger.\nc. She was brimming with rage.\nIn these cases, the PPs by/with anger/rage make it clear that consume, (be) filled and brimming are not used literally.\nIf we limit ourselves just to metaphorical expressions of this type, i.e. expressions that explicitly mention both semantic fields involved in the metaphorical expression, it becomes possible to retrieve metaphors of anger semi-automatically.\nWe could construct a query that would retrieve all instances of the lemmas ANGER, RAGE, FURY, and other synonyms of anger, and then select those results that also contain (within the same clause or within a window of a given number of words) vocabulary from domains like 'liquids', 'heat', 'containers', etc.\nThis can be done manually by going through the concordance line by line (see, e.g., Limiting the focus to a subset of cases sharing a particular formal feature is a feasible strategy in other areas of linguistics, too.\nFor example, As in the example of the ditransitive construction discussed above, retrieval strategies like those used by In cases, where the phenomenon in question does not have any consistent formal features that would allow us to construct a query, and cannot plausibly be restricted to a subset that does have such features, a mixed strategy of elicitation and corpus query may be possible.\nFor example, (BNC FP6) There is no way to search for these expressions (and others with the same function) unless you are willing to read through the entire BNC -or unless you already know what to look for.\nData retrieval and annotation\nOf course, this query will miss any expressions that were not part of their initial list, but the conditional distribution of those expressions that are included may still yield interesting results -we can still learn something about which of these expressions are preferred in a particular variety, by a particular group of speakers, in a particular situation, etc.\nIf we assemble our initial list of expressions systematically, perhaps from a larger number of native speakers that are representative of the speech community in question in terms of regional origin, sex, age group, educational background, etc., we should end up with a representative sample of expressions to base our query on.\nIf we make our query flexible enough, we will likely even capture additional variants of these expressions.\nIf other strategies are not available, this is certainly a feasible approach.\nOf course, this approach only works with relatively routinized speech event categories like the Bathroom Formula -greetings and farewells, asking for the time, proposing marriage, etc. -which, while they do not have any invariable formal features, do not vary infinitely either.\nTo sum up, it depends on the phenomenon under investigation and on the research question whether we can take an automatic or at least a semi-automatic approach or whether we have to resort to manual data extraction.\nObviously, the more completely we can extract our object of research from the corpus, the better.\nAnnotating\nOnce the data have been extracted from the corpus (and, if necessary, false positives have been removed), they typically have to be annotated in terms of the variables relevant for the research question.\nIn some cases, the variables and their values will be provided externally; they may, for example, follow from the structure of the corpus itself, as in the case of british english vs. american english defined as \"occurring in the LOB corpus\" and \"occurring in the BROWN corpus\" respectively.\nIn other cases, the variables and their values may have been operationalized in terms of criteria that can be applied objectively (as in the case of Length defined as \"number of letters\").\nIn most cases, however, some degree of interpretation will be involved (as in the case of Animacy or the metaphors discussed above).\nWhatever the case, we need an annotation scheme -an explicit statement of the operational definitions applied.\nOf course, such an annotation scheme is especially important in cases where interpretative judgments are involved in categorizing the data.\nIn this case, the annotation scheme should contain not just operational definitions, but also explicit guidelines as to how these definitions should be applied to the corpus data.\nThese guidelines must be explicit 4.2\nAnnotating enough to ensure a high degree of agreement if different annotators (sometimes also referred to as coders or raters apply it to the same data.\nLet us look at each of these aspects in some detail.\nAnnotating as interpretation\nFirst of all, it is necessary to understand that the categorization of corpus data is an interpretative process in the first place.\nThis is true regardless of the kind of category.\nEven externally given categories typically receive a specific interpretation in the context of a specific research project.\nIn the simplest case, this consists in accepting the operational definitions used by the makers of a particular corpus (as well as the interpretative judgments made in applying them).\nTake the example of british english and american english used in Chapters 3 and 2: If we accept the idea that the LOB corpus contains \"British English\" we are accepting an interpretation of language varieties that is based on geographical criteria: British English means \"the English spoken by people who live (perhaps also: who were born and grew up) in the British Isles\".\nOr take the example of Sex, one of the demographic speaker variables included in many modern corpora: By accepting the values of this variable, that the corpus provides (typically male and female), we are accepting a specific interpretation of what it means to be \"male\" or \"female\".\nIn some corpora, this may be the interpretation of the speakers themselves (i.e., the corpus creators may have asked the speakers to specify their sex), in other cases this may be the interpretation of the corpus creators (based, for example, on the first names of the speakers or on the assessment of whoever collected the data).\nFor many speakers in a corpus, these different interpretations will presumably match, so that we can accept whatever interpretation was used as an approximation of our own operation definition of Sex.\nBut in research projects that are based on a specific understanding of Sex (for example, as a purely biological, a purely social or a purely psychological category), simply accepting the (often unstated) operational definition used by the corpus creators may distort our results substantially.\nThe same is true of other demographic variables, like education, income, social class, etc., which are often defined on a \"common sense\" basis that does not hold up to the current state of sociological research.\nInterpretation also plays a role in the case of seemingly objective criteria.\nEven though a criterion such as \"number of letters\" is largely self-explanatory, there are cases requiring interpretative judgments that may vary across researchers.\nIn the absence of clear instructions they may not know, among other things, 4 Data retrieval and annotation whether to treat ligatures as one or two letters, whether apostrophes or wordinternal hyphens are supposed to count as letters, or how to deal with spelling variants (for example, in the BNC the noun programme also occurs in the variant program that is shorter by two letters).\nThis kind of orthographic variation is very typical of older stages of English (before there was a somewhat standardized orthography), which causes problems not only for retrieval (cf. the discussion in Section 4.1.1 above, cf. also\nIn such cases, the role of interpretation can be reduced by including explicit instructions for dealing with potentially unclear cases.\nHowever, we may not have thought of all potentially unclear cases before we start annotating our data, which means that we may have to amend our annotation scheme as we go along.\nIn this case, it is important to check whether our amendments have an effect on the data we have already annotated, and to re-annotate them if necessary.\nIn cases of less objective criteria (such as Animacy discussed in Section 3.2.2.5 of Chapter 3), the role of interpretation is obvious.\nNo matter how explicit our annotation scheme, we will come across cases that are not covered and will require individual decisions; and even the clear cases are always based on an interpretative judgment.\nAs mentioned in Chapter 1, this is not necessarily undesirable in the same way that intuitive judgements about acceptability are undesirable; interpreting linguistic utterances is a natural activity in the context of using language.\nThus, if our operational definitions of the relevant variables and values are close to the definitions speakers implicitly apply in everyday linguistic interactions, we may get a high degree of agreement even in the absence of an explicit annotation scheme, Annotation schemes\nWe can think of a linguistic annotation scheme as a comprehensive operational definition for a particular variable, with detailed instructions as to how the values of this variable should be assigned to linguistic data (in our case, corpus data, but annotation schemes are also needed to categorize experimentally elicited linguistic data).\nThe annotation scheme would typically also include a coding scheme, Annotating specifying the labels by which these categories are to be represented.\nFor example, the distinctions between different degrees of Animacy need to be defined in a way that allows us to identify them in corpus data (this is the annotation scheme, cf. below), and the scheme needs to specify names for these categories (for example, the category containing animate entities could be labelled by the codes animate, anim, #01, cat:8472, etc. -as long as we know what the label stands for, we can choose it randomly).\nIn order to keep different research projects in a particular area comparable, it is desirable to create annotation and coding schemes independently of a particular research project.\nHowever, the field of corpus linguistics is not well-established and methodologically mature enough yet to have yielded uncontroversial and widely applicable annotation schemes for most linguistic phenomena.\nThere are some exceptions, such as the part-of-speech tag sets and the parsing schemes used by various wide-spread automatic taggers and parsers, which have become de facto standards by virtue of being easily applied to new data; there are also some substantial attempts to create annotation schemes for the manual annotation of phenomena like topicality (cf. Whenever it is feasible, we should use existing annotation schemes instead of creating our own -searching the literature for such schemes should be a routine step in the planning of a research project.\nOften, however, such a search will come up empty, or existing annotation schemes will not be suitable for the specific data we plan to use or they may be incompatible with our theoretical assumptions.\nIn these cases, we have to create our own annotation schemes.\nThe first step in creating an annotation scheme for a particular variable consists in deciding on a set of values that this variable may take.\nAs the example of Animacy in Chapter 3) shows, this decision is loosely constrained by our general operational definition, but the ultimate decision is up to us and must be justified within the context of our theoretical assumptions and our specific research question.\nThere are, in addition, several general criteria that the set of values for any variable must meet.\nFirst, they must be non-overlapping.\nThis may seem obvious, but it is not at all unusual, for example, to find continuous dimensions split up into overlapping categories, as in the following quotation: Hunters aged 15-25 years old participated more in non-consumptive activities than those aged 25-35 and 45-65 (𝑃 < 0.05), as were those aged 35-45 compared to those 55-65 years old (𝑃 < 0.05).\nHere, the authors obviously summarized the ages of their subjects into the following four classes: (I) 25-35, (II) 35-45, (III) 45-55 and (IV) 55-65: thus, subjects aged 35 could be assigned to class I or class II, subjects aged 45 to class II or class III, and subjects aged 55 to class III or class IV.\nThis must be avoided, as different annotators might make different decisions, and as other researchers attempting to replicate the research will not know how we categorized such cases.\nSecond, the variable should be defined such that it does not conflate properties that are potentially independent of each other, as this will lead to a set of values that do not fall along a single dimension.\nAs an example, consider the so-called Silverstein Hierarchy used to categorize nouns for (inherent) Topicality (after Deane 1987: 67):\nNote, first, that there is a lot of overlap in this annotation scheme.\nFor example, a first or second person pronoun will always refer to a human or animate NP and a third person pronoun will frequently do so, as will a proper name or a kin term.\nSimilarly, a container is a concrete object and can also be a location, and everything above the category \"Perceivable\" is also perceivable.\nThis overlap can only be dealt with by an instruction of the kind that every nominal expression should be put into the topmost applicable category; in other words, we need to add an \"except for expressions that also fit into one of the categories above\" to every category label.\nSecondly, although the Silverstein Hierarchy may superficially give the impression of providing values of a single variable that could be called Topicality, it is actually a mixture of several quite different variables and their possible values.\nOne attempt of disentangling these variables and giving them each a set of plausible values is the following: The set of variables in ( (20) a. pronoun + other + animate + touchable + object b. pronoun + other + inanimate + touchable + object c. pronoun + other + inanimate + non-touchable + object (or perhaps location, cf. in the air) d. pronoun + other + inanimate + abstract + object (or perhaps location, cf. the locative preposition in the phrase in a democracy)\nThere are two advantages of this more complex annotation scheme.\nFirst, it allows a more principled categorization of individual expressions: the variables and their values are easier to define and there are fewer unclear cases.\nSecond, it would allow us to determine empirically which of the variables are actually relevant in the context of a given research question, as irrelevant variables will not show a skew in their distribution across different conditions.\nOriginally, the Silverstein Hierarchy was meant to allow for a principled description of split ergative systems; it is possible, that the specific conflation of variables is suitable 4\nData retrieval and annotation for this task.\nHowever, it is an open question whether the same conflation of variables is also suitable for the analysis of other phenomena.\nIf we were to apply it as is, we would not be able to tell whether this is the case.\nThus, we should always define our variables in terms of a single dimension and deal with complex concepts (like Topicality) by analyzing the data in terms of a set of such variables.\nAfter defining a variable (or set of variables) and deciding on the type and number of values, the second step in creating an annotation scheme consists in defining what belongs into each category.\nWhere necessary, this should be done in the form of a decision procedure.\nFor example, the annotation scheme for Animacy mentioned in Chapter 3 The category organization is much more complex to apply consistently, since there is no intuitively accessible and generally accepted understanding of what constitutes an organization.\nIn particular, it needs to be specified what distinguishes an organization from other groups of human beings (that are to be categorized as human according to the annotation scheme).\nThe annotation scheme defines an organization as a referent involving \"more than one human\" with \"some degree of group identity\".\nIt then provides the following hierarchy of properties that a group of humans may have (where each property implies the presence of all properties below its position in the hierarchy): (21) +/-chartered/official +/-temporally stable +/-collective voice/purpose +/-collective action +/-collective\nIt then states that \"any group of humans at + collective voice or higher\" should be categorized as organization, while those below should simply be annotated as human.\nBy listing properties that a group must have to count as an organization in the sense of the annotation scheme, the decision is simplified considerably, and by providing a decision procedure, the number of unclear cases is reduced.\nThe annotation scheme also illustrates the use of the hierarchy: Annotating\nThus, while \"the posse\" would be an org, \"the mob\" might not be, depending on whether we see the mob as having a collective purpose.\n\"The crowd\" would not be considered org, but rather simply human.\nWhether or not to include such specific examples is a question that must be answered in the context of particular research projects.\nOne advantage is that examples may help the annotators understand the annotation scheme.\nA disadvantage is that examples may be understood as prototypical cases against which the referents in the data are to be matched, which may lead annotators to ignore the definitions and decision procedures.\nThe third step, discussed in detail in the next section, consists in testing the reliability of our annotation scheme.\nWhen we are satisfied that the scheme can be reliably applied to the data, the final step is the annotation itself.\nThe reliability of annotation schemes\nIn some cases, we may be able to define our variables in such a way that they can be annotated automatically.\nFor example, if we define Word Length in terms of \"number of letters\", we could write a simple computer program to go through our corpus, count the letters in each word and attach the value as a tag.\nSince computers are good at counting, it would be easy to ensure that such a program is completely accurate in its decisions.\nWe could also, for example, create a list of the 2500 most frequent nouns in English and their Animacy values, and write a program that goes through a tagged corpus and, whenever it encounters a word tagged as a noun, looks up this value and attaches it to the word as a tag.\nIn this case, the accuracy is likely to be much lower, as the program would not be able to distinguish between different word senses, for example assigning the label animate to the word horse regardless of whether it refers to an actual horse, a hobby horse (which should be annotated as inanimate or abstract depending on whether an actual or a metaphorical hobby horse is referred to) or whether it occurs in the idiom straight from the horse's mouth (where it would presumably have to be annotated as human, if at all).\nIn these more complex cases, we can, and should, assess the quality of the automatic annotation in the same way in which we would assess the quality of the results returned by a particular query, in terms of precision and recall (cf. Section 4.1.2, Table The most obvious way of doing this is to have (at least) two different annotators apply the annotation scheme to the data -if our measurements cannot be made objective (and, as should be clear by now, they rarely can in linguistics), this will at least allow us to ensure that they are intersubjectively reliable.\nOne approach would be to have the entire data set annotated by two annotators independently on the basis of the same annotation scheme.\nWe could then identify all cases in which the two annotators did not assign the same value and determine where the disagreement came from.\nObvious possibilities include cases that are not covered by the annotation scheme at all, cases where the definitions in the annotation scheme are too vague to apply or too ambiguous to make a principled decision, and cases where one of the annotators has misunderstood the corpus example or made a mistake due to inattention.\nWhere the annotation scheme is to blame, it could be revised accordingly and re-applied to all unclear cases.\nWhere an annotator is at fault, they could correct their annotation decision.\nAt the end of this process we would have a carefully annotated data set with no (or very few) unclear cases left.\nHowever, in practice there are two problems with this procedure.\nFirst, it is extremely time-consuming, which will often make it difficult to impossible to find a second annotator.\nSecond, discussing all unclear cases but not the apparently clear cases holds the danger that the former will be annotated according to different criteria than the latter.\nBoth problems can be solved (or at least alleviated) by testing the annotation scheme on a smaller dataset using two annotators and calculating its reliability across annotators.\nIf this so-called interrater reliability is sufficiently high, the annotation scheme can safely be applied to the actual data set by a single annotator.\nIf not, it needs to be made more explicit and applied to a new set of test data by two annotators; this process must be repeated until the interrater reliability is satisfactory.\nAnnotating A frequently used measure of interrater reliability in designs with two annotators is\nCohen's 𝜅 -𝑝 𝑒 In this formula, 𝑝 𝑜 is the relative observed agreement between the raters (i.e. the percentage of cases where both raters have assigned the same category) and 𝑝 𝑒 is the relative expected agreement (i.e. the percentage of cases where they should have agreed by chance).\nTable (23) a. a tired horse's plodding step (BROWN K13) b. every leaping stride of the horse (BROWN N02)\nLet us assume that we want to investigate the factors determining the choice between these two constructions (as we will do in Chapters 5 and 6).\nIn order to do so, we need to identify the subset of constructions with of that actually 4\nData retrieval and annotation correspond to the s-possessive semantically\n-note that the of -construction encodes a wide range of relations, including many -for example quantification or partition -that are never expressed by an s-possessive.\nThis means that we must manually go through the hits for the of -construction in our data and decide whether the relation encoded could also be encoded by an s-posessive.\nLet us use the term \"of -possessive\" for these cases.\nIdeally, we would do this by searching a large corpus for actual examples of paraphrases with the s-possessive, but let us assume that this is too time-consuming (a fair assumption in many research contexts) and that we want to rely on introspective judgments instead.\nWe might formulate a simple annotation scheme like the following: For each case of the structure [(DET i ) (AP i ) N i of NP j ], paraphrase it as an spossessive of the form [NP j 's (APi) N i ] (for example, the leaping stride of the horse becomes the horse's leaping stride).\nIf the result sounds like something a speaker of English would use, assign the label poss, if not, assign the label other.\nIn most cases, following this instruction should yield a fairly straightforward response, but there are more difficult cases.\n(24) a. a lack of unity b. ?? unity's lack (25) a. the concept of unity b. ??\nunity's concept\nAt first glance, neither of them seems to be paraphraseable, so they would both be assigned the value other according to our annotation scheme.\nHowever, in a context that strongly favors s-possessives -namely, where the possessor is realised as a possessive determiner -, the paraphrase of (24a) sounds acceptable, while (25a) still sounds odd: (26) a. Unity is important, and its lack can be a problem.\nb. ??\nUnity is important, so its concept must be taught early.\nThus, we might want to expand our annotation scheme as follows: Annotating For each case of the structure [(DET i ) (AP i ) N i of NP j ]\n, 1. paraphrase it as an s-possessive of the form [NP j 's (AP i ) N i ] (for example, the leaping stride of the horse becomes the horse's leaping stride).\nIf the result sounds like something a speaker of English would use, assign the label poss.\nIf not, 2. replace NP j by a possessive determiner (for example, the horse's leaping stride becomes its leaping stride and construct a coordinated sentence with NPj as the subject of the first conjunct and [PDET j (AP i ) N i ] as the subject of the second conjunct (for example, The horse tired and its leaping stride shortened.\nIf the s-possessive sounds like something a speaker of English would use in this context, assign the label poss,) if not, assign the label other.\nObviously, it is no simple task to invent a meaningful context of the form required by these instructions and then deciding whether the result is acceptable.\nIn other words, it is not obvious that this is a very reliable operationalization of the construct of-possessive, and it would not be surprising if speakers' introspective judgments varied too drastically to yield useful results.\nTable\nAs discussed above, the relative observed agreement is the percentage of cases both raters chose poss or both raters chose other, i.e. 𝑝 𝑜 = 18 + 9 18 + 2 + 1 + 9 = 27 30 = 0.9\nThe relative expected agreement, i.e. the probability that both raters agree in their choice between poss or other by chance, can be determined as follows (we will return to this issue in the next chapter and keep it simple here).\nRater 1 chose poss with a probability of 20 /30 = 0.6667 (i.e., 66.67 percent of the time) and other with a probability of 10 /30 = 0.3333 (i.e., 33.33 percent of the time).\nRater 2 chose poss with a probability of 19 /30 = 0.6333, and other with a probability of 11 /30 = 0.3667.\nThe joint probability that both raters will choose poss by chance is the product of their individual probabilities of doing so, i.e. 0.6667 × 0.6333 = 0.4222; for no it is 0.3333 × 0.3667 = 0.1222.\nAdding these two probabilities gives us the overall probability that the raters will agree by chance: 0.4222 + 0.1222 = 0.5444.\nWe can now calculate the interrater reliability for the data in Table\nThere are various suggestions as to what value of 𝜅 is to be taken as satisfactory.\nOne reasonable suggestion (following McHugh 2012) is shown in Table Reproducibility 4 Data retrieval and annotation and methods to an extent that allows other researchers to reproduce our results.\nThis is referred to as reproducibility and/or (with a different focus, replicability) -since there is some variation in how these terms are used, let us briefly switch to more specific, non-conventional terminology.\nThe minimal requirement of an incremental and collaborative research cycle is what we might call retraceability: our description of the research design and the associated procedures must be explicit and detailed enough for another researcher to retrace and check for correctness each step of our analysis and when provided with all our research materials (i.e., the corpora, the raw extracted data and the annotated data) and all other resources used (such as our annotation scheme and the software used in the extraction and statistical analysis of the data) and all our research notes, intermediate calculations, etc.\nIn other words, our research project must be documented in sufficient detail for others to make sure that we arrived at our results via the procedures that we claim to have used, and to identify possible problems in our data and/or procedure.\nThus retraceability is closely related to the idea of accountability in accounting.\nGoing slightly beyond retraceability, we can formulate a requirement which we might call reconstructibility: given all materials and resources, the description of our procedure must be detailed enough to ensure that a researcher independently applying this procedure to the same data using the same resources, but without access to our research notes, intermediate results, etc., should arrive at the same result.\nBoth of the requirements just described fall within the range of what is referred to as reproducibility.\nObviously, as long as our data and other resources (and, if relevant, our research notes and intermediate results) are available, reproducibility is largely a matter of providing a sufficiently explicit and fine-grained description of the steps by which we arrived at our results.\nHowever, in actual practice it is surprisingly difficult to achieve reproducibility even if our research design does not involve any manual data extraction or annotation (see Matters become even more difficult if our data or other resources are not accessible, for example, if we use a corpus or software constructed specifically for our research project that we cannot share publicly due to copyright restrictions, or if our corpus contains sensitive information such that sharing it would endanger individuals, violate non-disclosure agreements, constitute high treason, etc.\nAnnotating In such a case, our research will not be reproducible (retraceable or reconstructible), which is why we should avoid this situation.\nIf it cannot be avoided, however, our research should still meet a requirement that we might call transferability: the description of our materials, resources and procedures must be detailed enough for other researchers to transfer it to similar materials and resources and arrive at a similar result.\nThis is broadly what replicability refers to, but the term may also be used in an extended sense to situations where a researcher attempts to replicate our results and conclusions rather than our entire research design.\nObviously, research designs that meet the criterion of reproducibility are also transferable, but not vice versa.\nIn the context of the scientific research cycle, reproducibility and replicability are crucial: a researcher building on previous research must be able to reconstruct this research in order to determine its quality and thus the extent to which our results can be taken to corroborate our hypotheses.\nThey must also be able to reconstruct the research in order to build on it in a meaningful way, for example by adapting our methods to other kinds of data, new hypotheses or other linguistic phenomena.\nSay, for example, a researcher wanted to extend the analysis of the words windscreen and windshield presented in Chapter 3, Section 3.1 to other varieties of English.\nThe first step would be to reconstruct our analysis (if they had access to the LOB, Data retrieval and annotation It would not be surprising if a researcher attempting to reconstruct our analysis would get different results.\nThis is not a purely theoretical possibility even with such a simple research design -you will often find that even word frequencies reported for a given corpus in the literature will not correspond to what your own query of the same corpus yields.\nObviously, the more complex the phenomenon, the more difficult it will be to guess what query or queries a researcher has used if they do not tell us.\nAnd if the data had to be annotated in any way more complex than \"number of letters\", that annotation will be difficult to reconstruct even if an annotation scheme is provided, and impossible to reconstruct if this is not the case.\nUnfortunately, corpus linguists have long paid insufficient attention to this (and I include much of my own research in this criticism).\nIt is high time that this change and that corpus linguists make an honest effort to describe their designs in sufficient detail to make them reproducible (in all senses discussed above).\nIn many disciplines, it is becoming customary to provide raw data, categorized data, computer scripts, etc. in the form of supplementary materials with every research article.\nThis is not yet standard in corpus linguistics, but it is a good idea to plan and document your research as though it already were.\nData storage\nWe will conclude this chapter with a discussion of a point that may, at first, appear merely practical but that is crucial in carrying out corpus-linguistic research (and that has some methodological repercussions, too): the question of how to store our data and annotation decisions.\nThere are broadly two ways of doing so: first, in the corpus itself, and second, in a separate database of some sort.\nThe first option is routinely chosen in the case of automatically annotated variables like Part of Speech, as in the passage from the BROWN corpus cited in Here, the annotation (i.e., the part-of-speech tags) are attached to the data they refer to (i.e., words) by an underscore (recall that alternatively, a vertical format with words in the first and tags in the second column is frequently used, as are various kinds of XML notation).\nAnnotating The second option is more typically chosen in the case of annotations added in the context of a specific research project (especially if they are added manually): the data are extracted, stored in a separate file, and then annotated.\nFrequently, spreadsheet applications are used to store the corpus data and annotation decisions, as in the example in The first line contains labels that tell us what information is found in each column respectively.\nThis should include the example itself (either as shown in Figure In particular, we should never store our data in summarized form, for example, as shown in There is simply no need to store data in this form -if we need this kind of summary, it can be created automatically from a raw data table like that in Figure As mentioned above, however, the format of storage is not simply a practical matter, but a methodological one.\nIf we did store our data in the form shown in Figure Since quantitative analysis always requires a raw data table, we might conclude that it is the only useful way of recording our annotation decisions.\nHowever, there are cases where it may be more useful to record them in the form of annotations in (a copy of) the original corpus instead, i.e., analogously to automatically added annotations.\nFor example, the information in Figure The advantage would be that we, or other researchers, could also use our annotated data for research projects concerned with completely different research questions.\nThus, if we are dealing with a variable that is likely to be of general interest, we should consider the possibility of annotating the corpus itself, instead of first extracting the relevant data to a raw data table and annotating them afterwards.\nWhile the direct annotation of corpus files is rare in corpus 4.2 Annotating linguistics, it has become the preferred strategy in various fields concerned with qualitative analysis of textual data.\nThere are open-source and commercial software packages dedicated to this task.\nThey typically allow the user to define a set of annotation categories with appropriate codes, import a text file, and then assign the codes to a word or larger textual unit by selecting it with the mouse and then clicking a button for the appropriate code that is then added (often in XML format) to the imported text.\nThis strategy has the additional advantage that one can view one's annotated examples in their original context (which may be necessary when annotating additional variables later).\nHowever, the available software packages are geared towards the analysis of individual texts and do not let the user to work comfortably with large corpora.\nQuantifying research questions\nRecall, once again, that at the end of Chapter 2, we defined corpus linguistics as the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus.\nWe discussed the fact that this definition covers cases of hypotheses phrased in absolute terms, i.e. cases where the distribution of a phenomenon across different conditions is a matter of all or nothing (as in \"All speakers of American English refer to the front window of a car as windshield; all speakers of British English refer to it as windscreen\") as well as cases where the distribution is a matter of more-or-less (as in \"British English speakers prefer the word railway over railroad when referring to train tracks; American English speakers prefer railroad over railway\" or \"More British speakers refer to networks of train tracks as railway instead of railroad; more American English speakers refer to them as railroad instead of railway\").\nIn the case of hypotheses stated in terms of more-or-less, predictions must be stated in quantitative terms which in turn means that our data have to be quantified in some way so that we can compare them to our predictions.\nIn this chapter, we will discuss in more detail how this is done when dealing with different types of data.\nSpecifically, we will discuss three types of data (or levels of measurement) that we might encounter in the process of quantifying the (annotated) results of a corpus query (Section 5.1): nominal data (discussed in more detail in Section 5.2), ordinal (or rank) data (discussed in more detail in Section 5.3, and cardinal data (discussed in more detail in Section 5.4.\nThese discussions, summarized in Section 5.5, will lay the ground work for the introduction to statistical hypothesis testing presented in the next chapter.\nTypes of data\nIn order to illustrate these types of data, let us turn to a linguistic phenomenon that is more complex than the distribution of words across varieties, and closer to the kind of phenomenon actually of interest to corpus linguists: that of the two English possessive constructions introduced in Section 4.2.3 of Chapter 4 above.\nAs discussed there, the two constructions can often be used seemingly interchangeably, as in (1a, b): (1) a. The city's museums are treasure houses of inspiring objects from all eras and cultures.\n(www.res.org.uk)\nb.\nToday one can find the monuments and artifacts from all of these eras in the museums of the city.\n(www.travelhouseuk.co.uk)\nHowever, there are limits to this interchangeability.\nFirst, there are a number of relations that are exclusively encoded by the of -construction, such as quantities (both generic, as in a couple/bit/lot of, and in terms of measures, as in six miles/years/gallons of ), type relations (a kind/type/sort/class of ) and composition or constitution (a mixture of water and whisky, a dress of silk, etc.) (cf., e.g., Second, and more interestingly, even where a relation can be expressed by both constructions, there is often a preference for one or the other in a given context.\nA number of factors underlying these preferences have been suggested and investigated using quantitative corpus-linguistic methods.\nAmong these, there are three that are widely agreed upon to have an influence, namely the givenness, animacy and weight of the modifier.\nThese three factors nicely illustrate the levels of measurement mentioned above, so we will look at each of them in some detail.\n(a) Givenness Following the principle of Functional Sentence Perspective, the spossessive will be preferred if the modifier (the phrase marked by 's or of ) refers to given information, while the construction with of will be preferred if the modifier is new\n(2) a. In New York, we visited the city's many museums.\nb. ??\nIn New York, we visited the many museums of the city.\n(3) a. The Guggenheim is much larger than the museums of other major cities.\nb. ??\nThe Guggenheim is much larger than other major cities' museums.\n(b) Animacy Since animate referents tend to be more topical than inanimate ones and more topical elements tend to precede less topical ones, if the modifier is animate, the s-possessive will be preferred, if it is inanimate, the construction with of will be preferred\n(cf. Solomon R. Guggenheim's collection contains some fine paintings.\nb. ??\nThe collection of Solomon R. Guggenheim contains some fine paintings.\n(5) a.\nThe collection of the Guggenheim museum contains some fine paintings.\nb. ??\nThe Guggenheim museum's collection contains some fine paintings.\n(c) Length Since short constituents generally precede long constituents, if the modifier is short, the s-possessive will be preferred, if it is long, the construction with of will be preferred\n(6) a. The museum's collection is stunning.\nb. ??\nThe collection of the museum is stunning.\n(7) a. The collection of the most famous museum in New York is stunning.\nb. ??\nThe most famous museum in New York's collection is stunning.\nIn the case of all three factors, we are dealing with hypotheses concerning preferences rather than absolute difference.\nNone of the examples with question marks are ungrammatical and all of them could conceivably occur; they just sound a little bit odd.\nThus, the predictions we can derive from each hypothesis must be stated and tested in terms of relative rather than absolute differencesthey all involve predictions stated in terms more-or-less rather than all-or-nothing.\nRelative quantitative differences are expressed and dealt with in different ways depending on the type of data they involve.\nNominal data\nA nominal variable is a variable whose values are labels for categories that have no intrinsic order with respect to each other (i.e., there is no aspect of their definition that would allow us to put them in a natural order) -for example, Sex, Nationality or Native Language.\nIf we categorize data in terms of such a nominal variable, the only way to quantify them is to count the number of observations of each category in a given sample and express the result in absolute frequencies (i.e., raw numbers) or relative frequencies (such as percentages).\nFor example, in the population of the world in 2005, there were 92 million native speakers of german and 75 million speakers of french.\nWe cannot rank the values of nominal variables based on intrinsic criteria.\nFor example, we cannot rank the German language higher than the French language 5\nQuantifying research questions on the basis of any intrinsic property of German and French.\nThey are simply two different manifestations of the phenomenon Language, part of an unordered set including all human languages.\nThat we cannot rank them based on intrinsic criteria does not mean that we cannot rank them at all.\nFor example, we could rank them by number of speakers worldwide (in which case, as the numbers cited above show, German ranks above French).\nWe could also rank them by the number of countries in which they are an official language (in which case French, which has official status in 29 countries, ranks above German, with an official status in only 6 countries).\nBut the number of native speakers or the number of countries where a language has an official status is not an intrinsic property of that language -German would still be German if its number of speakers was reduced by half by an asteroid strike, and French would still be French if it lost its official status in all 29 countries).\nIn other words, we are not really ranking french and german as values of Language at all; instead, we are ranking values of the variables Size of Native Speech Community and Number of Countries with Official Language X respectively.\nWe also cannot calculate mean values (\"averages\") between the values of nominal variables.\nWe cannot claim, for example, that Javanese is the mean of German and French because the number of Javanese native speakers falls (roughly) halfway between that of German and French native speakers.\nAgain, what we would be calculating a mean of is the values of the variable Size of Native Speech Community, and while it makes a sort of sense to say that the mean of the values number of french native speakers and number of german native speakers was 83.5 in 2005, it does not make sense to refer to this mean as number of javanese speakers.\nWith respect to the three hypotheses concerning the distribution of the spossessive and the of -possessive, it is obvious that they all involve at least one nominal variable -the constructions themselves.\nThese are essentially values of a variable we could call Type of Possessive Construction.\nWe could categorize all grammatical expressions of possession in a corpus in terms of the values s-possessive and of-possessive, count them and express the result in terms of absolute or relative frequencies.\nFor example, the s-possessive occurs 22 193 times in the BROWN corpus (excluding proper names and instances of the double spossessive), and the of -possessive occurs 17 800 times.\nTypes of data\nAs with the example of the variable Native Language above, we can rank the constructions (i.e., the values of the variable Type of Possessive Construction) in terms of their frequency (the s-possessive is more frequent), but again we are not ranking these values based on an intrinsic criterion but on an extrinsic one: their corpus frequency in one particular corpus.\nWe can also calculate their mean frequency (19 996.50), but again, this is not a mean of the two constructions, but of their frequencies in one particular corpus.\nOrdinal data\nAn ordinal variable is a variable whose values are labels for categories that do have an intrinsic order with respect to each other but that cannot be expressed in terms of natural numbers.\nIn other words, ordinal variables are variables that are defined in such a way that some aspect of their definition allows us to order them without reference to an extrinsic criterion, but that does not give us any information about the distance (or degree of difference) between one category and the next.\nIf we categorize data in terms of such an ordinal variable, we can treat them accordingly (i.e., we can rank them), or we can treat them like nominal data by simply ignoring their inherent order (i.e., we can still count the number of observations for each value and report absolute or relative frequencies).\nWe cannot calculate mean values.\nSome typical examples of ordinal variables are demographic variables like Education or (in the appropriate sub-demographic) Military Rank, but also School Grades and the kind of ratings often found in questionnaires (both of which are, however, often treated as though they were cardinal data, see below).\nFor example, academic degrees are intrinsically ordered: it is part of the definition of a PhD degree that it ranks higher than a master's degree, which in turn ranks higher than a bachelor's degree.\nThus, we can easily rank speakers in a sample of university graduates based on the highest degree they have completed.\nWe can also simply count the number of PhDs, MAs, and BAs and ignore the ordering of the degrees.\nBut we cannot calculate a mean: if five speakers in our sample of ten speakers have a PhD and five have a BA, this does not allow us to claim that all of them have an MA degree on average.\nThe first important reason for this is that the size of the difference in terms of skills and knowledge that separates a BA from an MA is not the same as that separating an MA from a PhD: in Europe, one typically studies two years for an MA, but it typically takes three to five years to complete a PhD.\nThe second important reason is that the values of ordinal variables typically differ along more than one dimension: while it is true that a PhD is a higher degree than an MA, which is a higher degree 5\nQuantifying research questions than a BA, the three degrees also differ in terms of specialization (from a relatively broad BA to a very narrow PhD), and the PhD degree differs from the two other degrees qualitatively: a BA and an MA primarily show that one has acquired knowledge and (more or less practical skills), but a PhD primarily shows that one has acquired research skills.\nWith respect to the three hypotheses concerning the distribution of the s-possessive and the of -possessive, clearly Animacy is an ordinal variable, at least if we think of it in terms of a scale, as we did in Chapter 3, Section 3.2.\nRecall that a simple animacy scale might look like this: This means that we could categorize and rank all nouns in a corpus according to their animacy.\nBut again, we cannot calculate a mean.\nIf we have 50 human nouns and 50 abstract nouns, we cannot say that we have 100 nouns with a mean value of inanimate.\nAgain, this is because we have no way of knowing whether, in terms of animacy, the difference between animate and inanimate is the same quantitatively as that between inanimate and abstract, but also, because we are, again, dealing with qualitative as well as quantitative differences: the difference between animate and inanimate on the one hand and abstract on the other is that the first two have physical existence; and the difference between animate on the one hand and inanimate and abstract on the other is that animates are potentially alive and the other two are not.\nIn other words, our scale is really a combination of at least two dimensions.\nAgain, we could ignore the intrinsic order of the values on our Animacy scale and simply treat them as nominal data, i.e., count them and report the frequency with which each value occurs in our data.\nPotentially ordinal data are actually frequently treated like nominal data in corpus linguistics (cf. Section 5.3.2), and with complex scales combining a range of different dimensions, this is probably a good idea; but ordinal data also have a useful place in quantitative corpus linguistics.\nCardinal data\nCardinal variables are variables whose values are numerical measurements along a particular dimension.\nIn other words, they are intrinsically ordered (like ordinal data), but not because some aspect of their definition allows us to order them, 5.1 Types of data but because of their nature as numbers.\nAlso, the distance between any two measurements is precisely known and can directly be expressed as a number itself.\nThis means that we can perform any arithmetic operation on cardinal data -crucially, we can calculate means.\nOf course, we can also treat cardinal data like rank data by ignoring all of their mathematical properties other than their order, and we can also treat them as nominal data.\nTypical cases of cardinal variables are demographic variables like Age or Income.\nFor example, we can categorize a sample of speakers by their age and then calculate the mean age of our sample.\nIf our sample contains five 50-year-olds and five 30-year-olds, it makes perfect sense to say that the mean age in our sample is 40; we might need additional information to distinguish between this sample and another sample that consists of 5 41-year-olds and 5 39-year-olds, that would also have a mean age of 40 (cf. Chapter 6), but the mean itself is meaningful, because the distance between 30 and 40 is the same as that between 40 and 50 and all measurements involve just a single dimension (age).\nWith respect to the two possessives, the variables Length and Givenness are cardinal variables.\nIt should be obvious that we can calculate the mean length of words or other constituents in a corpus, a particular sample, a particular position in a grammatical construction, etc.\nAs mentioned above, we can also treat cardinal data like ordinal data.\nThis may sometimes actually be necessary for mathematical reasons (see Chapter 6 below); in other cases, we may want to transform cardinal data to ordinal data based on theoretical considerations.\nFor example, the measure of Referential Distance discussed in Chapter 3, Section 3.2 yields cardinal data ranging from 0 to whatever maximum distance we decide on and it would be possible, and reasonable, to calculate the mean referential distance of a particular type of referring expression.\nHowever 5 Quantifying research questions\nInterim summary\nIn the preceding three subsections, we have repeatedly mentioned concepts like frequency, percentage, rank and mean.\nIn the following three sections, we will introduce these concepts in more detail, providing a solid foundation of descriptive statistical measures for nominal, ordinal and cardinal data.\nNote, however, that most research designs, including those useful for investigating the hypotheses about the two possessive constructions, involve (at least) two variables: (at least) one independent variable and (at least) one dependent variable.\nEven our definition of corpus linguistics makes reference to this fact when it states that research questions should be framed such that it enables us to answer them by looking at the distribution of linguistic phenomena across different conditions.\nSince such conditions are most likely to be nominal in character (a set of language varieties, groups of speakers, grammatical constructions, etc.), we will limit the discussion to combinations of variables where at least one variable is nominal, i.e., (a) designs with two nominal variables, (b) designs with one nominal and one ordinal variable, and (c) designs with one nominal and one cardinal variable.\nLogically, there are three additional designs, namely designs with (d) two ordinal variables, (e) two cardinal variables or (f) one ordinal and one cardinal variable.\nFor such cases, we would need different variants of correlation analysis, which we will not discuss in this book in any detail (but there are pointers to the relevant literature in the Study Notes to Chapter 6 and we will touch upon such designs in some of the Case Studies in Part II of this book).\nDescriptive statistics for nominal data\nMost examples we have looked at so far in this book involved two nominal variables: the independent variable Variety (with the values british english vs. american english) and a dependent variable consisting of some linguistic alternation (mostly regional synonyms of some lexicalized concept).\nThus, this kind of research design should already be somewhat familiar.\nFor a closer look, we will apply it to the first of the three hypotheses introduced in the preceding section, which is restated here with the background assumption from which it is derived: (9) Assumption: Discourse-old items occur before discourse-new items.\nHypothesis: The s-possessive will be used when the modifier is discourse-old, the of-possessive will be used when the modifier is discourse-new.\nDescriptive statistics for nominal data Note that the terms s-possessive and of-possessive are typeset in small caps in these hypotheses.\nThis is done in order to show that they are values of a variable in a particular research design, based on a particular theoretical construct.\nAs such, these values must, of course, be given operational definitions (also, the construct upon which the variable is based should be explicated with reference to a particular model of language, but this would lead us too far from the purpose of this chapter and so I will assume that the phenomenon \"English nominal possession\" is self-explanatory).\nThe definitions I used were the following: (10) a. s-\npossessive\n:\nA construction consisting of a possessive pronoun or a noun phrase marked by the clitic 's modifying a noun following it, where the construction as a whole is not a proper name.\nb. of-possessive: A construction consisting of a noun modified by a prepositional phrase with of, where the construction as a whole encodes a relation that could theoretically also be encoded by the s-possessive and is not a proper name.\nProper names (such as Scotty's Bar or District of Columbia) are excluded in both cases because they are fixed and could not vary.\nTherefore, they will not be subject to any restrictions concerning givenness, animacy or length.\nTo turn these definitions into operational definitions, we need to provide the specific queries used to extract the data, including a description of those aspects of corpus annotation used in formulating these queries.\nWe also need annotation schemes detailing how to distinguish proper names from other uses and how to identify of -constructions that encode relations that could also be encoded by the s-possessive.\nThe s-possessive is easy to extract if we use the tagging present in the BROWN corpus: words with the possessive clitic (i.e. 's, or just ' for words whose stem ends in s) as well as possessive pronouns are annotated with tags ending in the dollar sign $, so a query for words tagged in this way will retrieve all cases with high precision and recall.\nFor the of -possessive, extraction is\nmore difficultthe\nsafest way seems to be to search for words tagged as nouns followed by the preposition of, which already excludes uses like [most of NP] (where the quantifying expression is tagged as a post-determiner) [because of NP], [afraid of NP], etc.\nThe annotation of the results for proper name or common noun status can be done in various ways -in some corpora (but not in the BROWN corpus), the POS tags may help, in others, we might use capitalization as a hint, etc.\nThe 5\nQuantifying research questions annotation for whether or not an of -construction encodes a relation that could also be encoded by an s-possessive can be done as discussed in Section 4.2.3 of Chapter 4.\nUsing these operationalizations for the purposes of the case studies in this chapter, I retrieved and annotated a one-percent sample of each construction (the constructions are so frequent that even one percent leaves us with 222 sand 178 of -possessives (the full data set for the studies presented in this and the following two subsections can be found in the Supplementary Online Material, file HKD3).\nNext, the values discourse-old and discourse-new have to be operationalized.\nThis could be done using the measure of referential distance discussed in Section 3.2 of Chapter 3, which (in slightly different versions) is the most frequently used operationalization in corpus linguistics.\nSince we want to demonstrate a design with two nominal variables, however, and in order to illustrate that constructs can be operationalized in different ways, I will use a different, somewhat indirect operationalization.\nIt is well established that pronouns tend to refer to old information, whereas new information must be introduced using common nouns in full lexical NPs.\nThus, we can assume a correlation between the construct discourse-old and the construct pronoun on the one hand, and the construct discourse-new and the construct common noun on the other.\nThis correlation is not perfect, as common nouns can also encode old information, so using Part of Speech of Nominal Expression as an operational definition for Givenness is somewhat crude in terms of validity, but the advantage is that it yields a highly reliable, easy-to-annotate definition: we can use the part-of-speech tagging to annotate our sample automatically.\nWe can now state the following quantitative prediction based on our hypothesis:\n(11) Prediction: There will be more cases of the s-possessive with discourseold modifiers than with discourse-new modifiers, and more cases of the of-possessive with discourse-new modifiers than with discourse-old modifiers.\nPossessive is presented as the dependent variable here, since logically the hypothesis is that the information status of the modifier influences the choice of construction, but mathematically it does not matter in contingency tables what we treat as the dependent or independent variable.\nIn addition, there are two cells showing the row totals (the sum of all cells in a given row) and the column totals (the sum of all cells in a given column), and one cell showing the table total (the sum of all four intersections).\nThe row and column totals for a given cell are referred to as the marginal frequencies for that cell.\nPercentages The frequencies in Table\nThus, it is generally useful to convert the absolute frequencies to relative frequencies, abstracting away from the differences in marginal frequencies.\nIn order to convert an absolute frequency n into a relative one, we simply divide it by the total number of cases N of which it is a part.\nThis gives us a decimal fraction expressing the frequency as a proportion of 1.\nIf we want a percentage instead, we multiply this decimal fraction by 100, thus expressing our frequency as a proportion of 100.\n5\nQuantifying research questions\nFor example, if we have a group of 31 students studying some foreign language and six of them study German, the percentage of students studying German is\nMultiplying this by 100, we get 0.1953 × 100 = 19.35%\nIn other words, a percentage is just another way of expressing a decimal fraction, which is just another way of expressing a fraction, all of which are (among other things) ways of expressing relative frequencies (i.e., proportions).\nIn academic papers, it is common to report relative frequencies as decimal fractions rather than as percentages, so we will follow this practice here.\nIf we want to convert the absolute frequencies in Table The column proportions can be related to our prediction most straightforwardly: based on our hypothesis, we predicted that in our sample a majority of s-possessives should have modifiers that refer to discourse-old information and, conversely a majority of of -possessives should have modifiers that refer to discourse-new information.\nThe relevance of the row proportions is less clear in this case.\nWe might predict, based on our hypothesis, that the majority of modifiers referring to old information should occur in s-possessives and the majority of modifiers referring to new information should occur in of -possessives.\nThis is the case in Table Descriptive statistics for nominal data It would be less confusing if we had a way of taking into account both values of both variables at the same time.\nThe table proportions allow this to some extent.\nThe way our hypothesis is phrased, we would expect a majority of cases to instantiate the intersections s-possessive ∩ discourse-old and of-possessive ∩ discourse-new, with a minority of cases instantiating the other two intersections.\nIn Table While relative frequencies (whether expressed as decimal fractions or as percentages) are, with due care, more easily interpretable than absolute frequencies, they have two disadvantages.\nFirst, by abstracting away from the absolute frequencies, we lose valuable information: we would interpret a distribution such as that in Table Observed and expected frequencies\nSo how do we determine the expected frequencies of the intersections of our variables?\nConsider the textbook example of a random process: flipping a coin onto a hard surface.\nIgnoring the theoretical and extremely remote possibility that the coin will land, and remain standing, on its edge, there are two possible outcomes, heads and tails.\nIf the coin has not been manipulated in some clever way, for example, by making one side heavier than the other, the probability for heads and tails is 0.5 (or fifty percent) each (such a coin is called a fair coin in statistics).\nFrom these probabilities, we can calculate the expected frequency of heads and tails in a series of coin flips.\nIf we flip the coin ten times, we expect five heads and five tails, because 0.5×10 = 5.\nIf we flip the coin 42 times, the expected frequency is 21 for heads and 21 for tails (0.5 × 42), and so on.\nIn the real world, we would of course expect some variation (more on this in Chapter 6), so expected frequency refers to a theoretical expectation derived by multiplying the probability of an event by the total number of observations.\nSo how do we transfer this logic to a contingency table like Table\nThese marginal frequencies of our variables and their values are a fact about our data that must be taken as a given when calculating the expected frequencies: our hypothesis says nothing about the overall frequency of the two constructions or the overall frequency of discourse-old and discourse-new modifiers, but only about the frequencies with which these values should co-occur.\nIn other words, the question we must answer is the following: Given that the s-and the of -possessive occur 200 and 156 times respectively and given that there are 183 discourse-old modifiers and 173 discourse-new modifiers, how frequently would each combination these values occur by chance?\nPut like this, the answer is conceptually quite simple: the marginal frequencies should be distributed across the intersections of our variables such that the relative frequencies in each row should be the same as those of the row total and the relative frequencies in each column should be the same as those of the column total.\nFor example, 56.18 percent of all possessive constructions in our sample are s-possessives and 43.82 percent are of -possessives; if there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for the 183 constructions with old modifiers, i.e. 183 × 0.5618 = 102.81 s-possessives and 183 × 0.4382 = 80.19 of -possessives.\nLikewise, there are 173 constructions with new modifiers, so 173 × 0.5618 = 97.19 of them should be s-possessives and 173 × 0.4382 = 75.81 of them should be ofpossessives.\nThe same goes for the columns: 51.4 percent of all constructions have old modifiers and 41.6 percent have new modifiers.\nIf there were a random relationship between type of construction and givenness of the modifier, we should find the same proportions for both types of possessive construction: there should be 200 × 0.514 = 102.8 s-possessives with old modifiers and 97.2 with new modifiers, as well as 156 × 0.514 = 80.18 of -possessives with old modifiers and 156 × 0.486 = 75.82 of -possessives with new modifiers.\nNote that the expected frequencies for each intersection are the same whether we use the total row percentages or the total column percentages: the small differences are due to rounding errors.\nQuantifying research questions\nTo avoid rounding errors, we should not actually convert the row and column totals to percentages at all, but use the following much simpler way of calculating the expected frequencies: for each cell, we simply multiply its marginal frequencies and divide the result by the table total as shown in Table\nWe can now compare the observed and expected frequencies of each intersection to see whether the difference conforms to our quantitative prediction.\nThis is clearly the case: for the intersections s-possessive ∩ discourse-old and of-possessive ∩ discourse-new, the observed frequencies are higher than the expected ones, for the intersections s-possessive ∩ discourse-new and of-possessive ∩ discourse-old, the observed frequencies are lower than the expected ones.\nThis conditional distribution seems to corroborate our hypothesis.\nHowever, note that it does not yet prove or disprove anything, since, as mentioned above, we would never expect a real-world distribution of events to match the expected distribution perfectly.\nWe will return to this issue in Chapter 6.\nDescriptive statistics for ordinal data Descriptive statistics for ordinal data\nLet us turn, next, to a design with one nominal and one ordinal variable: a test of the second of the three hypotheses introduced at the beginning of this chapter.\nAgain, it is restated here together with the background assumption from which it is derived: (12) Assumption: Animate items occur before inanimate items.\nHypothesis: The s-possessive will be used when the modifier is high in Animacy, the of-possessive will be used when the modifier is low in Animacy.\nThe constructions are operationalized as before.\nThe data used are based on the same data set, except that cases with proper names are now included.\nFor expository reasons, we are going to look at a ten-percent subsample of the full sample, giving us 22 s-possessives and 17 of -possessives.\nAnimacy was operationally defined in terms of the annotation scheme shown in Table As pointed out above, Animacy hierarchies are a classic example of ordinal data, as the categories can be ordered (although there may be some disagreement about the exact order), but we cannot say anything about the distance between one category and the next, and there is more than one conceptual dimension involved\n(I ordered them according to dimensions like \"potential for life\", \"touchability\" and \"conceptual independence\").\n5 Quantifying research questions We can now formulate the following prediction: (13) Prediction: The modifiers of the s-possessive will tend to occur high on the Animacy scale, the modifiers of the of-possessive will tend to occur low on the Animacy scale.\nNote that phrased like this, it is not yet a quantitative prediction, since \"tend to\" is not a mathematical concept.\nWhile frequency for nominal data and mean (or \"average\") for cardinal data are used in everyday language with something close to their mathematical meaning, we do not have an everyday word for dealing with differences in ordinal data.\nWe will return to this point presently, but first, let us look at the data impressionistically.\nTable\nA simple way of finding out whether the data conform to our prediction would be to sort the entire data set by the rank assigned to the examples and check whether the s-possessives cluster near the top of the list and the of -possessives near the bottom.\nTable Table\nHowever, we need a less impressionistic way of summarizing data sets coded as ordinal variables, since not all data set will be\nas\nstraightforwardly interpretable as this one\n.\nSo let us turn to the question of an appropriate descriptive statistic for ordinal data.\n5.3 Descriptive statistics for ordinal data Descriptive statistics for ordinal data Medians\nAs explained above, we cannot calculate a mean for a set of ordinal values, but we can do something similar.\nThe idea behind calculating a mean value is, essentially, to provide a kind of mid-point around which a set of values is distributed -it is a so-called measure of central tendency.\nThus, if we cannot calculate a mean, the next best thing is to simply list our data ordered from highest to lowest and find the value in the middle of that list.\nThis value is known as the median -a value that splits a sample or population into a higher and a lower portion of equal sizes.\nFor example, the rank values for the Animacy of our sample of s-possessives are shown in Figure\nUsing the idea of a median, we can now rephrase our prediction in quantitative terms: Our data conform to this prediction, as 1 is higher on the scale than 5.5.\nAs before, this does not prove or disprove anything, as, again, we would expect some random variation.\nAgain, we will return to this issue in Chapter 6.\n5\nQuantifying research questions\nFrequency lists and mode\nRecall that I mentioned above the possibility of treating ordinal data like nominal data.\nTable\nFor completeness' sake, let me point out that there is a third measure of central tendency, that is especially suited to nominal data (but can also be applied to ordinal and cardinal data): the mode.\nThe mode is simply the most frequent value in a sample, so the modifiers of the of -possessive have a mode of 5 (or concrete touchable) and those of the s-possessive have a mode of 1 (or human) with respect to animacy (similarly, we could have said that the mode of s-possessive modifiers is discourse-old and the mode of of -possessive modifiers is discourse-new).\nThere may be more than one mode in a given sample.\nDescriptive statistics for cardinal data\nFor example, if we had found just a single additional modifier of the type abstract in the sample above (which could easily have happened), its frequency would also be five; in this case, the of -possessive modifier would have two modes (concrete touchable and abstract).\nThe concept of mode may seem useful in cases where we are looking for a single value by which to characterize a set of nominal data, but on closer inspection it turns out that it does not actually tell us very much\n: it tells us what the most frequent value is, but it does not tell us how much more frequent that value is than the next most frequent one, how many other values occur in the data at all, etc.\nThus, it is always preferable to report the frequencies of all values, and, in fact, I have never come across a corpus-linguistic study reporting modes.\nDescriptive statistics for cardinal data\nLet us turn, finally, to a design with one nominal and one cardinal variable: a test of the third of the three hypotheses introduced at the beginning of this chapter.\nAgain, it is restated here together with the background assumption from which it is derived: (15) Assumption: Short items tend to occur toward the beginning of a constiutent, long items tend to occur at the end.\nHypothesis: The s-possessive will be used with short modifiers, the ofpossessive will be used with long modifiers.\nThe constructions are operationalized as before.\nThe data used are based on the same data set as before, except that cases with proper names and pronouns are excluded.\nThe reason for this is that we already know from the first case study that pronouns, which we used as an operational definition of old information prefer the s-possessive.\nSince all pronouns are very short (regardless of whether we measure their length in terms of words, syllables or letters), including them would bias our data in favor of the hypothesis.\nThis left 20 cases of the s-possessive and 154 cases of the of -possessive.\nTo get samples of roughly equal size for expository clarity, let us select every sixth case of the of -possessive, giving us 25 cases (note that in a real study, there would be no good reason to create such roughly equal sample sizes -we would simply use all the data we have).\nThe variable Length was defined operationally as \"number of orthographic words\".\nWe can now state the following prediction:\n5\nQuantifying research questions (16) Prediction: The mean length of modifiers of the s-possessive should be smaller than that of the modifiers of the of-possessive.\nTable Means How to calculate a mean (more precisely, an arithmetic mean) should be common knowledge, but for completeness' sake, the formula is given in ( (17 𝑛\nIn other words, in order to calculate the mean of a set of values 𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 of size n, we add up all values and divide them by n (or multiply them by 1 /𝑛, which is the same thing).\nSince we have stated our hypothesis and the corresponding prediction only in terms of the modifier, we should first make sure that the heads of the two possessives do not differ greatly in length: if they did, any differences we find for the modifiers could simply be related to the fact that one of the constructions may be longer in general than the other.\nAdding up all 20 values for the s-possessive heads gives us a total of 57, so the mean is 57 /20 = 2.85.\nAdding up all 25 values of the of -possessive heads gives us a total of 59, so the mean is 59 /25 = 2.36.\nWe have, as yet, no way of telling whether this difference could be due to chance, but the two values are so close together that we will assume so for now.\nIn fact, note that there is one obvious outlier (a value that is much bigger than the others: example (a 1) in Table If we apply the same formula to the modifiers, however, we find that they differ substantially: the mean length of the s-possessive modifiers is 38 /20 = 1.9, while the mean length of the of -possessive's modifiers is more than twice as much, namely 112 /25 = 4.48.\nEven if we remove the obvious outlier, example (b 18) in Table Descriptive statistics for cardinal data Summary\nWe have looked at three case studies, one involving nominal, one ordinal and one cardinal data.\nIn each case, we were able to state a hypothesis and derive a quantitative prediction from it.\nUsing appropriate descriptive statistics (percentages, observed and expected frequencies, modes, medians and means), we were able to determine that the data conform to these predictions -i.e., that the quantitative distribution of the values of the variables Givenness (measured by Part of Speech, Animacy and Length across the conditions s-possessive and of-possessive fits the predictions formulated.\nHowever, these distributions by themselves do not prove (or, more precisely, fail to disprove) the hypotheses for two related reasons.\nFirst, the predictions are stated in relative terms, i.e. in terms of more-or-less, but they do not tell us how much more or less we should expect to observe.\nSecond, we do not know, and currently have no way of determining, whether the more-or-less that we observe reflects real differences in distribution, or whether it falls within the range of random variation that we always expect when observing tendencies.\nMore generally, we do not know how to apply the Popperian all-or-nothing research logic to quantitative predictions.\nAll this will be the topic of the next chapter.\nSignificance testing\nAs discussed extensively in Chapter 3, scientific hypotheses that are stated in terms of universal statements can only be falsified (proven to be false), but never verified (proven to be true).\nThis insight is the basis for the Popperian idea of a research cycle where the researcher formulates a hypothesis and then attempts to falsify it.\nIf they manage to do so, the hypothesis has to be rejected and replaced by a new hypothesis.\nAs long as they do not manage to do so, they may continue to treat it as a useful working hypothesis.\nThey may even take the repeated failure to falsify a hypothesis as corroborating evidence for its correctness.\nIf the hypothesis can be formulated in such a way that it could be falsified by a counterexample (and if it is clear what would count as a counterexample), this procedure seems fairly straightforward.\nHowever, as also discussed in Chapter 3, many if not most hypotheses in corpus linguistics have to be formulated in relative terms -like those introduced in Chapter 5.\nAs discussed in Section 3.1.2, individual counterexamples are irrelevant in this case: if my hypothesis is that most swans are white, this does not preclude the existence of differently-colored swans, so the hypothesis is not falsified if we come across a black swan in the course of our investigation.\nIn this chapter, we will discuss how relative statements can be investigated within the scientific framework introduced in Chapter 3.\nStatistical hypothesis testing\nObviously, if our hypothesis is stated in terms of proportions rather than absolutes, we must also look at our data in terms of proportions rather than absolutes.\nA single counterexample will not disprove our hypothesis, but what if the majority cases we come across are counterexamples?\nFor example, if we found more black swans than white swans, would this not falsify our hypothesis that most swans are white?\nThe answer is: not quite.\nWith a hypothesis stated in absolute terms, it is easy to specify how many counterexamples we need to disprove it: one.\nIf we find just one black swan, then it cannot be true that all swans are white, regardless of how many swans we have looked at and how many swans there are.\nBut with a hypothesis stated in terms of proportions, matters are different: even if the majority or even all of the cases in our data contradict it, this does not preclude the possibility that our hypothesis is true -our data will always just constitute a sample, and there is no telling whether this sample corresponds to the totality of cases from which it was drawn.\nEven if most or all of the swans we observe are black, this may simply be an unfortunate accident -in the total population of swans, the majority could still be white.\n(By the same reasoning, of course, a hypothesis is not verified if our sample consists exclusively of cases that corroborate it, since this does not preclude the possibility that in the total population, counterexamples are the majority).\nSo if relative statements cannot be falsified, and if (like universal statements) they cannot be verified, what can we do?\nThere are various answers to this question, all based in probability theory (i.e., statistics).\nThe most widely-used and broadly-accepted of these, and the one we adopt in this book, is an approach sometimes referred to as \"Null Hypothesis Significance Testing\".\nStatistical hypothesis testing\nUnless we have a very specific prediction as to exactly what proportion of our data should consist of counterexamples, we cannot draw any conclusions from a sample.\nFor most research hypotheses, we cannot specify such an exact proportion -if our hypothesis is that Most swans are white, then \"most\" could mean anything from 50.01 percent to 99.99 percent.\nBut as we will see in the next subsection, we can always specify the exact proportion of counterexamples that we would expect to find if there was a random relationship between our variables, and we can then use a sample whether such a random relationship holds (or rather, how probable it is to hold).\nStatistical hypothesis testing utilizes this fact by formulating not one, but two hypotheses -first, a research hypothesis postulating a relationship between two variables (like \"Most swans are white\" or like the hypotheses introduced in Chapter 5), also referred to as H 1 or alternative hypothesis; second, the hypothesis that there is a random relationship between the variables mentioned in the research hypothesis, also referred to as H 0 or null hypothesis.\nWe then attempt to falsify the null hypothesis and to show that the data conform to the alternative hypothesis.\nIn a first step, this involves turning the null hypothesis and the alternative hypothesis are turned into quantitative predictions concerning the intersections of the variables, as schematically shown in (1a, b): (1) a. Null hypothesis (H 0 ): There is no relationship between Variable A and Variable B. Prediction: The data should be distributed randomly across the intersections of A and B; i.e., the frequency/medians/means of the intersections should not differ from those expected by chance.\nb. Alternative hypothesis (H 1 ): There is a relationship between Variable A and Variable B such that some value(s) of A tend to co-occur with some value(s) of B.\nPrediction: The data should be distributed non-randomly across the intersections of A and B; i.e., the frequency/medians/means of some the intersections should be higher and/or lower than those expected by chance.\nOnce we have formulated our research hypothesis and the corresponding null hypothesis in this way (and once we have operationalized the constructs used in formulating them), we collect, annotate and quantify the relevant data, as discussed in the preceding chapter.\nSignificance testing\nThe crucial step in terms of statistical significance testing then consists in determining whether the observed distribution differs from the distribution we would expect if the null hypothesis were true -if the values of our variables were distributed randomly in the data.\nOf course, it is not enough to observe a difference\n-a certain amount of variation is to be expected even if there is no relationship between our variables.\nAs will be discussed in detail in the next section, we must determine whether the difference is large enough to assume that it does not fall within the range of variation that could occur randomly.\nIf we are satisfied that this is the case, we can (provisionally) reject the null hypothesis.\nIf not, we must (provisionally) reject our research hypothesis.\nIn a third step (or in parallel with the second step), we must determine whether the data conform to our research hypothesis, or, more precisely, whether they differ from the prediction of H 0 in the direction predicted by H 1 .\nIf they do (for example, if there are more white swans than black swans), we can (provisionally) accept our research hypothesis, i.e., we can continue to use it as a working hypothesis in the same way that we would continue to use an absolute hypothesis in this way as long as we do not find a counterexample.\nIf the data differ from the prediction of H 0 in the opposite direction to that predicted by our research hypothesis -for example, if there are more black than white swans -we must, of course, also reject our research hypothesis, and treat the unexpected result as a new problem to be investigated further.\nLet us now turn to a more detailed discussion of probabilities, random variation and how statistics can be used to (potentially) reject null hypotheses.\nProbabilities and significance testing\nRecall the example of a coin that is flipped onto a hard surface: every time we flip it, there is a fifty percent probability that it will come down heads, and a fifty percent probability that it will come down tails.\nFrom this it follows, for example, that if we flip a coin ten times, the expected outcome is five heads and five tails.\nHowever, as pointed out in the last chapter, this is only a theoretical expectation derived from the probabilities of each individual outcome.\nIn reality, every outcome -from ten heads to ten tails is possible, as each flip of the coin is an independent event.\nIntuitively, we know this: if we flip a coin ten times, we do not really expect it to come down heads and tails exactly five times each but we accept a certain amount of variation.\nHowever, the greater the imbalance between heads and tails, the less willing we will be to accept it as a result of chance.\nIn other words, we 6.2 Probabilities and significance testing would not be surprised if the coin came down heads six times and tails four times, or even heads seven times and tails three times, but we might already be slightly surprised if it came down heads eight times and tails only twice, and we would certainly be surprised to get a series of ten heads and no tails.\nLet us look at the reasons for this surprise, beginning with a much shorter series of just two coin flips.\nThere are four possible outcomes of such a series: (2) a. heads -heads\nb.\nheads -tails c. tails -heads d. tails -tails\nObviously, none of these outcomes is more or less probable than the others: since there are four possible outcomes, they each have a probability of 1 /4 = 0.25 (i.e., 25 percent, we will be using the decimal notation for percentages from here on).\nAlternatively, we can calculate the probability of each series by multiplying the probability of the individual events in each series, i.e. 0.5 × 0.5 = 0.25.\nCrucially, however, there are differences in the probability of getting a particular set of results (i.e, a particular number of heads and regardless of the order they occur in): There is only one possibility of getting two heads (2a) and one of getting two tails (2d), but there are two possibilities of getting one head and one tail (2b, c).\nWe calculate the probability of a particular set by adding up the probabilities of all possible series that will lead to this set.\nThus, the probabilities for the sets {heads, heads} and {tails, tails} are 0.25 each, while the probability for the set {heads, tails}, corresponding to the series heads-tails and tails-heads, is 0.25 + 0.25 = 0.5.\nThis kind of coin-flip logic (also known as probability theory), can be utilized in evaluating quantitative hypotheses that have been stated in quantitative terms.\nTake the larger set of ten coin flips mentioned at the beginning of this section: now, there are eleven potential outcomes, shown in Table Again, these outcomes differ with respect to their probability.\nThe third column of Table\nThe basic idea behind statistical hypothesis testing is simple: we calculate the probability of the result that we have observed.\nThe lower this probability is, the less likely it is to have come about by chance and the more probable is it that we will be right if we reject the null hypothesis.\nFor example, if we observed a series of ten heads and zero tails, we know that the likelihood that the deviation from the expected result of five heads and five tails is due to chance is 0.000977 (i.e. roughly a tenth of a percent).\nThis tenth of a percent is also the probability that we are wrong if we reject the null hypothesis and claim that the coin is not behaving randomly (for example, that it is manipulated in some way).\nIf we observed one heads and nine tails, we would know that the likelihood that this deviation from the expected result is 0.009766 (i.e. almost one percent).\nThus we might think that, again, if we reject the null hypothesis, this is the probability that we are wrong.\nHowever, we must add to this the probability of getting ten heads and zero tails.\nThe reason for this is that if we accept a result of 1:9 as evidence for a non-random distribution, we would also accept the even more 6.2 Probabilities and significance testing extreme result of 0:10.\nSo the probability that we are wrong in rejecting the null hypothesis is 0.000977 + 0.009766 = 0.010743.\nIn other words: the probability that we are wrong in rejecting the null hypothesis is always the probability of the observed result plus the probabilities of all results that deviate from the null hypothesis even further in the direction of the observed frequency.\nThis is called the probability of error (or simply p-value) in statistics.\nIt must be mentioned at this point that some researchers (especially opponents of null-hypothesis statistical significance testing) disagree that p can be interpreted as the probability that we are wrong in rejecting the null hypothesis, raising enough of a controversy to force the American Statistical Association to take an official stand on the meaning of p:\nInformally, a 𝑝-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.\nIt also tells us how likely we are wrong if we reject this null hypothesis: If our model (i.e. the null hypothesis) will occasionally produce our observed result (or a more extreme one), then we will be wrong in rejecting it at those occasions.\nThe 𝑝-value tells us, how likely it is that we are dealing with such an occasion.\nOf course, it does not tell us how likely it is that the null hypothesis is actually true or false -we do not know this likelihood and we can never know it.\nStatistical hypotheses are no different in this respect from universal hypotheses.\nEven if we observe a result with a probability of one in a million, the null hypothesis could be true (as we might be dealing with this one-in-a-million event), and even if we observe a result with a probability of 999 999 in a million, the null hypothesis could be false (as our result could nevertheless have come about by chance).\nThe 𝑝-value simply tells us how likely it is that our study -with all its potential faults, confounding variables, etc. -would produce the result we observe and thus how likely we are wrong on the basis of this study to reject the null hypothesis.\nThis simply means that we should not start believing in our hypothesis until additional studies have rejected the null hypothesis -an individual study may 6\nSignificance testing lead us to wrongly reject the null hypothesis, but the more studies we conduct that allow us to reject the null hypothesis, the more justified we are in treating them as corroborating our research hypothesis.\nBy convention, probability of error of 0.05 (five percent) is considered to be the limit as far as acceptable risks are concerned in statistics -if 𝑝 < 0.05 (i.e., if p is smaller than five percent), the result is said to be statistically significant (i.e., not due to chance), if it is larger, the result is said to be non-significant (i.e., likely due to chance).\nTable\nIn real life, of course, researchers do not treat these cut-off points as absolute.\nNobody would simply throw away a set of carefully collected data as soon as their calculations yielded a 𝑝-value of 0.06 or even 0.1.\nSome researchers actually report such results, calling 𝑝-values between 0.05 and 0.10 \"marginally significant\", and although this is often frowned upon, there is nothing logically wrong with it.\nEven the majority of researchers who are unwilling to report such results would take them as an indicator that additional research might be in order (especially if there is a reasonable effect size, see further below).\nThey might re-check their operational definitions and the way they were applied, they might collect additional data in order to see whether a larger data set yields a lower probability of error, or they might replicate the study with a different data set.\nNote that this is perfectly legitimate, and completely in line with the research cycle sketched out in Section 3.3 -provided we retain all of our data.\nWhat we must not do, of course, is test different data sets until we find one that gives us a significant result, and then report just that result, ignoring all 6.2 Probabilities and significance testing attempts that did not yield significant results.\nWhat we must also not do is collect an extremely large data set and then keep drawing samples from it until we happen to draw one that gives us a significant result.\nThese practices are sometimes referred to as p-hacking, and they constitute a scientific fraud (imagine a a researcher who wants to corroborate their hypothesis that all swans are white and does so by simply ignoring all black swans they find).\nClearly, what probability of error one is willing to accept for any given study also depends on the nature of the study, the nature of the research design, and a general disposition to take or avoid risk.\nIf mistakenly rejecting the null hypothesis were to endanger lives (for example, in a study of potential side-effects of a medical treatment), we might not be willing to accept a 𝑝-value of 0.05 or even 0.01.\nWhy would collecting additional data be a useful strategy, or, more generally speaking, why are corpus-linguists (and other scientists) often intent on making their samples as large as possible and/or feasible?\nNote that the probability of error depends not just on the proportion of the deviation, but also on the overall size of the sample.\nFor example, if we observe a series of two heads and eight tails (i.e., twenty percent heads), the probability of error in rejecting the null hypothesis is 0.000977 + 0.009766 + 0.043945 = 0.054688.\nHowever, if we observe a series of four heads and sixteen tails (again, twenty percent heads), the probability of error would be roughly ten times lower, namely 0.005909.\nThe reason is the following: There are 1 048 576 possible series of twenty coin flips.\nThere is still only one way of getting one head and nineteen tails, so the probability of getting one head and nineteen tails is 1 /1048576 = 0.0000009536743; however, there are already 20 ways of getting one tail and nineteen heads (so the probability is 20 /1048576 = 0.000019), 190 ways of getting two heads and eighteen tails (𝑝 = 190 /1048576 = 0.000181), 1140 ways of getting three heads and seventeen tails (𝑝 = 1140 /1048576 = 0.001087) and 4845 ways of getting four heads and sixteen tails (𝑝 = 4845 /1048576 = 0.004621).\nAnd adding up these probabilities gives us 0.005909.\nMost research designs in any discipline are more complicated than coin flipping, which involves just a single variable with two values.\nHowever, it is theoretically possible to generalize the coin-flipping logic to any research design, i.e., calculate the probabilities of all possible outcomes and add up the probabilities of the observed outcome and all outcomes that deviate from the expected outcome even further in the same direction.\nMost of the time, however, this is only a theoretical possibility, as the computations quickly become too complex to be performed in a reasonable time frame even by supercomputers, let alone by a standard-issue home computer or manually.\n6 Significance testing\nTherefore, many statistical methods use a kind of mathematical detour: they derive from the data a single value whose probability distribution is known -a socalled test statistic.\nInstead of calculating the probability of our observed outcome directly, we can then assess its probability by comparing the test statistic against its known distribution.\nMathematically, this involves identifying its position on the respective distribution and, as we did above, adding up the probability of this position and all positions deviating further from a random distribution.\nIn practice, we just have to look up the test statistic in a chart that will give us the corresponding probability of error (or p-value, as we will call it from now on).\nIn the following three sections, I will introduce three widely-used tests involving test statistics for the three types of data discussed in the previous section: the chi-square (𝜒 2 ) test for nominal data, the Wilcoxon-Mann-Whitney test (also known as Mann-Whitney U test or Wilcoxon rank sum test) for ordinal data, and Welch's 𝑡-test for cardinal data.\nI will also briefly discuss extensions of the 𝜒 2 test for more complex research designs, including those involving more than two variables.\nGiven the vast range of corpus-linguistic research designs, these three tests will not always be the ideal choice.\nIn many cases, there are more sophisticated statistical procedures which are better suited to the task at hand, be it for theoretical (mathematical or linguistic) or for practical reasons.\nHowever, the statistical tests introduced here have some advantages that make them ideal procedures for an initial statistical evaluation of results.\nFor example, they are easy to perform: we don't need more than a paper and a pencil, or a calculator, if we want to speed up things, and they are also included as standard functions in widely-used spreadsheet applications.\nThey are also relatively robust in situations where we should not really use them (a point I will return to below).\nThey are also ideal procedures for introducing statistics to novices.\nAgain, they are easy to perform and do not require statistical software packages that are typically expensive and/or have a steep learning curve.\nThey are also relatively transparent with respect to their underlying logic and the steps required to perform them.\nThus, my purpose in introducing them in some detail here is at least as much to introduce the logic and the challenges of statistical analysis, as it is to provide basic tools for actual research.\nI will not introduce the mathematical underpinnings of these tests, and I will mention alternative and/or more advanced procedures only in passing -this includes, at least for now, research designs where neither variable is nominal.\nIn these cases, correlation tests are used, such as Pearson's product-moment correlations (if are dealing with two cardinal variables) and Spearman's rank correlation 6.3\nNominal data: The chi-square test coefficient or the Kendall tau rank correlation coefficient (if one or both of our variables are ordinal).\nI will not, in other words, do much more than scratch the surface of the vast discipline of statistics.\nIn the Study Notes to this chapter, there are a number of suggestions for further reading that are useful for anyone interested in a deeper understanding of the issues introduced here, and obligatory for anyone serious about using statistical methods in their own research.\nWhile I will not be making reference to any statistical software applications, such applications are necessary for serious quantitative research; again, the Study Notes contain useful suggestions where to look.\nNominal data: The chi-square test\nAs mentioned in the preceding chapter, nominal data (or data that are best treated like nominal data) are the type of data most frequently encountered in corpus linguistics.\nI will therefore treat them in slightly more detail than the other two types, introducing different versions and (in the next chapter) extensions of the most widely used statistical test for nominal data, the chi-square (𝜒 2 ) test.\nThis test in all its variants is extremely flexible; it is thus more useful across different research designs than many of the more specific and more sophisticated procedures (much like a Swiss army knife is an excellent all-purpose tool despite the fact that there is usually a better tool dedicated to a specific task at hand).\nDespite its flexibility, there are two requirements that must be met in order for the 𝜒 2 test to be applicable: first, no intersection of variables must have a frequency of zero in the data, and second, no more than twenty-five percent of the intersections must have frequencies lower than five.\nWhen these conditions are not met, an alternative test must be used instead (or we need to collect additional data).\nTwo-by-two designs Let us begin with a two-by-two design and return to the case of discourse-old and discourse-new modifiers in the two English possessive constructions.\nHere is the research hypothesis again, paraphrased from ( (3) H 1 : There is a relationship between Discourse Status and Type of Possessive such that the s-possessive is preferred when the modifier is discourse-old, the of-possessive is preferred when the modifier is discourse-new.\nSignificance testing Prediction: There will be more cases of the s-possessive with discourseold modifiers than with discourse-new modifiers, and more cases of the of-possessive with discourse-new modifiers than with discourse-old modifiers.\nThe corresponding null hypothesis is stated in ( (4) H 0 : There is no relationship between Discourse Status and Type of Possessive.\nPrediction: Discourse-old and discourse-new modifiers will be distributed randomly across the two Possessive constructions.\nWe already reported the observed and expected frequencies in Table First, we need to assess the magnitude of the differences between observed and expected frequencies.\nThe simplest way of doing this would be to subtract the expected differences from the observed ones, giving us numbers that show for each cell the size of the deviation as well as its direction (i.e., are the observed frequencies higher or lower than the expected ones).\nFor example, the values for Table\nThe first problem is solved by squaring the differences.\nThis converts all deviations into positive numbers, and thus their sum will no longer be zero, and it has the additional effect of weighing larger deviations more strongly than smaller ones.\nThe second problem is solved by dividing the squared difference by the expected frequencies.\nThis will ensure that a deviation of a particular size will be weighed more heavily for a small expected frequency than for a large expected frequency.\nThe values arrived at in this way are referred to as the cell components 6 Significance testing of 𝜒 2 (or simply 𝜒 2 components); the formulas for calculating the cell components in this way are shown in Table\nBefore we can do so, there is a final technical point to make.\nNote that the degree of variation in a given table that is expected to occur by chance depends quite heavily on the size of the table.\nThe bigger the table, the higher the number of cells that can vary independently of other cells without changing the marginal sums (i.e., without changing the overall distribution).\nThe number of such cells that a table contains is referred to as the number of degrees of freedom of the table.\nIn the case of a two-by-two table, there is just one such cell: if we change any single cell, we must automatically adjust the other three cells in order to keep the marginal sums constant.\nThus, a two-by-two table has one degree of freedom.\nNominal data: The chi-square test\nThe general formula for determining the degrees of freedom of a table is the following, where 𝑁 𝑟𝑜𝑤𝑠 is the number of rows and 𝑁 𝑐𝑜𝑙𝑢𝑚𝑛 is the number of columns:\n(5) df = (N rows -1) × (N columns -1)\nSignificance levels of 𝜒 2 values differ depending on how many degrees of freedom a table has, so we always need to determine the degrees of freedom before we can determine the 𝑝-value.\nTurning to the table of 𝜒 2 values in Section 14.1, we first find the row for one degree of freedom (this is the first row); we then check whether our 𝜒 2 -value is larger than that required for the level of significance that we are after.\nIn our case, the value of 272.16 is much higher than the 𝜒 2 value required for a significance level of 0.001 at one degree of freedom, which is 10.83.\nThus, we can say that the differences in Table (6) Format for reporting the results of a 𝜒 2 test\nIn the present case, the analysis might be summarized along the following lines: \"This study has shown that s-possessives are preferred when the modifier is discourse-old while of -possessives are preferred when the modifier is discourse-new.\nThe differences between the constructions are highly significant (𝜒 2 = 272.16, df = 1, 𝑝 < 0.001)\".\nA potential danger to this way of formulating the results is the meaning of the word significant.\nIn statistical terminology, this word simply means that the results obtained in a study based on one particular sample are unlikely to be due to chance and can therefore be generalized, with some degree of certainty, to the entire population.\nIn contrast, in every-day usage the word means something along the lines of 'having an important effect or influence'\nFirst, and perhaps most obviously, statistical significance has nothing to do with the validity of the operational definitions used in our research design.\nIn our case, this validity is reasonably high, provided that we limit our conclusions to written English.\nAs a related point, statistical significance has nothing to do with the quality of our data.\nIf we have chosen unrepresentative data or if we have extracted or annotated our data sloppily, the statistical significance of the results is meaningless.\nSignificance testing\nSecond, statistical significance has nothing to do with theoretical relevance.\nPut simply, if we have no theoretical model in which the results can be interpreted meaningfully, statistical significance does not add to our understanding of the object of research.\nIf, for example, we had shown that the preference for the two possessives differed significantly depending on the font in which a modifier is printed, rather than on the discourse status of the modifier, there is not much that we conclude from our findings.\nFirst, there are generally agreed-upon verbal descriptions for different ranges that the value of a correlation coefficient may have (similarly to the verbal descriptions of p-values discussed above.\nThese descriptions are shown in Table Significance testing\nUnfortunately, studies in corpus linguistics (and in the social sciences in general) often fail to report effect sizes, but we can usually calculate them from the data provided, and one should make a habit of doing so.\nMany effects reported in the literature are actually somewhat weaker than the significance levels might lead us to believe.\nOne-by-𝑛 designs\nIn the vast majority of corpus linguistic research issues, we will be dealing with designs that are at least bivariate (i.e., that involve the intersection of at least two variables), like the one discussed in the preceding section.\nHowever, once in a while we may need to test a univariate distribution for significance (i.e., a distribution of values of a single variable regardless of any specific condition).\nWe may, for instance, have annotated an entire corpus for a particular speaker variable (such as sex), and we may now want to know whether the corpus is actually balanced with respect to this variable.\nConsider the following example: the spoken part of the BNC contains language produced by 1317 female speakers and 2311 male speakers (as well as 1494 speakers whose sex is unknown, which we will ignore here).\nIn order to determine whether the BNC can be considered a balanced corpus with respect to Speaker Sex, we can compare this observed distribution of speakers to the expected one more or less exactly in the way described in the previous sections except that we have two alternative ways of calculating the expected frequencies.\nFirst, we could simply take the total number of elements and divide it by the number of categories (values), on the assumption that \"random\" distribution means that every category should occur with the same frequency.\nIn this case, the expected number of male and female speakers would be [Total Number of Speakers / Sex Categories], i.e. 3628 /2 = 1814.\nWe can now calculate the 𝜒 2 components just as we did in the preceding sections, using the formula ((𝑂 -𝐸) 2 )/𝐸. Table\nAdding up the components gives us a 𝜒 2 value of 272.34.\nA one-by-two table has one degree of freedom (if we vary one cell, we have to adjust the other one automatically to keep the marginal sum constant).\nChecking the appropriate row in the table in Section 14.1, we can see that this value is much higher than the 10.83 required for a significance level of 0.01.\nThus, we can say that \"the BNC corpus contains a significantly higher proportion of male speakers than expected by chance (𝜒 2 = 272.34, df = 1, 𝑝 < 0.001)\" -in other words, the corpus is not balanced well with respect to the variable Speaker Sex (note that since this is a test of proportions rather than correlations, we cannot calculate a phi value here).\n6.3 Nominal data: The chi-square test\nThe second way of deriving expected frequencies for a univariate distribution is from prior knowledge concerning the distribution of the values in general.\nIn our case, we could find out the proportion of men and women in the relevant population and then derive the expected frequencies for our table by assuming that they follow this proportion.\nThe relevant population in this case is that of the United Kingdom between 1991 and 1994, when the BNC was assembled.\nAccording to the World Bank, the women made up 51.4 percent and men 48.6 percent of the total population at that time, so the expected frequencies of male and female speakers in the corpus are as shown in Table\nIncidentally, the BNC not only contains speech by more male speakers than female speakers, it also includes more speech by male than by female speakers: men contribute 5 654 348 words, women contribute 3 825 804.\nI will leave it as an exercise to the reader to determine whether and in what direction these frequencies differ from what would be expected either under an assumption of equal proportions or given the proportion of female and male speakers in the corpus.\nIn the case of speaker sex it does not make much of a difference how we derive the expected frequencies, as men and women make up roughly half of the population each.\nFor variables where such an even distribution of values does not exist, the differences between these two procedures can be quite drastic.\nAs an example, consider Table The exact alternative to the univariate 𝜒 2 test with a two-level variable is the binomial test, which we used (without calling it that), in our coin-flip example in Section 6.2 above and which is included as a predefined function in many major spreadsheet applications and in R; for one-by-n tables, there is a multinomial test also available in R and other statistics packages.\nOrdinal data: The Mann-Whitney U -test Where one variable is nominal (more precisely, nominal with two values) and one is ordinal\n, the most widely used test statistic is the Mann-Whitney U -test (also called Wilcoxon rank sum test).\nLet us return to the case study of the animacy of modifiers in the two English possessive constructions.\nHere is the research hypothesis again, from (\nThe corresponding null hypothesis is stated in ( (9) H 0 : There is no relationship between Animacy and Type of Possessive.\nPrediction: There will be no difference between the medians of the modifiers of the s-possessive and the of-possessive on the Animacy scale.\nThe median animacy of all modifiers in our sample taken together is 2, 5 so the H 0 predicts that the medians of s-possessive and the of -possessive should 6 Significance testing also be 2.\nRecall that the observed median animacy in our sample was 1 for the spossessive and 5 for the of -possessive, which deviates from the prediction of the H 0 in the direction of our H 1 .\nHowever, as in the case of nominal data, a certain amount of deviation from the null hypothesis will occur due to chance, so we need a test statistic that will tell us how likely our observed result is.\nFor ordinal data, this test statistic is the U value, which is calculated as follows.\nIn a first step, we have to determine the rank order of the data points in our sample.\nFor expository reasons, let us distinguish between the rank value and the rank position of a data point: the rank value is the ordinal value it received during annotation (in our case, its value on the Animacy scale), its rank position is the position it occupies in an ordered list of all data points.\nIf every rank value occurred only once in our sample, rank value and rank position would be the same.\nHowever, there are 41 data points in our sample, so the rank positions will range from 1 to 41, and there are only 10 rank values in our annotation scheme for Animacy.\nThis means that at least some rank values will occur more than once, which is a typical situation for corpus-linguistic research involving ordinal data.\nTable\nEvery rank value except 4, 8 and 9 occurs more than once; for example, there are sixteen cases that have an Animacy rank value of 1 and six cases that have a rank value of 2, two cases that have a rank value of 3, and so on.\nThis means we cannot simply assign rank positions from 1 to 41 to our examples, as there is no way of deciding which of the sixteen examples with the rank value 1 should receive the rank position 1, 2, 3, etc.\nInstead, these 16 examples as a group share the range of ranks from 1 to 16, so each example gets the mean rank position of this range.\nThere are sixteen cases with rank value 1, to their mean rank is 6 Significance testing\nOnce we have determined the rank position of each data point, we separate them into two subsamples corresponding to the values of the nominal variable Type of Possessive again, as in Table\nThe rank sum and the number of data points for each sample allow us to calculate the U values for both group using the following simple formulas:\nApplying these formulas to the measures for the s-possessive (10a) and ofpossessive (10b) respectively, we get the U values\nThe U value for the entire data set is always the smaller of the two U values.\nIn our case this is 𝑈 2 , so our U value is 48.5.\nThis value can now be compared against its known distribution in the same way as the 𝜒 2 value for nominal data.\nIn our case, this means looking it up in the table in Section 14.3 in the Statistical Tables at the end of this book, which tells us that the 𝑝-value for this U value is smaller than 0.001 -the difference between the s-and the of -possessive is, again, highly significant.\nThe Mann-Whitney U test may be reported as follows:\n(11) Format for reporting the results of a Mann-Whitney test Thus, we could report the results of this case study as follows: \"This study has shown that s-possessives are preferred when the modifier is high in animacy, while of -possessives are preferred when the modifier is low in animacy.\nA Mann-Whitney test shows that the differences between the constructions are highly significant (𝑈 = 48.5, 𝑁 1 = 18, 𝑁 2 = 23, 𝑝 < 0.001)\".\nInferential statistics for cardinal data\nWhere one variable is nominal (more precisely, nominal with two values) and one is cardinal, the a widely-used test is the t-test, of which there are two wellknown versions, Welch's t-test and Student's t-test, that differ in terms of the requirements that the data must meet in order for them to be applicable.\nIn the following, I will introduce Welch's 𝑡-test, which can be applied more broadly, although it still has some requirements that I will return to below.\n6 Significance testing Welch's 𝑡-test\nLet us return to the case study of the length of modifiers in the two English possessive constructions.\nHere is the research hypothesis again, paraphrased slightly from ( (12) H 1 : The s-possessive will be used with short modifiers, the of-possessive will be used with long modifiers.\nPrediction: The mean Length (in \"number of words\") of modifiers of the s-possessive should be smaller than that of the modifiers of the of-possessive.\nThe corresponding null hypothesis is stated in ( (13) H 0 : There is no relationship between Length and Type of Possessive.\nPrediction: There will be no difference between the mean length of the modifiers of the s-possessive and the of-possessive.\nTable First, note that one case that was still included in Table In order to calculate Welch's t-test, we determine three values on the basis of our measurements of Length: the number of measurements N, the mean length for each group (𝑥), and a value called \"sample variance\" (𝑠 2 ).\nThe number of measurements is easy to determine -we just count the cases in each group: 20 s-possessives and 24 of -possessives.\nWe already calculated the mean lengths in Chapter 5: for the s-possessive, the mean length is 1.9 words, for the of -possessive it is 3.83 words.\nAs we already discussed in Chapter 5, this difference conforms to our hypothesis: s-possessives are, on average, shorter than of -possessives.\nThe question is, again, how likely it is that this difference is due to chance.\nWhen comparing group means, the crucial question we must ask in order to determine this is how large the variation is within each group of measurements: put simply, the more widely the measurements within each group vary, the more likely it is that the differences across groups have come about by chance.\nThe first step in assessing the variation consists in determining for each measurement, how far away it is from its group mean.\nThus, we simply subtract each measurement for the s-possessive from the group mean of 1.9, and each measurement for the of-possessive from the group mean of 3.83.\nThe results are shown in the third column of each sub-table in Table Inferential statistics for cardinal data The sample variances themselves cannot be very easily interpreted (see further below), but we can use them to calculate our test statistic, the t-value, using the following formula (𝑥 stands for the group mean, s 2 stands for the sample variance, and N stands for the number of cases; the subscripts 1 and 2 indicate the two subsamples: Note that this formula assumes that the measures with the subscript 1 are from the larger of the two samples (if we don't pay attention to this, however, all that happens is that we get a negative t-value, whose negative sign we can simply ignore).\nIn our case, the sample of of -possessives is the larger one, giving us: As should be familiar by now, we compare this t-value against its distribution to determine the probability of error (i.e., we look it up in the table in Again, the subscripts indicate the sub-samples, s 2 is the sample variance, and 𝑁 is the number of items the degrees of freedom for the two groups (df 1 and df 2 ) are defined as 𝑁 -1.\nIf we apply the formula to our data, we get the following: df ≈ (\nThus, a straightforward way of reporting our results would be something like this: \"This study has shown that for modifiers that are realized by lexical NPs, s-possessives are preferred when the modifier is short, while of -possessives are preferred when the modifier is long.\nThe difference between the constructions is very significant (𝑡(25.50) = 3.5714, 𝑝 < 0.01)\".\nAs pointed out above, the value for the sample variance does not, in itself, tell us very much.\nWe can convert it into something called the sample standard deviation, however, by taking its square root.\nThe standard deviation is an indicator of the amount of variation in a sample (or sub-sample) that is frequently reported; it is good practice to report standard deviations whenever we report means.\nFinally, note that, again, the significance level does not tell us anything about the size of the effect, so we should calculate an effect size separately.\nThe most widely-used effect size for data analyzed with a t-test is Cohen's d, also referred to as the standardized mean difference.\nThere are several ways to calculate it, the simplest one is the following, where 𝜎 is the standard deviation of the entire sample: 6 Significance testing\nFor our case study, this gives us 𝑑 = 3.8333 -1.9 2.1562 = 0.8966\nThis standardized mean difference can be converted to a correlation coefficient by the formula in (\nFor our case study, this gives us Since this is a correlation coefficient, it can be interpreted as described in Table 6.6 above.\nIt falls into the moderate range, so a more comprehensive way of summarizing the results of this case study would be the following:\n\"This study has shown that length has a moderate, statistically significant influence on the choice of possessive constructions with lexical NPs in modifier position: s-possessives are preferred when the modifier is short, while of -possessives are preferred when the modifier is long.\n(𝑡(25.50) = 3.5714, 𝑝 < 0.01, 𝑟 = 0.41)\".\nNormal distribution requirement\nIn the context of corpus linguistics, there is one fundamental problem with the t-test in any of its variants: it requires data that follow what is called the normal distribution.\nBriefly, the normal distribution is a probability distribution where most measurements fall in the middle, decreasing on either side until they reach zero.\nFigure\nYou will often read that many natural phenomena approximate this distribution -examples mentioned in textbooks are, invariably, the size and weight of organisms, frequently other characteristics of organisms such as skin area, blood pressure or IQ, and occasionally social phenomena like test scores and salaries.\nFigure -\nThere are three broad ways of dealing with this issue.\nFirst, we could ignore it and hope that the t-test is robust enough to yield meaningful results despite this violation of the normality requirement.\nIf this seems like a bad idea, this is because it is fundamentally a bad idea -and statisticians warn against it categorically.\nHowever, many social scientists regularly adopt this approach -just like we did in the case study above.\nAnd in practice, this may be less of a problem than one might assume, since the t-test has been found to be fairly robust against violations of the normality requirement.\nHowever, we should not generally rely on this robustness, as linguistic data may depart from normality to quite an extreme degree.\nMore generally, ignoring the prerequisites of a statistical procedure is not exactly good scientific practice -the only reason I did it above was so you would not be too shocked when you see it done in actual research (which, inevitably, you will).\nSecond, and more recommendably, we could try to make the data fit the normality requirement.\nOne way in which this is sometimes achieved in the many cases where data do not follow the normal distribution is to log-transform the data (i.e., use the natural logarithm of the data instead of the data themselves).\nThis often, but not always, causes the data to approximate a normal distribution 6 Significance testing more closely.\nHowever, this does not work in all cases (it would not, for example, bring the distribution in Figure\nThus, third, and most recommendably, we could try to find a way around having to use a t-test in the first place.\nOne way of avoiding a t-test is to treat our non-normally distributed cardinal data as ordinal data, as described in Chapter 5.\nWe can then use the Mann-Whitney U -test, which does not require a normal distribution of the data.\nI leave it as an exercise to the reader to apply this test to the data in Table\nAnother way of avoiding the t-test is to find an operationalization of the phenomenon under investigation that yields rank data, or, even better, nominal data in the first place.\nWe could, for example, code the data in Table\nThe normal-distribution requirement is only one of several requirements that our data set must meet in order for particular statistical methods to be applicable.\nComplex research designs\nFor example, many procedures for comparing group means -including the more widely-used Student 𝑡-test -can only be applied if the two groups have the same variance (roughly, if the measurements in both groups are spread out from the group means to the same extent), and there are tests to tell us this (for example, the F test).\nAlso, it makes a difference whether the two groups that we are comparing are independent of each other (as in the case studies presented here), or if they are dependent in that there is a correspondence between measures in the two groups.\nFor example, if we wanted to compare the length of heads and modifiers in the s-possessive, we would have two groups that are dependent in that for any data point in one of the groups there is a corresponding data point in the other group that comes from the same corpus example.\nIn this case, we would use a paired test (for example, the matched-pairs Wilcoxon test for ordinal data and Student's paired 𝑡-test for cardinal data).\nComplex research designs\nIn Chapter 5 and in this chapter so far, we have restricted our discussion to the simplest possible research designs -cases where we are dealing with two variables with two values each.\nTo conclude our discussion of statistical hypothesis testing, we will look at two cases of more complex designs -one with two variables that each have more than two values, and one with more than two variables.\nVariables with more than two values\nIn our case studies involving the English possessive constructions, the dependent variable (Type of) Possessive was treated as binary -we assumed that it had two values, s-possessive and of-possessive.\nThe dependent variables were more complex: the cardinal variable Length obviously has a potentially infinite number of values and the ordinal variable Animacy was treated as having ten values in our annotation scheme.\nThe nominal value Discourse Status, was treated like a binary variable (although potentially it has an infinite number of values, too).\nFrequently, perhaps even typically, corpus linguistic research questions will be more complex, and we will be confronted with designs where both the dependent and the independent variable will have (or be treated as having) more than two values.\nSince we are most likely to deal with nominal variables in corpus linguistics, we will discuss in detail an example where both variables are nominal.\nIn the preceding chapters we treated as s-possessive constructions where the modifier is a possessive pronoun as well as constructions where the modifier is 6 Significance testing a proper name or a noun with a possessive clitic.\nGiven that the proportion of pronouns and nouns in general varies across language varieties Searching the BNC Baby for words tagged as possessive pronouns and for words tagged unambiguously as proper names or common nouns yields the observed frequencies shown in the first line of each row in Table\nRecall that the mere fact of a significant association does not tell us anything about the strength of that association -we need a measure of effect size.\nIn the preceding chapter, 𝜙 was introduced as an effect size for two-by-two tables (see 7).\nFor larger tables, there is a generalized version of 𝜙, referred to as Cramer's V (or, occasionally, as Cramer's 𝜙 or 𝜙 ′ ), which is calculated as follows (N is the table sum, k is the number of rows or columns, whichever is smaller): For our table, this gives us: Recall that the square of a correlation coefficient tells us the proportion of the variance captured by our design, which, in this case, is 0.0275.\nIn other words, Variety explains less than three percent of the distribution of s-possessor modifier types across language varieties; or \"This study has shown a very weak but highly significant influence of language variety on the realization of s-possessor modifiers as pronouns, proper names or common nouns (𝜒 2 = 473.73, df = 12, 𝑝 < 0.001, 𝑟 = 0.0275). \"\n6 Significance testing\nDespite the weakness of the effect, this result confirms our expectation that general preferences for pronominal vs. nominal reference across language varieties is also reflected in preferences for types of modifiers in the s-possessive.\nHowever, with the increased size of the contingency table, it becomes more difficult to determine exactly where the effect is coming from.\nMore precisely, it is no longer obvious at a glance which of the intersections of our two variables contribute to the overall significance of the result in what way and to what extent.\nTo determine in what way a particular intersection contributes to the overall result, we need to compare the observed and expected frequencies in each cell.\nFor example, there are 9593 cases of s-possessives with pronominal modifiers in spoken language, where 8378.38 are expected, showing that pronominal modifiers are more frequent in spoken language than expected by chance.\nIn contrast, there are 8533 such modifiers in academic language, where 8959.86 are expected, showing that they are less frequent in academic language than expecte by chance.\nThis comparison is no different from that which we make for two-by-two tables, but with increasing degrees of freedom, the pattern becomes less predictable.\nIt would be useful to visualize the relation between observed and expected frequencies for the entire table in a way that would allow us to take them in at a glance.\nTo determine to what extent a particular intersection contributes to the overall result, we need to look at the size of the 𝜒 2 components -the larger the component, the greater its contribution to the overall 𝜒 2 value.\nIn fact, we can do more than simply compare the 𝜒 2 components to each other -we can determine for each component, whether it, in itself, is statistically significant.\nIn order to do so, we first imagine that the large contingency table (in our case, the 4-by-3 table) consists of a series of tables with a single cell each, each containing the result for a single intersection of our variables.\nWe now treat the 𝜒 2 component as a 𝜒 2 value in its own right, checking it for statistical significance in the same way as the overall 𝜒 2 value.\nIn order to do so, we first need to determine the degrees of freedom for our one-cell tables -obviously, this can only be 1.\nChecking the table of critical 𝜒 2 values in Section 14.1, we find, for example, that the 𝜒 2 component for the intersection pronoun ∩ spoken, which is 176.08, is higher than the critical value 10.83, suggesting that this intersection's contribution is significant at 𝑝 < 0.001.\nHowever, matters are slightly more complex: by looking at each intersection separately, we are essentially treating each cell as an independent result -in our case, it is as if we had performed twelve tests instead of just one.\nNow, recall that levels of significance are based on probabilities of error -for example, 𝑝 = 0.05 means, roughly, that there is a five percent likelihood that a result is due to chance.\nObviously, the more tests we perform, the more likely it becomes that one 6.6\nComplex research designs of the results will, indeed, be due to chance -for example, if we had performed twenty tests, we would expect one of them to yield a significant result at the 5-percent level, because 20 × 0.05 = 1.00.\nTo avoid this situation, we have to correct the levels of significance when performing multiple tests on the same set of data.\nThe simplest way of doing so is the so-called Bonferroni correction, which consists in dividing the conventionally agreed-upon significance levels by the number of tests we are performing.\nIn the case of Table There is no standard way of representing the way in, and degree to, which each cell of a complex table contributes to the overall result, but the representation in Table This table presents the complex results at a single glance; they can now be interpreted.\nSome patterns now become obvious: For example, spoken language and fiction are most similar to each other -they both favor pronominal modifiers, while proper names and common nouns are disfavored, and the 𝜒 2 -components for these preferences are very similar.\nAlso, if we posit a kind of gradient of referent familiarity from pronouns over proper names to nouns, we can place spoken language and fiction at one end, academic language at the other, and newspaper language somewhere in the middle.\nDesigns with more than two variables Note that from a certain perspective the design in Table But even then, it would be useful to treat Medium and Discourse Domain as independent variables, just in case our model is wrong in assuming this.\nIn contrast to all examples of research designs we have discussed so far, which involved just two variables and were thus bivariate, this design would be multivariate: there is more than one independent variable whose influcence on the dependent variable we wish to assess.\nSuch multivariate research designs are often useful (or even necessary) even in cases where the variables in our design are not conflations of more basic variables.\nComplex research designs\nIn the study of language use, we will often -perhaps even typically -be confronted with a fragment of reality that is too complex to model in terms of just two variables.\nIn some cases, this may be obvious from the outset: we may suspect from previous research that a particular linguistic phenomenon depends on a range of factors, as in the case of the choice between the s-and the of -possessive, which we saw in the preceding chapters had long been hypothesized to be influenced by the animacy, the length and/or the givenness of the modifier.\nIn other cases, the multivariate nature of the phenomenon under investigation may emerge in the course of pursuing an initially bivariate design.\nFor example, we may find that the independent variable under investigation has a statistically significant influence on our dependent variable, but that the effect size is very small, suggesting that the distribution of the phenomenon in our sample is conditioned by more than one influencing factor.\nEven if we are pursuing a well-motivated bivariate research design and find a significant influence with a strong effect size, it may be useful to take additional potential influencing factors into account: since corpus data are typically unbalanced, there may be hidden correlations between the variable under investigation and other variables, that distort the distribution of the phenomenon in a way that suggests a significant influence where no such influence actually exists.\nThe next subsection will use the latter case to demonstrate the potential shortcomings of bivariate designs and the subsection following it will present a solution.\nNote that this solution is considerably more complex than the statistical procedures we have looked at so far and while it will be presented in sufficient detail to enable the reader in principle to apply it themselves, some additional reading will be highly advisable.\nA danger of bivariate designs\nIn recent years, attention has turned to sociolinguistic factors potentially influencing the choice between the s-possessive and the of-construction.\nIt has long been known that the level of formality has an influence Significance testing\nLet us take a look at the influence of Sex and Age on the choice between the two possessives in the spoken part of the BNC.\nSince it is known that women tend to use pronouns more than men do (see Case Study 10.2.3.1 in Chapter 10), let us exclude possessive pronouns and operationalize the s-possessive as \"all tokens tagged POS in the BNC\", which will capture the possessive clitic 's and zero possessives (on common nouns ending in alveolar fricatives).\nSince the spoken part of the BNC is too large to identify of -possessives manually, let us operationalize them somewhat crudely as \"all uses of the preposition of \"; this encompasses not just of -possessives, but also the quantifying and partitive of -constructions that we manually excluded in the preceding chapters, the complementation of adjectives like aware and afraid, verbs like consist and dispose, etc.\nOn the one hand, this makes our case study less precise, on the other hand, any preference for ofconstructions may just be a reflex of a general preference for the preposition of, in which case we would be excluding relevant data by focusing on of -constructions.\nAnyway, our main point will be one concerning statistical methodology, so it does not matter too much either way.\nSo, let us query all tokens tagged as possessives (POS) or the preposition of (PRF) in the spoken part of the BNC, discarding all hits for which the information about speaker sex or speaker age is missing.\nLet us further exclude the age range 0-14, as it may include children who have not fully acquired the grammar of the language, and the age range 60+ as too unspecific.\nTo keep the design simple, let us recode all age classes between 15 and 44 years of age as young and the age range 45-59 als old (I fall into the latter, just in case someone thinks this category label discriminates people in their prime).\nLet us further accept the categorization of speakers into male and female that the makers of the BNC provide.\nTable\nUnlike the studies mentioned above, we find a clear influence of Sex on Construction, with female speakers preferring the s-possessive and male speakers preferring the of -construction(s).\nThe difference is highly significant, although the effect size is rather weak (𝜒 2 = 773.55, df = 1, 𝑝 < 0.001, 𝜙 = 0.1061).\nNext, let us look at the intersections of Construction and Sex in the results of our query, which are shown in Table\nLike previous studies, we find a significant effect of age, with younger speakers preferring the s-possessive and older speakers preferring the of -construction(s).\nAgain, the difference is highly significant, but the effect is extremely weak (𝜒 2 = 58.73, df = 1, 𝑝 < 0.001, 𝜙 = 0.02922).\nWe might now be satisfied that both speaker age and speaker sex have an influence on the choice between the two constructions.\nHowever, there is a potential problem that we need to take into account: the values of the variables Sex and 6.6 Complex research designs Age and their intersections are not necessarily distributed evenly in the subpart of the BNC used here; although the makers of the corpus were careful to include a broad range of speakers of all ages, sexes (and class memberships, ignored in our study), they did not attempt to balance all these demographic variables, let alone their intersections.\nSo let us look at the intersection of Sex and Age in the results of our query.\nThese are shown in Table\nThere are significantly fewer hits produced by old women and significantly more produced by young women in our sample, and, conversely, significantly fewer hits produced by young men and significantly more produced by old men.\nThis overrepresentation of young women and old men is not limited to our sample, but characterizes the spoken part of the BNC in general, which should intrigue feminists and psychoanalysts; for us, it suffices to know that the asymmetries in our sample are highly significant, with an effect size larger than that of that in the preceding two tables (𝜒 2 = 2142.72, df = 1, 𝑝 < 0.001, 𝜙 = 0.1765).\n6 Significance testing\nThis correlation in the corpus of old and male on the one hand and young and female on the other may well be enough to distort the results such that a linguistic behavior typical for female speakers may be wrongly attributed to young speakers (or vice versa), and, correspondingly, a linguistic behavior typical for male speakers may be wrongly interpreted to old speakers (or vice versa).\nMore generally, the danger of bivariate designs is that a variable we have chosen for investigation is correlated with one or more variables ignored in our research design, whose influence thus remains hidden.\nA very general precaution against this possibility is to make sure that the corpus (or our sample) is balanced with respect to all potentially confounding variables.\nIn reality, this is difficult to achieve and may in fact be undesirable, since we might, for example, want our corpus (or sample) to reflect the real-world correlation of speaker variables).\nTherefore, we need a way of including multiple independent variables in our research designs even if we are just interested in a single independent variable, but all the more so if we are interested in the influence of several independent variables.\nIt may be the case, for example, that both Sex and Age influence the choice between 's and of, either in that the two effects add up, or in that they interact in more complex ways.\nConfigural frequency analysis\nThere is a range of multivariate statistical methods that are routinely used in corpus linguistics, such as the ANOVA mentioned at the end of the previous chapter for situations where the dependent variable is measured in terms of cardinal numbers, and various versions of logistic regression for situations where the dependent variable is ordinal or nominal.\nIn this book, I will introduce multivariate designs using Configural Frequency Analysis (CFA), a straightforward extension of the 𝜒 2 test to designs with more 6.6 Complex research designs than two nominal variables.\nThis method has been used in psychology and psychiatry since the 1970s, and while it has never become very wide-spread, it has, in my opinion, a number of didactic advantages over other methods, when it comes to understanding multivariate research designs.\nMost importantly, it is conceptually very simple (if you understand the 𝜒 2 test, you should be able to understand CFA), and the results are very transparent (they are presented as observed and expected frequencies of intersections of variables.\nThis does not mean that CFA is useful only as a didactic tool -it has been applied fruitfully to linguistic research issues, for example, in the study of language disorders\nAs hinted at above, in its simplest variant, configural frequency analysis is simply a 𝜒 2 test on a contingency table with more than two dimensions.\nThere is no logical limit to the number of dimensions, but if we insist on calculating this statistic manually (rather than, more realistically, letting a specialized software package do it for us), then a three-dimensional table is already quite complex to deal with.\nThus, we will not go beyond three dimensions here or in the case studies in the second part of this book.\nA three-dimensional contingency table would have the form of a cube, as shown in Figure While this kind of visualization is quite useful in grasping the notion of a three-dimensional contingency table, it would be awkward to use it as a basis for recording observed frequencies or calculating the expected frequencies.\nThus, a possible two-dimensional representation is shown in Table\nIn our case, each variable has two values, thus we get (2×2×2)-(2+2+2)+2 = 4.\nMore interestingly, we can also look at the individual cells to determine whether their contribution to the overall value is significant).\nIn this case, as before, each cell has one degree of freedom and the significance levels have to be adjusted for multiple testing.\nIn CFA, an intersection of variables whose observed frequency is significantly higher than expected is referred to as a type and one whose observed frequency is significantly lower is referred to as an antitype (but if we do not like this terminology, we do not have to use it and can keep talking about \"more or less frequent than expected\", as we do with bivariate 𝜒 2 tests).\nLet us apply this method to the question described in the previous sub-section.\nTable\nThe result is very interesting.\nA careful inspection of the individual cells shows that age does not, in fact, have a significant influence.\nYoung women use the s-possessive more frequently than expected and old women use it less (in the latter case, non-significantly), but young women also use the of -constructions significantly more frequently than expected and old women use it less.\nCrucially, young men use the s-possessive less frequently than expected, and old men use it more, but young men also use the of -construction less frequently than expected and old men use it more.\nIn other words, young women and old men use more of both constructions than young men and old women.\nA closer look at the contributions to 𝜒 2 tells us that Sex, however, does still have an influence on the choice between the two constructions even when Age is taken into account: for young women, the overuse of the s-possessive is more pronounced than that of the of -construction, while for old women, underuse of the s-possessive is less pronounced than underuse of the of -construction.\nIn other words, taking into account that old women are 6.6 Complex research designs underrepresented in the corpus compared to young women, there is a clear preference of all women for the s-possessive.\nConversely, young men's underuse of the s-possessive is less pronounced than that of the of -construction, while old men's overuse of the s-possessive is less pronounced than their overuse of the of -construction.\nIn other words, taking into account than young men are underrepresented in the corpus compared to old men, there is a clear preference of all men for the of -construction.\nArmed with this new insight from our multivariate analysis, let us return to bivariate analyses, looking at each of the two variables while keeping the other constant.\nTable\nIn contrast, Tables 6.22a and 6.22b show that the effect of Age that we saw in Table This section is intended to impress on the reader one thing: that looking at one potential variable influencing some phenomenon that we are interested in may not be enough.\nMultivariate research designs are becoming the norm rather than the exception, and rightly so.\nExcluding the danger of hidden variables is just one advantage of such designs -in many cases, it is sensible to include several independent variables simply because all of them potentially have an interesting influence on the phenomenon under investigation, or because there is just one particular combination of values of our variables that has an effect.\nIn the second part of this volume, there are several case studies that use CFA and that illustrate these possibilities.\nOne word of warning, however: the ability to include a large number of variables in our research designs should not lead us to do so for the sake of it.\nWe should be able to justify, for each dependent variable we include, why we are including it and in what way we expect it to influence our independent variable.\nCollocation\nThe (orthographic) word plays a central role in corpus linguistics.\nAs suggested in Chapter 4, this is in no small part due to the fact that all corpora, whatever additional annotations may have been added, consist of orthographically represented language.\nThis makes it easy to retrieve word forms.\nEvery concordancing program offers the possibility to search for a string of characters -in fact, some are limited to this kind of query.\nHowever, the focus on words is also due to the fact that the results of corpus linguistic research quickly showed that words (individually and in groups) are more interesting and show a more complex behavior than traditional, grammarfocused theories of language assumed.\nAn area in which this is very obvious, and which has therefore become one of the most heavily researched areas in corpus linguistics, is the way in which words combine to form so-called collocations.\nThis chapter is dedicated entirely to the discussion of collocation.\nAt first, this will seem like a somewhat abrupt shift from the topics and phenomena we have discussed so far -it may not even be immediately obvious how they fit into the definition of corpus linguistics as \"the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus\", which was presented at the end of Chapter 2.\nHowever, a closer look will show that studying the co-occurrence of words and/ or word forms is simply a special case of precisely this kind of research program.\nCollocates Trivially, texts are not random sequences of words.\nThere are several factors influencing the likelihood of two (or more) words occurring next to each other.\nFirst, the co-occurrence of words in a sequence is restricted by grammatical considerations.\nFor example, a definite article cannot be followed by another definite article or a verb, but only by a noun, by an adjective modifying a noun, by an adverb modifying such an adjective or by a post-determiner.\nLikewise, a transitive verb requires a direct object in the form of a noun phrase, so -barring cases where the direct object is pre-or post-posed -it will be followed by a word that can occur at the beginning of a noun phrase (such as a pronoun, a determiner, an adjective or a noun).\nSecond, the co-occurrence of words is restricted by semantic considerations.\nFor example, the transitive verb drink requires a direct object referring to a liquid, so it is probable that it will be followed by words like water, beer, coffee, poison, etc., and improbable that it will be followed by words like bread, guitar, stone, democracy, etc.\nSuch restrictions are treated as a grammatical property of words (called selection restrictions) in some theories, but they may also be an expression of our world knowledge concerning the activity of drinking.\nFinally, and related to the issue of world knowledge, the co-occurrence of words is restricted by topical considerations.\nWords will occur in sequences that correspond to the contents we are attempting to express, so it is probable that co-occurring content words will come from the same discourse domain.\nHowever, it has long been noted that words are not distributed randomly even within the confines of grammar, lexical semantics, world knowledge, and communicative intent.\nInstead, a given word will have affinities to some words, and disaffinities to others, which we could not predict given a set of grammatical rules, a dictionary and a thought that needs to be expressed.\nOne of the first principled discussions of this phenomenon is found in\nOne of the meanings of ass is its habitual collocation with an immediately preceding you silly, and with other phrases of address or of personal reference.\n...\nThere are only limited possibilities of collocation with preceding adjectives, among which the commonest are silly, obstinate, stupid, awful, occasionally egregious.\nYoung is much more frequently found than old.\nNote that Firth, although writing well before the advent of corpus linguistics, refers explicitly to frequency as a characteristic of collocations.\nThe possibility of using frequency as part of the definition of collocates, and thus as a way of identifying them, was quickly taken up.\nCollocation is the syntagmatic association of lexical items, quantifiable, textually, as the probability that there will occur, at n removes (a distance of n lexical items) from an item x, the items a, b, c...\nAny given item thus enters into a range of collocation, the items with which it is collocated being ranged from more to less probable...\nCollocation as a quantitative phenomenon\nEssentially, then, collocation is just a special case of the quantitative corpus linguistic research design adopted in this book: to ask whether two words form a collocation (or: are collocates of each other) is to ask whether one of these words occurs in a given position more frequently than expected by chance under the condition that the other word occurs in a structurally or sequentially related position.\nIn other words, we can decide whether two words a and b can be regarded as collocates on the basis of a contingency table like that in Table 7 Collocation\nThe combination silly ass is very rare in English, occurring just seven times in the 98 363 783 word BNC, but the expected frequencies in Table Generally speaking, the goal of a quantitative collocation analysis is to identify, for a given word, those other words that are characteristic for its context of usage.\nTables\nOften, we will be interested in the distribution of a word across two specific conditions -in the case of collocation, the distribution across the immediate contexts of two semantically related words.\nIt may be more insightful to compare adjectives occurring next to ass with those occurring next to the rough synonym donkey or the superordinate term animal.\nObviously, the fact that silly occurs 7.1 Collocates more frequently with ass than with donkey or animal is more interesting than the fact that silly occurs more frequently with ass than with stone or democracy.\nLikewise, the fact that silly occurs with ass more frequently than childish is more interesting than the fact that silly occurs with ass more frequently than precious or parliamentary.\nIn such cases, we can modify Table\nResearchers differ with respect to what types of co-occurrence they focus on when identifying collocations.\nSome treat co-occurrence as a purely sequential phenomenon defining collocates as words that co-occur more frequently than expected within a given span.\nSome researchers require a span of 1 (i.e., the words must occur directly next to each other), but many allow larger spans (five words being a relatively typical span size).\nOther researchers treat co-occurrence as a structural phenomenon, i.e., they define collocates as words that co-occur more frequently than expected in two related positions in a particular grammatical structure, for example, the adjective and noun positions in noun phrases of the form [Det Adj N] or the verb and noun position in transitive verb phrases of the form [V [ NP (Det) (Adj) N]].\nIt should have become clear that the designs in Tables 7.1 and 7.3 are essentially variants of the general research design introduced in previous chapters and used as the foundation of defining corpus linguistics: it has two variables, Collocates Position 1 and Position 2, both of which have two values, namely word x vs. other words (or, in the case of differential collocates, word x vs. word y).\nThe aim is to determine whether the value word a is more frequent for Position 1 under the condition that word b occurs in Position 2 than under the condition that other words (or a particular other word) occur in Position 2.\nMethodological issues in collocation research\nWe may occasionally be interested in an individual pair of collocates, such as silly ass, or in a small set of such pairs, such as all adjective-noun pairs with ass as the noun.\nHowever, it is much more likely that we will be interested in large sets of collocate pairs, such as all adjective-noun pairs or even all word pairs in a given corpus.\nThis has a number of methodological consequences concerning the practicability, the statistical evaluation and the epistemological status of collocation research.\na. Practicability.\nIn practical terms, the analysis of large numbers of potential collocations requires creating a large number of contingency tables and subjecting them to the 𝜒 2 test or some other appropriate statistical test.\nThis becomes implausibly time-consuming very quickly and thus needs to be automated in some way.\nThere are concordancing programs that offer some built-in statistical tests, but they typically restrict our options quite severely, both in terms of the tests they allow us to perform and in terms of the data on which the tests are performed.\nAnyone who decides to become involved in collocation research (or some of the large-scale lexical research areas described in the next chapter), should get acquainted at least with the simple options of automatizing statistical testing offered by spreadsheet applications.\nBetter yet, they should invest a few weeks (or, in the worst case, months) to learn a scripting language like Perl, Python or R (the latter being a combination of statistical software and programming environment that is ideal for almost any task that we are likely to come across as corpus linguists).\nb. Statistical evaluation.\nIn statistical terms, the analysis of large numbers of potential collocations requires us to keep in mind that we are now performing multiple significance tests on the same set of data.\nThis means that we must adjust our significance levels.\nThink back to the example of coin-flipping: the probability of getting a series of one head and nine tails is 0.009765.\nIf we flip a coin ten times and get this result, we could thus reject the null hypothesis with a probability of error of 0.010744, i.e., around 1 percent (because we would have to add the probability of getting ten tails, 0.000976).\nThis is well below the level required to claim statistical significance.\nHowever, if we perform one hundred series of ten coin-flips and one of these series consists of one head and nine tails (or ten tails), we could not reject the null hypothesis with the same confidence, as a probability of 0.010744 means that we would expect one such series to occur by chance.\nThis is not a problem as long as we do not accord this one result out of a hundred any special importance.\nHowever, if we were to identify a set of 100 collocations with 𝑝-values of 0.001 in a corpus, we are potentially treating all of them as important, even though it is very probable that at least one of them reached this level of significance by chance.\nTo avoid this, we have to correct our levels of significance when performing multiple tests on the same set of data.\nAs discussed in Section 6.6.1 above, the simplest way to do this is the Bonferroni correction, which consists in dividing the conventionally agreed-upon significance levels by the number of tests we are performing.\nAs noted in Section 6.6.1, this is an extremely conservative correction that might make it quite difficult for any given collocation to reach significance.\nOf course, the question is how important the role of 𝑝-values is in a design where our main aim is to identify collocates and order them in terms of their collocation strength.\nI will turn to this point presently, but before I do so, let us discuss the third of the three consequences of large-scale testing for collocation, the methodological one.\nc. Epistemological considerations.\nWe have, up to this point, presented a very narrow view of the scientific process based (in a general way) on the Popperian research cycle where we formulate a research hypothesis and then test it (either directly, by looking for counterexamples, or, more commonly, by attempting to reject the corresponding null hypothesis).\nThis is called the deductive method.\nHowever, as briefly discussed in Chapter 3, there is an alternative approach to scientific research that does not start with a hypothesis, but rather with general questions like \"Do relationships exist between the constructs in my data?\" and \"If so, what are those relationships?\".\nThe research then consists in applying statistical procedures to large amounts of data and examining the results for interesting patterns.\nAs electronic storage and computing power have become cheaper and more widely accessible, this approach -the exploratory or inductive approachhas become increasingly popular in all branches of science, particularly the social sciences.\nIt would be surprising if corpus linguistics was an exception, and indeed, it is not.\nEspecially the area of collocational research is typically exploratory.\nIn principle, there is nothing wrong with exploratory research -on the contrary, it would be unreasonable not to make use of the large amounts of language data and the vast computing power that has become available and accessible over 7.1 Collocates the last thirty years.\nIn fact, it is sometimes difficult to imagine a plausible hypothesis for collocational research projects.\nWhat hypothesis would we formulate before identifying all collocations in the LOB or some specialized corpus (e.g., a corpus of business correspondence, a corpus of flight-control communication or a corpus of learner language)?\nBut there is a danger, too: Most statistical procedures will produce some statistically significant result if we apply them to a large enough data set, and collocational methods certainly will.\nUnless we are interested exclusively in description, the crucial question is whether these results are meaningful.\nIf we start with a hypothesis, we are restricted in our interpretation of the data by the need to relate our data to this hypothesis.\nIf we do not start with a hypothesis, we can interpret our results without any restrictions, which, given the human propensity to see patterns everywhere, may lead to somewhat arbitrary post-hoc interpretations that could easily be changed, even reversed, if the results had been different and that therefore tell us very little about the phenomenon under investigation or language in general.\nThus, it is probably a good idea to formulate at least some general expectations before doing a large-scale collocation analysis.\nEven if we do start out with general expectations or even with a specific hypothesis, we will often discover additional facts about our phenomenon that go beyond what is relevant in the context of our original research question.\nFor example, checking in the BNC Firth's claim that the most frequent collocates of ass are silly, obstinate, stupid, awful and egregious and that young is \"much more frequent\" than old, we find that silly is indeed the most frequent adjectival collocate, but that obstinate, stupid and egregious do not occur at all, that awful occurs only once, and that young and old both occur twice.\nInstead, frequent adjectival collocates (ignoring second-placed wild, which exclusively refers to actual donkeys), are pompous and bad.\nPompous does not really fit with the semantics that Firth's adjectives suggest and could indicate that a semantic shift from 'stupidity' to 'self-importance' may have taken place between 1957 and 1991 (when the BNC was assembled).\nCollocation\nThis is, of course, a new hypothesis that can (and must) be investigated by comparing data from the 1950s and the 1990s.\nIt has some initial plausibility in that the adjectives blithering, hypocritical, monocled and opinionated also co-occur with ass in the BNC but are not mentioned by Firth.\nHowever, it is crucial to treat this as a hypothesis rather than a result.\nThe same goes for bad ass which suggests that the American sense of ass ('bottom') and/or the American adjective badass (which is often spelled as two separate words) may have begun to enter British English.\nIn order to be tested, these ideas -and any ideas derived from an exploratory data analysis -have to be turned into testable hypotheses and the constructs involved have to be operationalized.\nCrucially, they must be tested on a new data set -if we were to circularly test them on the same data that they were derived from, we would obviously find them confirmed.\nEffect sizes for collocations\nAs mentioned above, significance testing (while not without its uses) is not necessarily our primary concern when investigating collocations.\nInstead, researchers frequently need a way of assessing the strength of the association between two (or more) words, or, put differently, the effect size of their co-occurrence (recall from Chapter 6 that significance and effect size are not the same).\nA wide range of such association measures has been proposed and investigated.\nThey are typically calculated on the basis of (some or all) the information contained in contingency tables like those in Tables 7.1 and 7.3 above.\nLet us look at some of the most popular and/or most useful of these measures.\nI will represent the formulas with reference to the table in Table Collocates Now all we need is a good example to demonstrate the calculations.\nLet us use the adjective-noun sequence good example from the LOB corpus (but horse lovers need not fear, we will return to equine animals and their properties below).\nMeasures of collocation strength differ with respect to the data needed to calcuate them, their computational intensiveness and, crucially, the quality of their results.\nIn particular, many measures, notably the ones easy to calculate, have a problem with rare collocations, especially if the individual words of which they consist are also rare.\nAfter we have introduced the measures, we will therefore compare their performance with a particular focus on the way in which they deal (or fail to deal) with such rare events.\nChi-square\nThe first association measure is an old acquaintance: the chi-square statistic, which we used extensively in Chapter 6 and in Section 7.1.1 above.\nI will not demonstrate it again, but the chi-square value for Table\nRecall that the chi-square test statistic is not an effect size, but that it needs to be divided by the table total to turn it into one.\nAs long as we are deriving all our collocation data from the same corpus, this will not make a difference, since the table total will always be the same.\nHowever, this is not always the case.\nWhere table sizes differ, we might consider using the phi value instead.\nI am not aware of any research using phi as an association measure, and in fact the chi-square statistic itself is not used widely either.\nThis is because it has a serious problem: recall that it cannot be applied if more than 20 percent of the 7 Collocation cells of the contingency table contain expected frequencies smaller than 5 (in the case of collocates, this means not even one out of the four cells of the 2-by-2 table).\nOne reason for this is that it dramatically overestimates the effect size and significance of such events, and of rare events in general.\nSince collocations are often relatively rare events, this makes the chi-square statistic a bad choice as an association measure.\nMutual Information Mutual information is one of the oldest collocation measures, frequently used in computational linguistics and often implemented in collocation software.\nIt is given in (2) MI = log 2 ( 𝑂 11 𝐸 11 × 𝑆 ) 5\nA logarithm with a base 𝑏 of a given number 𝑥 is the power to which b must be raised to produce 𝑥, so, for example, log 10 (2) = 0.30103, because 10 0.30103 = 2.\nMost calculators offer at the very least a choice between the natural logarithm, where the base is the number 𝑒 (approx. 2.7183) and the common logarithm, where the base is the number 10; many calculators and all major spreadsheet programs offer logarithms with any base.\nIn the formula in (1), we need the logarithm with base 2; if this is not available, we can use the natural logarithm and divide the result by the natural logarithm of 2: Collocates\nThe mutual information measure suffers from the same problem as the 𝜒 2 statistic: it overestimates the importance of rare events.\nSince it is still fairly widespread in collocational research, we may nevertheless need it in situations where we want to compare our own data to the results of published studies.\nHowever, note that there are versions of the MI measure that will give different results, so we need to make sure we are using the same version as the study we are comparing our results to.\nBut unless there is a pressing reason, we should not use mutual information at all.\nThe log-likelihood ratio test\nThe 𝐺 value of the log-likelihood ratio test is one of the most popular -perhaps the most popular -association measure in collocational research, found in many of the central studies in the field and often implemented in collocation software.\nThe following is a frequently found form 𝐸 𝑖 )\nIn order to calculate the 𝐺 measure, we calculate for each cell the natural logarithm of the observed frequency divided by the expected frequency and multiply it by the observed frequency.\nWe then add up the results for all four cells and multiply the result by two.\nNote that if the observed frequency of a given cell is zero, the expression 𝑂 𝑖/𝐸 𝑖 will, of course, also be zero.\nSince the logarithm of zero is undefined, this would result in an error in the calculation.\nThus, log(0) is simply defined as zero when applying the formula in Applying the formula in Collocation Minimum Sensitivity Minimum sensitivity was proposed by\nWe simply divide the observed frequency of a collocation by the frequency of the first word (R 1 ) and of the second word (C 1 ) and use the smaller of the two as the association measure.\nFor the data in Table 236 ) = min (0.0108, 0.0381) = 0.0108 In addition to being extremely simple to calculate, it has the advantage of ranging from zero (words never occur together) to 1 (words always occur together); it was also argued by Fisher's exact test Fisher's exact test was already mentioned in passing in Chapter 6 as an alternative to the 𝜒 2 test that calculates the probability of error directly by adding up the probability of the observed distribution and all distributions that deviate from the null hypothesis further in the same direction.\nThere are some practical disadvantages to Fisher's exact test.\nFirst, it is computationally expensive -it cannot be calculated manually, except for very small tables, because it involves computing factorials, which become very large very quickly.\nFor completeness' sake, here is (one version of) the formula:\n(5) Obviously, it is not feasible to apply this formula directly to the data in Table 7.6, because we cannot realistically calculate the factorials for 236 or 836, let 7.1 Collocates alone 1 011 904.\nBut if we could, we would find that the 𝑝-value for Table Spreadsheet applications do not usually offer Fisher's exact test, but all major statistics applications do.\nHowever, typically, the exact 𝑝-value is not reported beyond the limit of a certain number of decimal places.\nThis means that there is often no way of ranking the most strongly associated collocates, because their 𝑝values are smaller than this limit.\nFor example, there are more than 100 collocates in the LOB corpus with a Fisher's exact 𝑝-value that is smaller than the smallest value that a standard-issue computer chip is capable of calculating, and more than 5000 collocates that have 𝑝-values that are smaller than what the standard implementation of Fisher's exact test in the statistical software package R will deliver.\nSince in research on collocations we often need to rank collocations in terms of their strength, this may become a problem.\nA comparison of association measures\nLet us see how the association measures compare using a data set of 20 potential collocations.\nInspired by Firth's silly ass, they are all combinations of adjectives with equine animals.\nTable\nAll combinations are perfectly normal, grammatical adjective-noun pairs, meaningful not only in the specific context of their actual occurrence.\nHowever, I have selected them in such a way that they differ with respect to their status as potential collocations (in the sense of typical combinations of words).\nSome are compounds or compound like combinations (rocking horse, Trojan horse, and, in specialist discourse, common zebra).\nSome are the kind of semi-idiomatic combinations that Firth had in mind (silly ass, pompous ass).\nSome are very conventional combinations of nouns with an adjective denoting a property specific to that noun (prancing horse, braying donkey, galloping horse -the first of these being a conventional way of referring to the Ferrari brand mark logo).\nSome only give the appearance of semi-idiomatic combinations (jumped-up jackass, actually an unconventional variant of jumped-up jack-in-office; dumb-fuck donkey, actually an extremely rare phrase that occurs only once in the documented history of English, namely in the book Trail of the Octopus: From Beirut to Lockerbie -Inside the DIA and that probably sounds like an idiom because of the alliteration and the semantic relationship to silly ass; and monocled ass, which brings to mind pompous ass but is actually not a very conventional combination).\nFinally, there are a number of fully compositional combinations that make sense but do not have any special status (caparisoned mule, new horse, old donkey, young zebra, large mule, female hinny, extinct quagga).\nIn addition, I have selected them to represent different types of frequency relations: some of them are (relatively) frequent, some of them very rare, for some of them the either the adjective or the noun is generally quite frequent, and for some of them neither of the two is frequent.\nTable 7.1 Collocates Collocation\nAll association measures fare quite well, generally speaking, with respect to the compositional expressions -these tend to occur in the lower third of all lists.\nWhere there are exceptions, the 𝜒 2 statistic, mutual information and minimum sensitivity rank rare cases higher than they should (e.g. caparisoned mule, extinct quagga), while the G and the 𝑝-value of Fisher's exact test rank frequent cases higher (e.g.galloping horse).\nWith respect to the non-compositional cases, 𝜒 2 and mutual information are quite bad, overestimating rare combinations like jumped-up jackass, dumb-fuck donkey and monocled ass, while listing some of the clear cases of collocations much further down the list (silly ass, and, in the case of MI, rocking horse).\nMinimum sensitivity is much better, ranking most of the conventionalized cases in the top half of the list and the non-conventionalized ones further down (with the exception of jumped-up jackass, where both the individual words and their combination are very rare).\nThe G and the Fisher 𝑝-value fare best (with no differences in their ranking of the expressions), listing the conventionalized cases at the top and the distinctive but non-conventionalized cases in the middle.\nTo demonstrate the problems that very rare events can cause (especially those where both the combination and each of the two words in isolation are very rare), imagine someone had used the phrase tomfool onager once in the BNC.\nSince neither the adjective tomfool (a synonym of silly) nor the noun onager (the name of the donkey sub-genus Equus hemionus, also known as Asiatic or Asian wild ass) occur in the BNC anywhere else, this would give us the distribution in Table\nAlthough the example is hypothetical, the problem is not.\nIt uncovers a mathematical weakness of many commonly used association measures.\nFrom an empirical perspective, this would not necessarily be a problem, if cases like that in Table To sum up, when doing collocational research, we should use the best association measures available.\nFor the time being, this is the p value of Fisher's exact test (if we have the means to calculate it), or G (if we don't, or if we prefer using a widely-accepted association measure).\nWe will use G through much of the remainder of this book whenever dealing with collocations or collocation-like phenomena.\nCase studies\nIn the following, we will look at some typical examples of collocation research, i.e. cases where both variables consist of (some part of) the lexicon and the values are individual words.\nCollocation for its own sake\nResearch that is concerned exclusively with the collocates of individual words or the extraction of all collocations from a corpus falls into three broad types.\nFirst, there is a large body of research on the explorative extraction of collocations from corpora.\nThis research is not usually interested in any particular collocation (or set of collocations), or in genuinely linguistic research questions; instead, the focus is on methods (ways of preprocessing corpora, which association measures to use, etc.).\nSecond, there is an equally large body of applied research that results 7 Collocation in lexical resources (dictionaries, teaching materials, etc.) rather than scientific studies on specific research questions.\nThird, there is a much smaller body of research that simply investigates the collocates of individual words or small sets of words.\nThe perspective of these studies tends to be descriptive, often with the aim of showing the usefulness of collocation research for some application area.\nThe (relative) absence of theoretically more ambitious studies of the collocates of individual words may partly be due to the fact that words tend to be too idiosyncratic in their behavior to make their study theoretically attractive.\nHowever, this idiosyncrasy itself is, of course, theoretically interesting and so such studies hold an unrealized potential at least for areas like lexical semantics.\nCase study: Degree adverbs\nA typical example of a thorough descriptive study of the collocates of individual words is Thus, as is typical for this kind of study, Kennedy adopts an exploratory perspective.\nThe study involves two nominal variables: first, Degree Adverb, with 24 values corresponding to the 24 specific adverbs he selects; second, Adjective, with as many different potential values as there are different adjectives in the BNC (in exploratory studies, it is often the case that we do not know the values of at least one of the two variables in advance, but have to extract them from the data).\nAs pointed out above, which of the two variables is the dependent one and which the independent one in studies like this depends on your research question: if you are interested in degree adverbs and want to explore which adjectives they co-occur with, it makes sense to treat Degree Adverb as the independent and Adjective as the dependent variable; if you are interested in adjectives and want to expore which degree adverbs they co-occur with, it makes sense to do it the other way around.\nStatistically, it does not make a difference, since our statistical tests for nominal data do not distinguish between dependent and independent variables.\nKennedy finds, first, that there are some degree adverbs that do not appear to have restrictions concerning the adjectives they occur with (for example, very, re-7.2 Case studies ally and particularly).\nHowever, most degree adverbs are clearly associated with semantically restricted sets of adjectives.\nThe restrictions are of three broad types.\nFirst, there are connotational restrictions (some adverbs are associated primarily with positive words (e.g. perfectly) or negative words (e.g. utterly, totally; on connotation cf. also\nUnlike Kennedy, I have used the G statistic of the log-likelihood ratio test,\nThis case study illustrates the exploratory design typical of collocational research as well as the kind of result that such studies yield and the observations possible on the basis of these results.\nBy comparing the results reported here to Kennedy's, you may also gain a better understanding as to how different association measures may lead to different results.\nLexical relations\nOne area of lexical semantics where collocation data is used quite intensively is the study of lexical relations -most notably, (near) synonymy\nCase study: Near synonyms\nNatural languages typically contain pairs (or larger sets) of words with very similar meanings, such as big and large, begin and start or high and tall.\nIn isolation, 7 Collocation tell what the difference in meaning is, especially since they are often interchangeable at least in some contexts.\nObviously, the distribution of such pairs or sets with respect to other words in a corpus can provide insights into their similarities and differences.\nOne example of such a study is The study, while not strictly hypothesis-testing, is thus somewhat deductive.\nIt involves two nominal variables; the independent variable Type of Entity with eleven values shown in Table As we can see, there is little we can learn from this table, since the frequencies in the individual cells are simply too small to apply the 𝜒 2 test to the table as a whole.\nThe only 𝜒 2 components that reach significance individually are those for the category human, which show that tall is preferred and high avoided with human referents.\nThe sparsity of the data in the table is due to the fact that the analyzed sample is very small, and this problem is exacerbated by the fact that the little data available is spread across too many categories.\nThe category labels are not well chosen either\n: they overlap substantially in several places (e.g., towers and walls are buildings, pieces of clothing are artifacts, etc.) and not all of them seem relevant to any expectation we might have about the words high and tall.\nTaylor later cites earlier psycholinguistic research indicating that tall is used when the vertical dimension is prominent, is an acquired property and is a property of an individuated entity.\nIt would thus have been better to categorize the corpus data according to these properties -in other words, a more strictly deductive approach would have been more promising given the small data set.\nAlternatively, we can take a truly exploratory approach and look for differential collocates as described in Section 7.1.1 above -in this case, for differential noun collocates of the adjectives high and tall.\nThis allows us to base our analysis on a much larger data set, as the nouns do not have to be categorized in advance.\nTable\nThis case study shows how the same question can be approached by a deductive or an inductive (exploratory) approach.\nThe deductive approach can be more precise, but this depends on the appropriateness of the categories chosen a priori for annotating the data; it is also time consuming and therefore limited to relatively small data sets.\nIn contrast, the inductive approach can be applied to a large data set because it requires no a priori annotation.\nIt also does not require any choices concerning annotation categories; however, there may be a danger to project patterns into the data post hoc.\nCase study: Antonymy\nAt first glance, we expect the relationship between antonyms to be a paradigmatic one, where only one or the other will occur in a given utterance.\nHowever, There are differences in detail in these studies, but broadly speaking, they take a deductive approach: they choose a set of test words for which there is agreement as to what their antonyms are, search for these words in a corpus, and check whether their antonyms occur in the same sentence significantly more frequently than expected.\nThe studies thus involve two nominal variables: Sentence (with the values contains test word and does not contain test word) and Antonym of Test Word (with the values occurs in sentence and does not occur in sentence).\nThis seems like an unnecessarily complicated way of representing the kind of co-occurrence design used in the examples above, but I have chosen it to show that in this case sentences containing a particular word are used as the condition under which the occurrence of another word is 7.2\nCase studies investigated -a straightforward application of the general research design that defines quantitative corpus linguistics.\nTable\nThese studies only look at the co-occurrence of antonyms; they do not apply the same method to word pairs related by other lexical relations (synonymy, taxonomy, etc.).\nThus, there is no way of telling whether co-occurrence within the same sentence is something that is typical specifically of antonyms, or whether it is something that characterizes word pairs in other lexical relations, too.\nAn obvious approach to testing this would be to repeat the study with other types of lexical relations.\nAlternatively, we can take an exploratory approach that does not start out from specific word pairs at all.\nTable\nClearly, antonymy is the dominant relation among these word pairs, which are mostly opposites (black/white, male/female, public/private, etc.), and sometimes relational antonyms (primary/secondary, economic/social, economic/political, social/political, lesbian/gay, etc.).\nThe only cases of non-antonymic pairs are economic/monetary, which is more like a synonym than an antonym and the fixed expressions deaf /dumb and hon(ourable)/learned (as in honourable and learned gentleman/member/friend).\nThe pattern does not just hold for the top 30 collocates but continues as we go down the list.\nThere are additional cases of relational antonyms, like British/American and Czech/Slovak and additional examples of fixed expressions (alive and well, far and wide, true and fair, null and void, noble and learned), but most cases are clear antonyms (for example, syntactic/ semantic, spoken/written, mental/physical, right/left, rich/poor, young/old, good/ evil, etc.).\nThe one systematic exceptions are cases like worse and worse (a special construction with comparatives indicating incremental change, cf. Stefanowitsch 2007b).\nThis case study shows how deductive and inductive domains may complement each other: while the deductive studies cited show that antonyms tend to cooccur syntagmatically, the inductive study presented here shows that words that co-occur syntagmatically (at least in certain syntactic contexts) tend to be antonyms.\nThese two findings are not equivalent; the second finding shows that the first finding may indeed be typical for antonymy as opposed to other lexical relations.\nCase studies\nThe exploratory study was limited to a particular syntactic/semantic context, chosen because it seems semantically and pragmatically neutral enough to allow all kinds of lexical relations to occur in it.\nThere are contexts which might be expected to be particularly suitable to particular kinds of lexical relations and which could be used, given a large enough corpus, to identify word pairs in such relations.\nFor example, the pattern [ADJ rather than ADJ] seems semantically predisposed for identifying antonyms, and indeed, it yields pairs like implicit/explicit, worse/better, negative/positive, qualitative/quantitative, active/passive, real/apparent, local/national, political/economical, etc.\nOther patterns are semantically more complex, identifying pairs in more context-dependent oppositions; for example, [ADJ but not ADJ] identifies pairs like desirable/essential, necessary/sufficient, similar/identical, small/insignificant, useful/essential, difficult/impossible.\nThe relation between the adjectives in these pairs is best described as pragmatic -the first one conventionally implies the second.\nSemantic prosody\nSometimes, the collocates of a node word (or larger expressions) fall into a more or less clearly recognizable semantic class that is difficult to characterize in terms of denotational properties of the node word.\nThis definition has been understood by collocation researchers in two different (but related) ways.\nMuch of the subsequent research on semantic prosody is based on the understanding that this \"aura\" consists of connotational meaning (cf. e.g. Case study: True feelings\nA typical example of Sinclair's approach to semantic prosody, both methodologically and theoretically, is his short case study of the expression true feelings.\nSecond, the expression collocates with verbs of expression (perhaps unsurprising for an expression relating to emotions); this, too, is true for our sample, where such verbs are found in 14 lines: reflect (line 2), show (lines 3, 9, 14, and 19), read (line 5), declare (line 6), disguise (line 7), reveal (line 8), hide (line 10), reveal (line 12), admit (line 13), give vent to (line 15), and tell (line 18).\nThird, and most interesting, Sinclair finds that a majority of his examples express a reluctance to express emotions.\nIn our sample, such cases are also noticeably frequent: I would argue that lines 2, Sinclair assumes that the denotational meaning of the phrase true feelings is \"genuine emotions\".\nBased on his observations, he posits that, in addition, it has the semantic prosody \"reluctance/inability to express emotions\" -an attitudinal meaning much more specific than a general \"positive\" or \"negative\" connotation.\nThe methodological approach taken by Sinclair (and many others in his tradition) can yield interesting observations (at least, if applied very carefully): descriptively, there is little to criticize.\nHowever, under the definition of corpus linguistics adopted in this book, Sinclair's observations would be just the first step towards a full analysis.\nFirst, note that Sinclair's approach is quantitative only in a very informal sense -he rarely reports exact frequencies for a given semantic feature in his sample, relying instead on general statements about the frequency or rarity of particular phenomena.\nAs we saw above, this is easy to remedy by simply determining the exact number of times that the phenomenon in question occurs in a given sample.\nHowever, such exact frequencies do not advance the analysis meaningfully: as long as we do not know how frequent a particular phenomenon is in the corpus as a whole, we cannot determine whether it is a characteristic property of the expression under investigation, or just an accidental one.\nSpecifically, as long as we do not know how frequent the semantic prosody 'reluctance or inability to express' is in general, we do not know whether it is particularly characteristic of the phrase true feelings.\nIt may be characteristic, among other things, (a) of utterances concerning emotions in general, (b) of utterances containing the plural noun feelings, (c) of utterances containing the adjective true, etc.\nCase studies\nIn order to determine this, we have to compare our sample of the expression true feelings to related expressions that differ with respect to each property potentially responsible for the semantic prosody.\nFor example, we might compare it to the noun feelings in order to investigate possibility (b).\nFigure\nThe semantic prosody is not characteristic of the noun feelings, even in possessive contexts.\nWe can thus assume that it is not characteristic of utterances concerned with emotions generally.\nBut is it characteristic of the specific expression true feelings, or would we find it in other contexts where a distinction between genuine and non-genuine emotions is made?\nIn order to answer this question, we have to compare the phrase to denotationally synonymous expressions, such as genuine emotions (which Sinclair uses to paraphrase the denotational meaning), genuine feelings, real emotions and real feelings.\nThe only one of these expressions that occurs in the BNC more than a handful of times is real feelings.\nA sample concordance is shown in Figure 1 r-head wolf-whistles .\nReal situations , [real feelings] , real people , real love .\nThe album s 2 onal Checklist :\nI do my best to hide my [real feelings] from others I always try to please othe 3 , how to manipulate , how to hide their [real feelings] and how to convince those that love the 4 f the death of a cousin\n.\nDisguising his [real feelins] he wrote cheerfully , telling them that 5 her words , the counsellor must seek the [real feelings] of the counsellee through careful liste 6 tant issues are fully discussed and that [real feelings] are expressed rather than avoided .\nAn 7 at prevented him from ever revealing his [real feelings] to any woman .\nHow she regretted those 8 ing process of mystification that denies [real feelings] and experiences is a necessary prop to 9\nthe play to whom he reveals some of his [real feelings] is Roderigo , but only while using him 10 sked her much sooner if he had known her [real feelings] towards him , but she had been so forma 11 of situation neither can say what their [real feelings] are .\nA true conversation might be , 12 clerks are not allowed to express their [real feelings] at work , it is not surprising that the 13 k foolish in public in order to hide his [real feelings] .\nMen were strange creatures at times .\nHere, the semantic prosody in question is quite dominant -by my count, it is present in lines 2,\nIt seems, then, that the semantic prosody Sinclair observes is not attached to the expression true feelings in particular, but that it is an epiphenomenon the fact that we typically distinguish between \"genuine\" (true, real, etc.) emotions and other emotions in a particular context, namely one where someone is reluctant on unable to express their genuine emotions.\nOf course, studies of additional expressions with adjectives meaning \"genuine\" modifying nouns meaning \"emotion\" might give us a more detailed and differentiated picture, as might studies of other nouns modified by adjectives like true (such as true nature, true beliefs, true intentions, etc.).\nSuch studies are left as an exercise to the reader -this case study was mainly meant to demonstrate how informal analyses based on the inspection of concordances can be integrated into a more rigorous research design involving quantification and comparison to a set of control data.\nCase study: The verb cause\nA second way in which semantic prosody can be studied quantitatively is implicit in Kennedy's study of collocates of degree adverbs discussed in Section 7.2.1 above.\nRecall that Kennedy discusses for each degree adverb whether a majority of its collocates has a positive or a negative connotation.\nThis, of course, is a statement about the (broad) semantic prosody of the respective adverb, based not on an inspection and categorization of usage contexts, but on inductively discovered strongly associated collocates.\nOne of the earliest applications of this procedure is found in Stubbs then notes that manual inspection and extraction becomes unfeasible as the number of corpus hits grows\nand\nsuggests that, instead, we should first identify significant collocates of the word or expression we are interested in, and then categorize these significant collocates according to our criteria\n-note that 7 Collocation this is the strategy we also used in Case Study 7.2.2.1 above in order to determine semantic differences between high and tall.\nWe will not follow Stubbs' discussion in detail here -his focus is on methodological issues regarding the best way to identify collocates.\nSince we decided in Section 7.1.3 above to stick with the G statistic, this discussion is not central for us.\nStubbs does not present the results of his procedure in detail and the corpus he uses is not accessible anyway, so let us use the BNC again and extract our own data.\nTable\nThese collocates clearly corroborate Stubbs' observation about the negative semantic prosody of cause.\nWe could now calculate the association strength between the verb and each of these nouns to get a better idea of which of them are significant collocates and which just happen to be frequent in the corpus overall.\nIt should be obvious, however, that the nouns in Table But even so, what does this tell us about the semantic prosody of the verb cause?\nIt has variously been pointed out (for example, by In order to determine this, it might be useful to compare different expressions of causation to each other rather than to the corpus as a whole -to perform a differentiating collocate analysis: just by inspecting the frequencies in Table Case studies\nWe can thus conclude, first, that all three verbal expressions of causation are likely to be used to some extent with direct object nouns with a negative connotation.\nHowever, it is only the verb cause that has a negative semantic prosody.\nEven the raw frequencies of nouns occurring in the object position of the three expressions suggest this: while cause occurs almost exclusively with negatively 7 Collocation to be negative -it can be interesting if it is the right problem and you are in the right mood (e. g. [O]ne of these excercises caused an interesting problem for several members of the class [Aiden Thompson, Who's afraid of the Old Testament God?]).\nEven damage can be a good thing in particular contexts from particular perspectives (e.g. [A] high yield of intact PTX\nKeeping this caveat in mind, however, the method presented in this case study can be applied fruitfully in more complex designs than the one presented here.\nFor example, we have treated the direct object position as a simple category here, but Cultural analysis\nIn collocation research, a word (or other element of linguistic structure) typically stands for itself -the aim of the researcher is to uncover the linguistic properties of a word (or set of words).\nHowever, texts are not just manifestations of a language system, but also of the cultural conditions under which they were produced.\nThis allows corpus linguistic methods to be used in uncovering at least some properties of that culture.\nSpecifically, we can take lexical items to represent culturally defined concepts and investigate their distribution in linguistic corpora in order to uncover these cultural definitions.\nOf course, this adds complexity to the question of operationalization: we must ensure that the words we choose are indeed valid representatives of the cultural concept in question.\nCase studies Case study: Small boys, little girls\nObviously, lexical items used conventionally to refer to some culturally relevant group of people are plausible representatives of the cultural concept of that group.\nFor example, some very general lexical items referring to people (or higher animals) exist in male and female versions -man/woman, boy/girl, lad/lass, husband/wife, father/mother, king/queen, etc.\nIf such word pairs differ in their collocates, this could tell us something about the cultural concepts behind them.\nFor example, Stubbs argues that this difference is due to different connotations of small and little which he investigates on the basis of the noun collocates to their right and the adjectival and adverbial collocates to the left.\nAgain, instead of Stubbs' original data (which he identifies on the basis of raw frequency of occurrence and only cites selectively), I use data from the BNC and the G test statistic.\nTable\nThe connotational difference between the two adjectives becomes clear when we look at the adjectives they combine with.\nThe word little has strong associations to evaluative adjectives that may be positive or negative, and that are often patronizing.\nSmall, in contrast, does not collocate with evaluative adjectives.\nStubbs sums up his analysis by pointing out that small is a neutral word for describing size, while little is sometimes used neutrally, but is more often \"nonliteral and convey[s] connotative and attitudinal meanings, which are often patronizing, critical, or both.\n\"\nStubbs remains unspecific as to what that ideology is -presumably, one that treats boys as neutral human beings and girls as targets for patronizing evaluation.\nIn order to be more specific, it would be necessary to turn around the perspective and study all adjectival collocates of boy and girl.\nStubbs does not do this, but Caldas-Coulthard & Moon (2010) look at adjectives collocating with man, woman, boy and girl in broadsheet and yellow-press newspapers.\nIn order to keep the results comparable with those reported above, let us stick with the BNC instead.\nTable\nThe results are broadly similar in kind to those in Caldas-Coulthard & Moon (2010): boy collocates mainly with neutral descriptive terms (small, lost, big, new), or with terms with which it forms a fixed expression (old, dear, toy, whipping).\nThere are the evaluative adjectives rude (which in Caldas-Coulthard and Moon's data is often applied to young men of Jamaican descent) and its positively connoted equivalent naughty.\nThe collocates of girl are overwhelmingly evaluative, related to physical appearance.\nThere are just two neutral adjective (other and dead, the latter tying in with a general observation that women are more often spoken of as victims of crimes and other activities than men).\nFinally, there is one adjective signaling marital status.\nThese results also generally reflect Caldas-Coulthard and Moon's findings (in the yellow-press, the evaluations are often heavily sexualized in addition).\nThis case study shows how collocation research may uncover facts that go well beyond lexical semantics or semantic prosody.\nIn this case, the collocates of boy and girl have uncovered a general attitude that sees the latter as up for constant evaluation while the former are mainly seen as a neutral default.\nThat the adjectives dead and unmarried are among the top ten collocates in a representative, relatively balanced corpus, hints at something darker -a patriarchal world view that sees girls as victims and sexual partners and not much else (other studies investigating gender stereotypes on the basis of collocates of man and woman are Case studies Grammar\nThe fact that corpora are most easily accessed via words (or word forms) is also reflected in many corpus studies focusing on various aspects of grammatical structure.\nMany such studies either take (sets of) words as a starting point for studying various aspects of grammatical structure, or they take easily identifiable aspects of grammatical structure as a starting point for studying the distribution of words.\nHowever, as the case studies of the English possessive constructions in Chapters 5 and 6 showed, grammatical structures can be (and are) also studied in their own right, for example with respect to semantic, information-structural and other restrictions they place on particular slots or sequences of slots, or with their distribution across texts, language varieties, demographic groups or varieties.\nGrammar in corpora\nThere are two major problems to be solved when searching corpora for grammatical structures.\nWe discussed both of them to some extent in Chapter 4, but let us briefly recapitulate and elaborate some aspects of the discussion before turning to the case studies.\nFirst, we must operationally define the structure itself in such a way that we (and other researchers) can reliably categorize potential instances as manifesting the structure or not.\nThis may be relatively straightforward in the case of simple grammatical structures that can be characterized based on tangible and stable characteristics, such as particular configurations of grammatical morphemes and/or categories occurring in sequences that reflect hierarchical relations relatively directly.\nIt becomes difficult, if not impossible, with complex structures, especially in frameworks that characterize such structures with recourse to abstract, non-tangible and theory-dependent constructs (see Second, we must define a query that will allow us to retrieve potential candidates from our corpus in the first place (a problem we discussed in some detail in Chapter 4).\nAgain, this is simpler in the case of morphologically marked and relatively simple grammatical structures, for example, the s-possessive (as defined above) is typically characterized by the sequence ⟨ [pos=\"noun\"] [word=\"'s\"%c] [pos=\"adjective\"]* [pos=\"noun\"] ⟩ in corpora containing texts in standard orthography; it can thus be retrieved from a POS-tagged corpus with a fairly high degree of precision and recall.\nHowever, even this simple case is more complex than it seems.\nThe query will produce false hits: in the sequence just given, 's may also stand for the verb be (Sam's head of marketing).\nThe query will also produce false misses: the modified nominal may not always be directly adjacent to the 's (for example in This office is Sam's or in Sam's friends and family), and the s-possessive may be represented by an apostrophe alone (for example in his friends' families).\nOther structures may be difficult to retrieve even though they can be characterized straightforwardly: most linguists would agree, for example, that transitive verbs, are verbs that take a direct object.\nHowever, this is of very little help in retrieving transitive verbs even from a POS-tagged corpus, since many noun-phrases following a verb will not be direct objects (Sam slept the whole day) and direct objects do not necessarily follow their verb (Sam, I have not seen); in addition, noun phrases themselves are not trivial to retrieve.\nYet other structures may be easy to retrieve, but not without retrieving many false hits at the same time.\nThis is the case with ambiguous structures like the ofpossessive, which can be retrieved by a query along the lines of ⟨ [pos=\"noun\"] [pos=\"determiner\"]? [pos=\"adjective\"]* [pos=\"noun\"] ⟩, which will also retrieve, among other things, partitive and quantitative uses of the of -construction.\nFinally, structures characterized with reference to invisible theoretical constructs (\"traces\", \"zero morphemes\", etc.) are so difficult to retrieve that this, in itself, may be a good reason to avoid such invisible constructs whenever possible when characterizing linguistic phenomena that we plan to investigate empirically.\nThese difficulties do not keep corpus linguists from investigating grammatical structures, including very abstract ones, even though this typically means retrieving the relevant data by mind-numbing and time-consuming manual analysis of the results of very broad searches or even of the corpus itself, if necessary.\nBut they are probably one reason why so much grammatical research in corpus linguistics takes a word-centered approach.\nA second reason for a word-centered approach is that it allows us to transfer well-established collocational methods to the study of grammar.\nIn the preceding chapter we saw that while collocation research often takes a sequential approach to co-occurrence, where any word within a given span around a node word is counted as a potential collocate, it is not uncommon to see a structure-sensitive approach that considers only those potential collocates that occur in a particular grammatical position relative to each other -for example, adjectives relative to the nouns they modify or vice versa.\nIn this approach, grammatical structure is already present in the design, even though it remains in the background.\nWe can move these types of grammatical structure into the focus of our investigation, giving us a range of research designs where one variable consists of (part of) the lexicon (with values that are individual words) and one variable consists of some aspect of grammatical structure.\nIn these studies, the retrieval becomes somewhat less of a problem, as we can search for lexical items and identify the grammatical structures in our search results afterwards, though identifying these structures reliably remains non-trivial.\nWe will begin with word-centered case studies and then move towards more genuinely grammatical research designs.\nCase studies Collocational frameworks and grammar patterns\nAn early extension of collocation research to the association between words and grammatical structure is The idea behind collocational frameworks was subsequently extended by\nThe patterns of a word can be defined as all the words and structures which are regularly associated with the word and which contribute to its meaning.\nA pattern can be identified if a combination of words occurs relatively 8 Grammar frequently, if it is dependent on a particular word choice, and if there is a clear meaning associated with it Collocational frameworks and especially grammar patterns have an immediate applied relevance: the COBUILD dictionaries included the most frequently found patterns for each word in their entries from 1995 onward, and there is a two-volume descriptive grammar of the patterns of verbs Case study: [a __ of ] As an example of a collocational framework, consider [a __ of ], one of the patterns that Table Renouf and Sinclair first present the twenty items occurring most frequently in the collocational framework, shown in the columns labeled (a).\nThese are, roughly speaking, the words most typical for the collocational framework: when we encounter the framework (in a corpus or in real life), these are the words that are most probable to fill the slot between a and of.\nRenouf and Sinclair then point out that the frequency of these items in the collocational framework does not correspond to their frequency in the corpus as a whole, where, for example, man is the most frequent of their twenty words, and lot is only the ninth-most frequent.\nThe \"promotion of lot to the top of the list\" in the framework [a __ of ], 8.2 Case studies they argue, shows that it is its \"tightest collocation\".\nAs discussed in Chapter 7, association measures are the best way to asses the difference in frequency of an item under a specific condition (here, the presence of the collocational framework) from its general frequency and I will present the strongest collocates as determined by the G statistic below.\nRenouf and Sinclair choose a different strategy: for each item, they calculate the percentage of all occurrences of that item within the collocational framework.\nThe results are shown in the columns labeled (b) (for example, number occurrs in the BNC a total of 48 806 times, so the 13 799 times that it occurs in the pattern [a __ of ] account for 28.21 percent of its occurrences).\nAs you can see, the order changes slightly, but the basic result appears to remain the same.\nBroadly speaking, the most strongly associated words in both corpora and by either measure tend to be related to quantities (e.g. lot, number, couple), part-whole relations (e.g. piece, member, group, part), or types (e.g. sort or variety).\nThis kind of semantic coherence is presented by Renouf and Sinclair as evidence that collocational frameworks are relevant units of language.\n8 Grammar\nKeeping this in mind, let us discuss the difference between frequency and percentages in some more detail.\nNote, first, that the reason the results do not change perceptibly is because Renouf and Sinclair do not determine the percentage of occurrences of words inside [a __ of ] for all words in the framework, but only for the twenty nouns that they have already identified as most frequent -the columns labeled (b) thus represent a ranking based on a mixed strategy of preselecting words by their raw frequency and then ranking them by their proportions inside and outside the framework.\nIf we were to omit the pre-selection stage and calculate the percentages for all words occurring in the framework -as we should, if these percentages are relevant -we would find 477 words in the BNC that occur exclusively in the framework, and thus all have an association strength of 100 percent -among them words that fit the proposed semantic preferences of the pattern, like barrelful, words that do not fit, like bomb-burst, hyserisation or Jesuitism, and many misspellings, like fct (for fact) and numbe and numbr (for number).\nThe problem here is that percentages, like some other association measures, massively overestimate the importance of rare events.\nIn order to increase the quality of the results, let us remove all words that occur five times or less in the BNC.\nThe twenty words in Table There are words that are typical for a particular framework, and there are frameworks that are typical for particular words; this difference in perspective may be of interest in particular research designs (cf. Grammar Case study: [there V link something ADJ about NP] As an example of a grammar pattern, consider [there V link something ADJ about NP] Hunston and Francis mention this pattern only in passing, noting that it \"has the function of evaluating the person or thing indicated in the noun group following about\".\nTheir usual procedure (which they skip in this case) is to back up such observations concerning the meaning of grammar patterns by listing the most frequent items occurring in this pattern and showing that these form one or more semantic classes (similar to the procedure used by (1) odd (21), special (20), different (18), familiar (16), strange (13), sinister\nThe list clearly supports Hunston and Francis's claim about the meaning of this grammar pattern -most of the adjectives are inherently evaluative.\nThere are a few exceptions -different, special, unusual and unique do not have to be used evaluatively.\nIf they occur in the pattern [there V link something ADJ about NP], however, they are likely to be interpreted evaluatively.\nAs The following examples demonstrate this: (2) a. There is something horribly cyclical about television advertising.\n(BNC CBC) b.\nIt is a big, square box, painted dirty white, and, although he is always knocking through, extending, repapering and spring-cleaning, there is something dead about the place.\n(BNC HR9) c.\nAt least it didn't sound now quite like a typewriter, but there was something metallic about it.\n(BNC J54)\nCase studies\nThe adjective cyclical is neutral, but the adverb horribly shows that it is meant evaluatively in (2a)\n; dead in its literal sense is purely descriptive, but when applied to things (like a house in 2b), it becomes an evaluation; finally, metallic is also neutral, but it is used to evaluate a sound negatively in (2c), as shown by the phrasing at least ... but.\nInstead of listing frequencies, of course, we could calculate the association strength between the pattern [there V link something ADJ about NP] and the adjectives occurring in it.\nI will discuss in more detail how this is done in the next subsection; for now, suffice it to say that it would give us the ranking in Table\nThis case study is meant to introduce the notion of grammar patterns and to show that these patterns often have a relatively stable meaning that can be uncovered by looking at the words that are frequent in (or strongly associated with) 8 Grammar them.\nLike the preceding case study, it also introduced the idea that the relationship between words and units of grammatical structure can be investigated using the logic of association measures.\nThe next sections look at this in more detail.\nCollostructional analysis\nUnder the definition of corpus linguistics used throughout this book, there should be nothing surprising about the idea of investigating the relationship between words and units of grammatical structure based on association measures: a grammatical structure is just another condition under which we can observe the occurrence of lexical items.\nThis is the basic idea of collostructional analysis, a methodological perspective first proposed in Case study: The ditransitive\nTheir design is broadly deductive, as their hypothesis is that constructions have meaning and specifically, that the ditransitive has a 'transfer' meaning.\nThe design has two nominal variables: Argument Structure (with the values ditransitive and other) and Verb (with values corresponding to all verbs occurring in the construction).\nThe prediction is that the most strongly associated verbs will be verbs of literal or metaphorical transfer.\nTable Case studies\nHowever, note that on the basis of lists like that in Table 8 Grammar Case study: Ditransitive and prepositional dative Collostructional analysis can also be applied in the direct comparison of two grammatical constructions (or other grammatical features), analogous to the differential collocate design discussed in Chapter 7.\nFor example, Generally speaking, the list for the ditransitive is very similar to the one we get if we calculate the simple collexemes of the construction; crucially, many of the differential collexemes of the to-dative highlight the spatial distance covered by the transfer, which is in line with what the hypothesis predicts.\nCase study: Negative evidence\nRecall from the introductory chapter that one of the arguments routinely made against corpus linguistics is that corpora do not contain negative evidence.\nEven Case studies (3) * He shines Tony books.\nThey point out that this sentence will not occur in any given finite corpus, but that this does not allow us to declare it ungrammatical, since it could simply be one of infinitely many sentences that \"simply haven't occurred yet\".\nThey then offer the same solution Chomsky has repeatedly offered: It is only by asking a native or expert speaker of a language for their opinion of the grammaticality of a sentence that we can hope to differentiate unseen but grammatical constructions from those which are simply grammatical but unseen Grammar\nHowever, as Table Note that since zero is no different from any other frequency of occurrence, this procedure does not tell us anything about the difference between an intersection of variables that did not occur at all and an intersection that occurred with any other frequency less than the expected one.\nAll the method tells us is whether an occurrence of zero is significantly less than expected.\nIn other words, the method makes no distinction between zero occurrence and other less-frequent-than-expected occurrences.\nHowever, Case studies\nOf course, applying significance testing to zero occurrences of some intersection of variables is not always going to provide a significant result: if one (or both) of the values of the intersection are rare in general, an occurrence of zero may not be significantly less than expected.\nIn this case, we still do not know whether the absence of the intersection is due to chance or to its impossibility -but with such rare combinations, acceptability judgments are also going to be variable.\nGrammar Words and their grammatical properties\nStudies that take a collocational perspective on associations between words and grammatical structure tend to start from one or more grammatical structures and inductively identify the words associated with these structures.\nMoving away from this perspective, we find research designs that begin to resemble more closely the kind illustrated in Chapters 5 and 6, but that nevertheless include lexical items as one of their variables.\nThese designs will typically start from a word (or a small set of words) and identify associated grammatical (structural and semantic) properties.\nThis is of interest for many reasons, for example in the context of how much idiosyncrasy there is in the grammatical behavior of lexical items in general (for example, what are the grammatical differences between near synonyms), or how much of a particular grammatical alternation is lexically determined.\nCase study: Complementation of begin and start\nThere are a number of verbs in English that display variation with respect to their complementation patterns, and the factors influencing this variation have provided (and continue to provide) an interesting area of research for corpus linguists.\nIn an early example of such a study, (4) a. It rained almost every day, and she began to feel imprisoned.\n(BNC A7H) b. (BNC ABS)\nSchmid's study is deductive.\nHe starts by deriving from the literature two hypotheses concerning the choice between the verbs begin and start on the one hand and to-and the ing-complements on the other: First, that begin signals gradual onsets and start signals sudden ones, and second, that ing-clauses are typical of dynamic situations while to-clauses are typical of stative situations.\nHis focus is on the second hypothesis, which he tests on the LOB corpus by, first, identifying all occurrences of both verbs with both complementation patterns and, second, categorizing them according to whether the verb in the complement clause refers to an activity, a process or a state.\nThe study involves three 8.2 Case studies nominal variables: Matrix Verb (with the values begin and start), Complementation (with the values ing-clause and to-clause) and Aktionsart (with the values activity, process and state).\nThus, we are dealing with a multivariate design.\nThe prediction with respect to the complementation pattern is clearto-complements should be associated with activities and ing-complements with states, with processes falling somewhere in between.\nThere is no immediate prediction with respect to the choice of verb, but Schmid points out that activities are more likely to have a sudden onset, while states and processes are more likely to have a gradual onset, thus the former might be expected to prefer start and the latter begin.\nSchmid does not provide an annotation scheme for the categories activity, process and state, discussing these crucial constructs in just one short paragraph: Essentially, the three possible types of events that must be considered in the context of a beginning are activities, processes and states.\nThus, the speaker may want to describe the beginning of a human activity like eating, working or singing; the beginning of a process which is not directly caused by a human being like raining, improving, ripening; or the beginning of a state.\nSince we seem to show little interest in the beginning of concrete, visible states (cf. e.g. ? The lamp began to stand on the table.)\nthe notion of state is in the present context largely confined to bodily, intellectual and emotive states of human beings.\nExamples of such \"private states\" (5) A simple annotation scheme for Aktionsart a. activity: any externally perceivable event under the control of an animate agent.\nb. process: any externally perceivable event not under the control of an animate agent, including events involving involuntary animate themes (like cry, menstruate, shiver) and events not involving animate entities.\nc. state: any externally not perceivable event involving human cognition, as well as unchanging situations not involving animate entities.\n8 Grammar Schmid also does not state how he deals with passive sentences like those in (6a, b): (6) a.\nThese could be annotated with reference to the grammatical subject; in this case they would always be processes, as passive subjects are never voluntary agents of the event described.\nAlternatively, they could be annotated with reference to the logical subject, in which case (6a) would be a state ('someone feels something') and (6b) would be an activity ('someone makes domestic ritual objects').\nLet us choose the second option here.\nQuerying (7a, b) yields 348 hits (Schmid reports 372, which corresponds to the total number of hits for these verbs in the corpus, including those with other kinds of complementation):\nAnnotating all 348 hits according to the annotation scheme sketched out above yields the data in Table As always in a configural frequency analysis, we have to correct for multiple testing: there are twelve cells in our table, so our probability of error must be lower than 0.05 /12 = 0.0042.\nThe individual cells (i.e., intersections of variables) have one degree of freedom, which means that our critical 𝜒 2 value is 8.20.\nThis means that there are two types and three antitypes that reach significance: as predicted, activity verbs are positively associated with the verb start in combination with ing-clauses and state verbs are positively associated with the verb begin in combination with to-clauses.\nProcess verbs are also associated with begin with to-clauses, but only marginally significantly so.\nAs for the antitypes, all three verb classes are negatively associated (i.e., less frequent than expected) with the verb begin with ing-clauses, which suggests that this complementation pattern is generally avoided with the verb begin, but this avoidance is particularly pronounced with state and process verbs, where it is statistically significant: the verb begin and state/process verbs both avoid ing-complementation, and this avoidance seems to add up when they are combined.\nFaced with these results, we might ask, first, how they relate to two simpler tests of Schmid's hypothesis -namely two bivariate designs separately testing (a) the relationship between Aktionsart and Complementation, (b) the relationship between Aktionsart and Matrix Verb and (c) the relationship between Matrix Verb and Complementation Type.\nWe have all the data we need to test this in Table Case studies Case studies\nThe second question we could ask faced with Schmid's results is to what extent his second hypothesis -that begin is used with gradual beginnings and start with sudden ones -is relevant for the results.\nAs mentioned above, it is not tested directly, so how could we remedy this?\nOne possibility is to look at each of the 348 cases in the sample and try to determine the gradualness or suddenness of the beginning they denote.\nThis is sometimes possible, as in (4a) above, where the context suggests that the referent of the subject began to feel imprisoned gradually the longer the rain went on, or in (4c), which suggests that the crying began suddenly as soon as the baby woke up.\nBut in many other cases, it is very difficult to judge, whether a beginning is sudden or gradual -as in (4b, c).\nTo come up with a reliable annotation scheme for this categorization task would be quite a feat.\nThere is an alternative, however: speakers sometimes use adverbs that explicitly refer to the type of beginning.\nA query of the BNC for ⟨ [word=\".*ly\"%c] [hw=\"(begin|start)\"] ⟩ yields three relatively frequent adverbs (occurring more than 10 times) indicating suddenness (immediately, suddenly and quickly), and three indicating gradualness (slowly, gradually and eventually).\nThere are more, like promptly, instantly, rapidly. leisurely, reluctantly, etc., which are much less frequent.\nBy extracting just these cases, we can directly test the hypothesis that begin signals gradual and start sudden onsets.\nThe BNC contains 307 cases of [begin/ start to V inf ] and [begin/start Ving] directly preceded by one of the six adverbs mentioned above.\nTable\nSince there are eight cells in the table, the corrected 𝑝-value is 0.05 /8 = 0.00625; the individual cells have one degree of freedom, so the critical 𝜒 2 value is 7.48.\nThere are two significant types: sudden ∩ start ∩ ing and gradual ∩ begin ∩ to.\nThere is one significant and one marginally significant antitype: sudden ∩ start ∩ to and gradual ∩ begin ∩ ing, respectively.\nThis corroborates the hypothesis that begin signals gradual onsets and start signals sudden ones, at least when the matrix verbs occur with their preferred complementation pattern.\nSumming up the results of both studies, we could posit two \"prototype\" patterns (in the sense of cognitive linguistics): [begin gradual V stative ing ] and [start sudden to V activity inf ], and we could hypothesize that speakers will choose the pattern that matches most closely the situation they are describing (something that could then be tested, for example, in a controlled production experiment).\nThis case study demonstrated a complex design involving grammar, lexis and semantic categories.\nIt also demonstrated that semantic categories can be included in a corpus linguistic design in the form of categorization decisions on the basis of an annotation scheme (in which case, of course, the annotation scheme must be documented in sufficient detail for the study to be replicable), or in the form of lexical items signaling a particular meaning explicitly, such as adverbs of gradualness (in which case we need a corpus large enough to contain a sufficient number of hits including these items).\nIt also demonstrated that such corpus-based studies may result in very specific hypotheses about the function of lexicogrammatical structures that may become the basis for claims about mental representation.\nGrammar and context\nThere is a wide range of contextual factors that are hypothesized or known to influence grammatical variation.\nThese include information status, animacy and length, which we already discussed in the case studies of the possessive constructions in Chapters 5 and 6.\nSince we have dealt with them in some detail, they will not be discussed further here, but they have been extensively studied for a range of phenomena (e.g. Case studies\nCase study: Adjective order and frequency\nIn a comprehensive study on adjective order already mentioned in Let us replicate part of Wulff's study using the spoken part of the BNC (Wulff uses the entire BNC).\nIf we extract all sequences of exactly two adjectives occurring before a noun (excluding comparatives and superlatives), and include only those adjective pairs that (a) occur at least five times, and (b) do not occur more than once in the opposite order, we get the sample in Table\nLet us first look at just the ten most frequent adjective pairs (Ranks 1-10).\nThe mean frequency of the first adjective is 5493.2, that of the second adjective is 4042.7.\nPending significance testing, this would corroborate Wulff's hypothesis.\nHowever, in six of these ten cases the second adjective is actually more frequent than the first, which would contradict Wulff's hypothesis.\nThe problem here is that some of the adjectives in first position, like good and right, are very frequent while some adjectives in second position, like honourable and northern are very infrequent -this influences the mean frequencies substantially.\nHowever, when comparing the frequency of the first and second adjective, we are making a binary choice concerning which of the adjectives is more frequent -we ignore the size of the difference.\nIf we look at the ten next most frequent adjectives, we find the situation reversed -the mean frequency is 3672.3 for the first adjective and 4072 for the second adjective, contradicting Wulff's hypothesis, but in seven of the ten cases, the first adjective is more frequent, corroborating Wulff's hypothesis.\nCase studies\nBoth ways of looking at the issue have disadvantages.\nIf we go by mean frequency, then individual cases might inflate the mean of either of the two adjectives.\nIn contrast, if we go by number of cases, then cases with very little difference in frequency count just as much as cases with a vast difference in frequency.\nIn cases like this, it may be advantageous to apply both methods and reject the null hypothesis only if both of them give the same result (and of course, our sample should be larger than ten cases).\nLet us now apply both perspectives to the entire sample in Table Next, let us look at the number of cases that match the prediction.\nThere are 34 cases where the first adjective is more frequent, and 25 cases where the second adjective is more frequent, which seems to corroborate Wulff's hypothesis, but as Table This case study was meant to demonstrate that sometimes we can derive different kinds of quantitative predictions from a hypothesis which may give us different results; in such cases, it is a good idea to test all predictions.\nThe case study also shows that word frequency may have effects on grammatical variation, which is interesting from a methodological perspective because not only is corpus linguistics the only way to test for such effects, but corpora are also the only source from which the relevant values for the independent variable can be extracted.\nGrammar Case study:\nBinomials and sonority Frozen binomials (i.e. phrases of the type flesh and blood, day and night, size and shape) have inspired a substantial body of research attempting to uncover principles determining the order of the constituents.\nA number of semantic, syntactic and phonological factors have been proposed and investigated using psycholinguistic and corpus-based methods (see In order to test this hypothesis, we need a sample of binomials.\nIn the literature, such samples are typically taken from dictionaries or similar lists assembled for other purposes, but there are two problems with this procedure.\nFirst, these lists contain no information about the frequency (and thus, importance) of the individual examples.\nSecond, these lists do not tell us exactly how \"frozen\" the phrases are; while there are cases that seem truly non-reversible (like flesh and blood), others simply have a strong preference (day and night is three times as frequent as night and day in the BNC) or even a relatively weak one (size and shape is only one-and-a-half times as frequent as shape and size).\nIt is possible that the degree of frozenness plays a role -for example, it would be expected that binomials that never (or almost never) occur in the opposite order would display the influence of any factor determining order more strongly than those where the two possible orders are more equally distributed.\nWe can avoid these problems by drawing our sample from the corpus itself.\nFor this case study, let us select all instances of [NOUN and NOUN] that occur at least 30 times in the BNC.\nLet us further limit our sample to cases where both nouns are monosyllabic, as it is known from the existing research literature that length and stress patterns have a strong influence on the order of binomials.\nLet us also exclude cases where one or both nouns are in the plural, as we are interested in the influence of the final consonant, and it is unclear whether this refers to the final consonant of the stem or of the word form.\nThis will give us the 78 expressions in Table\nThe result of all these steps is shown in Table\nLet us first simply look at the number of cases for which the claim is true or false.\nThere are 42 cases where the second word's final consonant is more sonorant than that of the second word (as predicted), and 36 cases where the second word's final consonant is less sonorant than that of the first (counter to the prediction).\nAs Table We could, of course, limit our analysis to cases with a high degree of frozenness, say, above 90 percent (the data is available, so you might want to try).\nHowever, it would be even better to keep all our data and make use of the rank order 8.2 Case studies that the frozenness measure provides: the prediction is that cases with a high frozenness rank will adhere to the sonority constraint with a higher probability than those with a low frozenness rank.\nTable\nLike the case study in Section 8.2.4.1, this case study was intended to show that sometimes we can derive different kinds of quantitative predictions from a hypothesis; however, in this case one of the possibilities more accurately reflects the hypothesis and is thus the one we should base our conclusions on.\nThe case study was also meant to provide an example of a corpus-based design where it is more useful to operationalize one of the constructs (Frozenness) as an ordinal, rather than a nominal variable.\nIn terms of content, it was meant to demonstrate how phonology can interact with grammatical variation (or, in this case, the absence of variation) and how this can be studied on the basis of corpus data; cf.\nCase study: Horror aequi\nIn a number of studies, Günter Rohdenburg and colleagues have studied the influence of contextual (and, consequently, conceptual) complexity on grammatical\nTake the example of matrix verbs that normally occur alternatively with a to-clause or an ing-clause (like begin and start in Case Study 8.2.3.1 above).\n(9) a. I started to think about my childhood again...\n(BNC A0F) b.\nSo I started thinking about alternative ways to earn a living...\n(BNC C9H) c. ... in the future they will actually have to start to think about a fairer electoral system...\n(BNC JSG)\nd.\nHe will also have to start thinking about a partnership with Skoda before (BNC A6W)\nImpressionistically, this seems to be true: in the BNC there are 11 cases of started to think about, 18 of started thinking about, only one of to start to think about but 35 of to start thinking about.\nLet us attempt a more comprehensive analysis and look at all cases of the verb start with a clausal complement in the BNC.\nSince we are interested in the influence of the tense/aspect form of the verb start on complementation choice, let us distinguish the inflectional forms start (base form), starts (3rd person), started (past tense/past participle) and starting (present participle); let us further distinguish cases of the base form start with and without the infinitive marker to (we can derive the last two figures by first searching for (10a) and (10b) and then for (10c) and (10d), and then subtracting the frequencies of the latter two from those of the former two, respectively\n: This case study is intended to introduce the notion of horror aequi, which has been shown to influence a number of grammatical and morphological variation phenomena (cf. e.g. Case study: Synthetic and analytic comparatives and persistence The horror aequi principle has been shown to have an effect on certain kinds of variation and it can plausibly be explained as a way of reducing complexity (if the same structure occurs twice in a row, this might cause problems for language processing).\nHowever, there is another well-known principle that is, in a way, the opposite of horror aequi: structural priming.\nThe idea of priming comes from psychology, where it refers to the fact that the response to a particular stimulus (the target) can be influenced by a preceding stimulus (the prime).\nFor example, if subjects are exposed to a sequence of two pictures, they will identify an object on the second picture (say, a loaf of bread) faster and more accurately if it is related to the the scene in the first picture (say, a kitchen counter) (cf.\nFor example, Let us attempt to replicate his findings in a small analysis focusing only on persistence and disregarding other factors.\nStudies of structural priming have two nominal variables: the Prime, a particular grammatical structure occurring in a discourse, and the Target, the same (or a similar) grammatical structure occurring in the subsequent discourse.\nIn our case, the grammatical structure is Comparative with the two values analytic and synthetic.\nIn order to determine whether priming occurs and under what conditions, we have to extract a set of potential targets and the directly preceding discourse from a corpus.\nThe hypothesis is always that the value of the prime will correlate with the value of the target -in our case, that analytic comparatives have a higher probability of occurrence after analytic comparatives and synthetic comparatives have a higher probability of occurrence after synthetic comparatives.\nFor our purposes, let us include only adjectives with exactly two syllables, as length and stress pattern are known to have an influence on the choice between the two strategies and must be held constant in our design.\nLet us further focus on adjectives with a relatively even distribution of the two strategies and include only cases where the less frequent comparative form accounts for at least forty percent of all comparative forms.\nFinally, let us discard all adjectives that occur as comparatives less than 20 times in the BNC.\nThis will leave just six adjectives: angry, empty, friendly, lively, risky and sorry.\nLet us extract all synthetic and analytic forms of these adjectives from the BNC.\nThis will yield 452 hits, of which two are double comparatives (more livelier), which are irrelevant to our design and must be discarded.\nThis leaves 450 potential targets -241 analytic and 209 synthetic.\nOf these, 381 do not have an additional comparative form in a span of 20 tokens preceding the comparativewhatever determined the choice between analytic and synthetic comparatives in these cases, it is not priming.\nThese non-primed comparatives are fairly evenly distributed -194 are analytic comparatives and 187 are synthetic, which is not significantly different from a chance distribution (𝜒 2 = 0.13, df = 1, 𝑝 = 0.7199), suggesting that our sample of adjectives does indeed consist of cases that are fairly evenly distributed across the two comparative strategies (see Supplementary Online Material for the complete annotated concordance).\nGrammar\nThis leaves 69 targets that are preceded by a comparative prime in the preceding span of 20 tokens.\nTable (11) a.\nBut the statistics for the second quarter, announced just before the October Conference of the Conservative Party, were even more damaging to the Government showing a rise of 17 percent on 1989.\nIndeed these figures made even sorrier reading for the Conservatives when one realised...\n(BNC G1J)\nb.\nOver the next ten years, China will become economically more liberal, internationally more friendly...\n(BNC ABD)\nObviously, we would expect a much stronger priming effect in a situation like that in (11b), where one word intervenes between the two comparatives, than in a situation like (11a), where 17 words (and a sentence boundary) intervene.\nLet us therefore restrict the context in which we count analytic comparatives to a size more likely to lead to a priming effect -say, seven words (based on the 8.2 Case studies factoid that short term memory can hold up to seven units).\nTable\nThis case study demonstrates structural priming\n(cf. Variation and change Case study: Sex differences in the use of tag questions\nGrammatical differences may also exist between varieties spoken by subgroups of speakers defined by demographic variables, for example, when the speech of younger speakers reflects recent changes in the language, or when speakers from different educational or economic backgrounds speak different established sociolects.\nEven more likely are differences in usage preference.\nFor example, This kind of analysis requires very careful, largely manual data extraction and annotation so it is limited to relatively small corpora, but let us see what we can do in terms of a larger-scale analysis.\nLet us focus on tag questions with negative polarity containing the auxiliary be (e.g. isn't it, wasn't she, am I not, was it not).\nThese can be extracted relatively straightforwardly even from an untagged corpus using the following queries: The query in (12a) will find all finite forms of the verb be (as non-finite forms cannot occur in tag questions), followed by the negative clitic n't, followed by a pronoun; the query in (12b) will do the same thing for the full form of the particle not, which then follows rather than precedes the pronoun.\nBoth queries will only find those cases that occur before a punctuation mark signaling a clause boundary (what to include here will depend on the transcription conventions of the corpus, if it is a spoken one).\nThe queries are meant to work for the spoken portion of the BNC, which uses the comma for all kinds of things, including hesitation or incomplete phrases, so we have to make a choice whether to exclude it and increase the precision or to include it and increase the recall (I will choose the latter option).\nThe queries are not perfect yet: British English also has the form ain't it, so we might want to include the query in (13a).\nHowever, ain't can stand for be or for have, which lowers the precision somewhat.\nFinally, there is also the form innit in (some varieties of) British English, so we might want to include the query in (13b).\nHowever, this is an invariant form that can occur with any verb or auxiliary in the main clause, so it will decrease the precision even further.\nWe will ignore ain't and innit here (they are not particularly frequent and hardly change the results reported below): (13) a.\nIn the part of the spoken BNC annotated for speaker sex, there are 3751 hits for the patterns in (12a,b) for female speakers (only 20 of which are for 12b), and 3050 hits for male speakers (only 42 of which are for 12b).\nOf course, we cannot 8.2\nCase studies assume that there is an equal amount of male and female speech in the corpus, so the question is what to compare these frequencies against.\nObviously, such tag questions will normally occur in declarative sentences with positive polarity containing a finite form of be.\nSuch sentences cannot be retrieved easily, so it is difficult to determine their precise frequency, but we can estimate it.\nLet us search for finite forms of be that are not followed by a negative clitic (isn't) or particle (is not) within the next three tokens (to exclude cases where the particle is preceded by an adverb, as in is just/obviously/... not).\nThere are 146 493 such occurrences for female speakers and 215 219 for male speakers.\nThe query will capture interrogatives, imperatives, subordinate clauses and other contexts that cannot contain tag questions, so let us draw a sample of 100 hits from both samples and determine how many of the hits are in fact declarative sentences with positive polarity that could (or do) contain a tag question.\nLet us assume that we find 67 hits in the female-speaker sample and 71 hits in the male-speaker sample to be such sentences.\nWe can now adjust the total results of our queries by multiplying them with 0.67 and 0.71 respectively, giving us 98 150 sentences for female and 152 806 sentences for male speakers.\nIn other words, male speakers produce 60.89 percent of the contexts in which a negative polarity tag question with be could occur.\nWe can cross-check this by counting the total number of words uttered by male and female speakers in the spoken part of the BNC: there are 5 654 348 words produced by men and 3 825 804 words produced by women, which means that men produce 59.64 percent of the words, which fits our estimate very well.\nThese numbers can now be used to compare the number of tag questions against, as shown in Table The difference between male and female speakers is highly significant, with female speakers using substantially more tag questions than expected, and male speakers using substantially fewer (𝜒 2 = 743.07, df = 1, 𝑝 < 0.001).\nThis case study was intended to introduce the study of sex-related differences in grammar (or grammatical usage); cf.\nCase study: Language change\nGrammatical differences between varieties of a language will generally change over time -they may increase, as speech communities develop separate linguistic identities or even lose contact with each other, or they may decrease, e.g. through mutual influence.\nFor example, The basic design with which to test the convergence or divergence of two varieties with respect to a particular feature is a multivariate one with Variety and 8.2 Case studies Period as independent variables and the frequency of the feature as a dependent one.\nLet us try to apply such a design to notwithstanding using the LOB, BROWN, FLOB and FROWN corpora (two British and two American corpora each from the 1960s and the 1990s).\nNote that these corpora are rather small and 30 years is not a long period of time, so we would not necessarily expect results even if the hypothesis were correct that American English reintroduced postpositional notwithstanding in the 20th century (it is likely to be correct, as Berlage shows on a much larger data sample from different sources).\nNotwithstanding is a relatively infrequent adposition: there are only 36 hits in the four corpora combined.\nTable\nDue to the small number of cases, we would be well advised not to place too much confidence in our results, but as it stands they fully corroborate Berlage's claims that British English prefers the prepositional use and American English has recently begun to prefer the postpositional use.\nThis case study is intended to provide a further example of a multivariate design and to show that even small data sets may provide evidence for or against a hypothesis.\nIt is also intended to introduce the study of the convergence and/ or divergence of varieties and the basic design required.\nThis field of studies is of interest especially in the case of pluricentric languages like, for example, English, Spanish or Arabic (see Grammar Case study: Grammaticalization\nOne of the central issues in grammaticalization theory is the relationship between grammaticalization and discourse frequency.\nVery broadly, the question is whether a rise in discourse frequency is a precondition for (or at least a crucial driving force in) the grammaticalization of a structure, or whether it is a consequence.\nSince corpora are the only source for the identification of changes in discourse frequency, this is a question that can only be answered using corpus-linguistic methodology.\nAn excellent example is He uses the OED's citation database as a corpus (not just the citations given in the OED's entry for the specific phenomena he is interested in, but all citations used in the entire OED).\nIt is an interesting question to what extent such a citation database can be treated as a corpus (cf. the extensive discussion in Hoffmann 2004).\nOne argument against doing so is that it is an intentional selection of certain examples over others and thus may not yield an authentic picture of any given phenomenon.\nHowever, as Mair points out, the vast majority of examples of a given phenomenon X will occur in citations that were collected to illustrate other phenomena, so they should constitute random samples with respect to X. The advantage of citation databases for historical research is that the sources for citations will have been carefully checked and very precise information will be available as to their year of publication and their author.\nLet us look at one of Mair's examples and compare his results to those derived from more traditional corpora, namely the Corpus of Late Modern English Texts (CLMET), LOB and FLOB.\nThe example is that of the going-to future.\nIt is relatively easy to determine at what point at the latest the sequence [going to V inf ] was established as a future marker.\nIn the literature on going to, the following example from the 1482 Revelation to the Monk of Evesham is considered the first documented use with a future meaning (it is also the first citation in the OED): ( Mair also notes that the going-to future is mentioned in grammars from 1646 onward; at the very latest, then, it was established at the end of the 17th century.\nIf a rise in discourse frequency is a precondition for grammaticalization, we should see such a rise in the period leading up to the end of the 17th century; if not, we should see such a rise only after this point.\nCase studies Figure\nThis case study demonstrates that very large collections of citations can indeed be used as a corpus, as long as we are investigating phenomena that are likely to occur in citations collected to illustrate other phenomena; the results are very similar to those we get from well-constructed linguistic corpora.\nThe case study also demonstrates the importance of corpora in diachronic research, a field of study which, as mentioned in Chapter 1 has always relied on citations drawn from authentic texts, but which can profit from querying large collections of such texts and quantifying the results.\nGrammar and cultural analysis\nLike words, grammatical structures usually represent themselves in corpus linguistic studies -they are either investigated as part of a description of the syntactic behavior of lexical items or they are investigated in their own right in order to learn something about their semantic, formal or functional restrictions.\nHowever, like words, they can also be used as representatives of some aspect of the speech community's culture, specifically, a particular culturally defined scenario.\nTo take a simple example: if we want to know what kinds of things are transferred between people in a given culture, we may look at the theme arguments of ditransitive constructions in a large corpus; we may look for collocates in the verb and theme positions of the ditransitive if we want to know how particular things are transferred (cf. Case study: He said, she said In a paper on the medial representation of men and women, Caldas-Coulthard (1993) finds that men are quoted vastly more frequently than women in the COBUILD corpus (cf. also Chapter 9).\nShe also notes in passing that the verbs of communication used to introduce or attribute the quotes differ -both men's and women's speech is introduced using general verbs of communication, such as say or tell, but with respect to more descriptive verbs, there are differences: \"Men shout and groan, while women (and children) scream and yell\"\nThe construction [QUOTE + Subj + V] is a perfect example of a diagnostic for a cultural frame: it is routinely used (in written language) to describe a speech event.\nCrucially, the verb slot offers an opportunity to introduce additional information (such as the manner of speaking, as in the examples of manner verbs verbs just mentioned (that often contain evaluations), but also the type of speech act being performed (ask, order, etc.).\nIt is also easy to find even in an untagged corpus, since it includes (by definition) a passage of direct speech surrounded by quotation marks, a subject that is, in an overwhelming number of cases, a pronoun, and a verb (or verb group) -typically in that order.\nIn a written corpus, we can thus query the sequence ⟨[word=\"''\"] [pos=\"pronoun\"] [pos=\"verb\"]⟩ to find the majority of examples of the construction.\nIn order to study differences in the representation of men and women, we can query the pronouns he and she separately to obtain representative samples of male and female speech act events without any annotation.\nCase studies\nThis design can be applied deductively, if we have hypotheses about the gender-specific usage of particular (sets of) verbs, or inductively, if we simply calculate the association strength of all verbs to one pronoun as compared to the other.\nIn either case we have two nominal variables, Subject of Quoted Speech, with the variables male (he) and female (she), and Speech Activity Verb with all occurring verbs as its values.\nTable\nThere is a clear difference that corroborates Caldas-Coulthard's casual observation: the top ten verbs of communication associated with men contain five verbs conveying a rough, unpleasant and/or aggressive manner of speaking (growl, grate, rasp, snarl, roar), while those for women only include one (snap, related to irritability rather than outright aggression).\nInterestingly, two very general communication verbs, say and write, are also typical for men's reported speech.\nWomen's speech is introduced by verbs conveying weakness or communicative subordination (whisper, cry, manage, protest, wail and deny).\nGrammar and counterexamples\nWhile this book focuses on quantitative designs, non-quantitative designs are possible within the general framework adopted.\nChapter 3 included a discussion of counterexamples and their place in a scientific framework for corpus linguistics.\nLet us conclude this chapter with a case study making use of them.\nCase study: To-vs. that-complements\nA good case study for English that is based largely on counterexamples is\nFor example, he takes the well-known claim that with verbs of knowing, infinitival complements present knowledge as subjective/personal, while that-clauses present knowledge as objective/impersonal/public.\nThis is supposed to explain acceptability judgments like the following 8 Grammar (15) a.\nHe found her to be intelligent.\nb.\n* I bet that if you look in the files, you'll find her to be Mexican.\nc.\nI bet that if you look in the files, you'll find that she is Mexican.\nThe crucial counterexample here would be one like (15b), with an infinitival complement that expresses knowledge that is \"public\" rather than \"personal/ experiential\"; also of interest would be examples with that-clauses that express personal/experiential knowledge.\nThe corresponding queries are easy enough to define: (16) a.\nThis query follows the specific example in (15b) very narrowly -we could, of course, define a broader one that would capture, for example, proper names and noun phrases in addition to pronouns.\nBut remember that we are looking for counterexamples, and if we can find these with a query following the structure of supposedly non-acceptable sentences very closely, they will be all the more convincing.\nThe BNC contains not just one, but many counterexamples.\nHere are some examples with that-complements expressing subjective, personal knowledge:\n(17) a. Erika was surprised to find that she was beginning to like Bach (BNC A7A) b.\n[A]che of loneliness apart, I found that I was stimulated by the challenge of finding my way about this great and beautiful city.\n(BNC AMC)\nAnd here are some with to-complements expressing objective, impersonal knowledge:\n(18) a. Li and her coworkers have been able to locate these sequence variations ... in the three-dimensional structure of the toxin, and found them to be concentrated in the 𝛽 sheets of domain II.\n(BNC ALV) b.\nThe visiting party, who were the first and last ever to get a good look at the crater of Perboewetan, found it to be about 1000 metres in diameter and about fifty metres deep.\n(BNC ASR)\nCase studies\nThese counterexamples (and others not cited here) in fact give us a new hypothesis as to what the specific semantic contribution of the to-complement may be: if used to refer to objective knowledge, it overwhelmingly refers to situations where this objective knowledge was not previously known to the participants of the situation described.\nIn fact, if we extend our search for counterexamples beyond the BNC to the world-wide web, we find examples that are even more parallel to (15b), such as ( (19) a.\nAfterwards found that French had stopped ship and found her to be German masquerading as Greek.\n(www.hmsneptune.com)\nb.\nI was able to trace back my Grandfathers name ... to Scotland and then into Sussex and Surrey, England.\nVery exciting!\nReason being is because we were told that the ancestors were Irish and we found them to be Scottish!\n(www.gettheeblowdryer.com)\nc. Eishaus\n-This place is an Erlangen institution, should you ever get to meet the owner you'll find him to be American, he runs this wonderful ice cream shop as his summer job away from his 'proper' job in the states.\n(theerlangenexpat.wordpress.com)\nAgain, what is different in these examples from the (supposedly unacceptable) example (15b) is that the knowledge in question is new (and surprising) to the participants of the situation -a seemingly Greek ship turning out to be German, a supposedly Irish ancestor turning out to be Scottish, and the proprietor of an icecream shop in the small Bavarian town of Erlangen turning out to be American.\nThis observation could now be used as the basis for a new hypothesis concerning the difference between the two constructions, but even if it is not or if this hypothesis turned out to be false, the counterexamples clearly disprove the claim by Wierzbicka and others concerning the subjective/objective distinction\nThis case study was intended to show how counterexamples may play a role in disproving hypotheses based on introspection and constructed examples (see Morphology We saw in Chapter 8 that the wordform-centeredness of most corpora and corpus-access tools requires a certain degree of ingenuity when studying structures larger than the word.\nIt does not pose particular problems for corpus-based morphology, which studies structures smaller than the word.\nCorpus morphology is mostly concerned with the distribution of affixes, and retrieving all occurrences of an affix plausibly starts with the retrieval of all strings potentially containing this affix.\nWe could retrieve all occurrences of -ness, for example, with a query like ⟨[word=\".+ness(es)?\"%c]⟩.\nThe recall of this query will be close to 100 percent, as all words containing the suffix -ness end in the string ness, optionally followed by the string es in the case of plurals.\nDepending on the tokenization of the corpus, this query might miss cases where the word containing the suffix -ness is the first part of a hyphenated compound, such as usefulness-rating or consciousness-altering; we could alter the query to something like ⟨[word=\".+ness(es)?(--.+=)?\"%c]⟩ if we believe that including these cases in our sample is crucial.\nThe precision of such a query will not usually be 100 percent, as it will also retrieve words that accidentally happen to end with the string specified in our query -in the case of -ness, these would be words like witness, governess or place names like Inverness.\nThe degree of precision will depend on how unique the string in our query is for the affix in question; for -ness and -ity it is fairly high, as there are only a few words that share the same string accidentally (examples like those just mentioned for -ness and words like city and pity for -ity), for a suffix like -ess ('female animate entity') it is quite low, as a query like ⟨[word=\".+ess(es)?\"%c]⟩ will also retrieve all words with the suffixes -ness and -less, as well as many words whose stem ends in ess, like process, success, press, access, address, dress, guess and many more.\nHowever, once we have extracted and -if necessary -manually cleaned up our data set, we are faced with a problem that does not present itself when studying lexis or grammar: the very fact that affixes do not occur independently but always as parts of words, some of which (like wordform-centeredness in the first sentence of this chapter) have been created productively on the fly for a specific purpose, while others (like ingenuity in the same sentence) are conventionalized lexical items that are listed in dictionaries, even though they are theoretically the result of attaching an affix to a known stem (like ingen-, also found in ingenious and, confusingly, its almost-antonym ingenuous).\nWe have to keep the difference between these two kinds of words in mind when constructing morphological research designs; since the two kinds are not always clearly distinguishable, this is more difficult than it sounds.\nAlso, the fact that affixes always occur as parts of words has consequences for the way we can, and should, count them; in quantitative corpus-linguistics, this is a crucial point, so I will discuss it in quite some detail before we turn to our case studies.\nQuantifying morphological phenomena Counting morphemes: Types, tokens and hapax legomena\nDetermining the frequency of a linguistic phenomenon in a corpus or under a particular condition seems a straightforward task: we simply count the number of instances of this phenomenon in the corpus or under that condition.\nHowever, this sounds straightforward (in fact, tautological) only because we have made tacit assumptions about what it means to be an \"instance\" of a particular phenomenon.\nWhen we are interested in the frequency of occurrence of a particular word, it seems obvious that every occurrence of the word counts as an instance.\nIn other words, if we know how often the word occurs in our data, we know how many instances there are in our data.\nFor example, in order to determine the number of instances of the definite article in the BNC, we construct a query that will retrieve the string the in all combinations of upper and lower case letters, i.e. at least the, The, and THE, but perhaps also tHe, ThE, THe, tHE and thE, just to be sure).\nWe then count the hits (since this string corresponds uniquely to the word the, we don't even have to clean up the results manually).\nThe query will yield 6 041 234 hits, so there are 6 041 234 instances of the word the in the BNC.\nWhen searching for grammatical structures (for example in Chapters 5 and 6), simply transferred this way of counting occurrences.\nFor example, in order to determine the frequency of the s-possessive in the BNC, we would define a reasonable query or set of queries (which, as discussed in various places in this book, can be tricky) and again simply count the hits.\nLet us assume that the query ⟨[pos=\"(POS|DPS)\"] [pos=\".*AJ.*\"]? [pos=\".*NN.*\"]⟩ is a reasonable approximation: it retrieves all instances of the possessive clitic (tagged POS in the BNC) or a possessive determiner (DPS), optionally followed by a word tagged as an adjective (AJ0, AJC or AJS, even if it is part of an ambiguity tag), followed by a word tagged as a noun (NN0, NN1 or NN2, even if it is part of an ambiguity tag).\nThis 9.1 Quantifying morphological phenomena query will retrieve 1 651 908 hits, so it seems that there are 1 651 908 instances of the s-possessive in the BNC.\nHowever, there is a crucial difference between the two situations: in the case of the word the, every instance is identical to all others (if we ignore upper and lower case).\nThis is not the case for the s-possessive.\nOf course, here, too, many instances are identical to other instances: there are exact repetitions of proper names, like King's Cross (322 hits) or People's revolutionary party (47), of (parts of) idiomatic expressions, like arm's length (216) or heaven's sake (187) or nonidiomatic but nevertheless fixed phrases like its present form (107) or child's best interest (26), and also of many free combinations of words that recur because they are simply communicatively useful in many situations, like her head (5105), his younger brother (112), people's lives (224) and body's immune system (29).\nThis means that there are two different ways to count occurrences of the spossessive.\nFirst, we could simply count all instances without paying any attention to whether they recur in identical form or not.\nWhen looking at occurrences of a linguistic item or structure in this way, they are referred to as tokens, so 1 651 908 is the token frequency of the possessive.\nSecond, we could exclude repetitions and count only the number of instances that are different from each other, for example, we would count King's Cross only the first time we encounter it, disregarding the other 321 occurrences.\nWhen looking at occurrences of linguistic items in this way, they are referred to as types; the type frequency of the s-possessive in the BNC is 268 450 (again, ignoring upper and lower case).\nThe type frequency of the, of course, is 1.\nLet us look at one more example of the type/token distinction before we move on.\nConsider the following famous line from the theme song of the classic television series \"Mister Ed\": (1) A horse is a horse, of course, of course...\nAt the word level, it consists of nine tokens (if we ignore punctuation): a, horse, is, a, horse, of, course, of, and course, but only of five types: a, horse, is, of, and course.\nFour of these types occur twice, one (is) occurs only once.\nAt the level of phrase structure, it consists of seven tokens: the NPs a horse, a horse, course, and course, the PPs of course and of course, and the VP is a horse, but only of three types: VP, NP and PP.\nIn other words, we can count instances at the level of types or at the level of tokens.\nWhich of the two levels is relevant in the context of a particular research design depends both on the kind of phenomenon we are counting and on our research question.\nWhen studying words, we will normally be interested in how often they are used under a particular condition, so it is their token frequency that is relevant to us; but we could imagine designs where we are mainly interested in whether a word occurs at all, in which case all that is relevant is whether its type frequency is one or zero.\nWhen studying grammatical structures, we will also mainly be interested in how frequently a particular grammatical structure is used under a certain condition, regardless of the words that fill this structure.\nAgain, it is the token frequency that is relevant to us.\nHowever, note that we can (to some extent) ignore the specific words filling our structure only because we are assuming that the structure and the words are, in some meaningful sense, independent of each other; i.e., that the same words could have been used in a different structure (say, an of -possessive instead of an s-possessive) or that the same structure could have been used with different words (e.g. John's spouse instead of his wife). Recall that in our case studies in Chapter 6 we excluded all instances where this assumption does not hold (such as proper names and fixed expressions); since there is no (or very little) choice with these cases, including them, let alone counting repeated occurrences of them, would have added nothing (we did, of course, include repetitions of free combinations, of which there were four in our sample: his staff, his mouth, his work and his head occurred twice each).\nObviously, instances of morphemes (whether inflectional or derivational) can be counted in the same two ways.\nTake the following passage from William Shakespeare's play Julius Cesar: (2) CINNA: ... Am I a married man, or a bachelor?\nThen, to answer every man directly and briefly, wisely and truly: wisely I say, I am a bachelor.\nLet us count the occurrences of the adverbial suffix -ly.\nThere are five word tokens that contain this suffix (directly, briefly, wisely, truly, and wisely), so its token frequency is five; however, there are only four types, since wisely occurs twice, so its type frequency in this passage is four.\nAgain, whether type or token frequency is the more relevant or useful measure depends on the research design, but the issue is more complicated than in the case of words and grammatical structures.\nLet us begin to address this problem by looking at the diminutive affixes -icle (as in cubicle, icicle) and mini-(as in minivan, mini-cassette).\nToken frequency\nFirst, let us count the tokens of both affixes in the BNC.\nThis is relatively easy in the case of -icle, since the string icle is relatively unique to this morpheme (the 9.1 Quantifying morphological phenomena name Pericles is one of the few false hits that the query ⟨[word=\".+icles?\"%c]⟩ will retrieve).\nIt is more difficult in the case of mini-, since there are words like minimal, minister, ministry, miniature and others that start with the string mini but do not contain the prefix mini-.\nOnce we have cleaned up our concordances (available in the Supplementary Online Material, file LMY7), we will find that -icle has a token frequency of 20 772 -more than ten times that of mini-, which occurs only 1702 times.\nWe might thus be tempted to conclude that -icle is much more important in the English language than mini-, and that, if we are interested in English diminutives, we should focus on -icle.\nHowever, this conclusion would be misleading, or at least premature, for reasons related to the problems introduced above.\nRecall that affixes do not occur by themselves, but always as parts of words (this is what makes them affixes in the first place).\nThis means that their token frequency can reflect situations that are both quantitatively and qualitatively very different.\nSpecifically, a high token frequency of an affix may be due to the fact that it is used in a small number of very frequent words, or in a large number of very infrequent words (or something in between).\nThe first case holds for -icle: the three most frequent words it occurs in (article, vehicle and particle) account for 19 195 hits (i.e., 92.41 percent of all occurrences).\nIn contrast, the three most frequent words with mini-(mini-bus, mini-bar and mini-computer) account for only 557 hits, i.e. 32.73 percent of all occurrences.\nTo get to 92.4 percent, we would have to include the 253 most frequent words (roughly two thirds of all types).\nIn other words, the high token frequency of -icle tells us nothing (or at least very little) about the importance of the affix; if anything, it tells us something about the importance of some of the words containing it.\nThis is true regardless of whether we look at its token frequency in the corpus as a whole or under specific conditions; if its token frequency turned out to be higher under one condition than under the other, this would point to the association between that condition and one or more of the words containing the affix, rather than between the condition and the affix itself.\nFor example, the token frequency of the suffix -icle is higher in the BROWN corpus (269 tokens) than in the LOB corpus (225 tokens).\nHowever, as Table Type frequency In contrast, the type frequency of an affix is a fairly direct reflection of the importance of the affix for the lexicon of a language: obviously an affix that occurs in many different words is more important than one that occurs only in a few words.\nNote that in order to compare type frequencies, we have to correct for the size of the sample: all else being equal, a larger sample will contain more types than a smaller one simply because it offers more opportunities for different types to occur (a point we will return to in more detail in the next subsection).\nA simple way of doing this is to divide the number of types by the number of tokens; the resulting measure is referred to very transparently as the type/token ratio (or TTR): (3) TTR = 𝑛 (types) 𝑛 (tokens)\nThe TTR is the percentage of types in a sample are different from each other; or, put differently, it is the mean probability that we will encounter a new type if we go through the sample item by item.\nFor example, the affix -icle occurs in just 31 different words in the BNC, so its TTR is 31 /20772 = 0.0015.\nIn other words, 0.15 percent of its tokens in the BNC are different from each other, the vast remainder consists of repetitions.\nPut differently, if we go through the occurrences of -icle in the BNC item by item, the probability that the next item instantiating this suffix will be a type we have not seen before is 0.15 percent, so we will encounter a new type on average once 9 Morphology every 670 words.\nFor mini-, the type-token ratio is much higher: it occurs in 382 different words, so its TTR is 382 /1702 = 0.2244.\nIn other words, almost a quarter of all occurrences of mini-are different from each other.\nPut differently, if we go through the occurrences of mini-in the BNC word by word, the probability that the next instance is a new type would be 22.4 percent, so we will encounter a new type about every four to five hits.\nThe differences in their TTRs suggests that mini-, in its own right, is much more central in the English lexicon than -icle, even though the latter has a much higher token frequency.\nNote that this is a statement only about the affixes; it does not mean that the words containing mini-are individually or collectively more important than those containing -icle (on the contrary: words like vehicle, article and particle are arguably much more important than words like minibus, minicomputer and minibar).\nLikewise, observing the type frequency (i.e. the TTR) of an affix under different conditions provides information about the relationship between these conditions and the affix itself, albeit one that is mediated by the lexicon: it tells us how important the suffix in question is for the subparts of the lexicon that are relevant under those conditions.\nFor example, there are 7 types and 9 tokens for mini-in the 1991 British FLOB corpus (two tokens each for mini-bus and mini-series and one each for mini-charter, mini-disc, mini-maestro, mini-roll and mini-submarine), so the TTR is 7 /9 = 0.7779.\nIn contrast, in the 1991 US-American FROWN corpus, there are 11 types and 12 tokens (two tokens for mini-jack, and one token each for mini-cavalry, Hapax legomena\nWhile type frequency is a useful way of measuring the importance of affixes in general or under specific conditions, it has one drawback: it does not tell us whether the affix plays a productive role in a language at the time from which we take our samples (i.e. whether speakers at that time made use of it when coining new words).\nAn affix may have a high TTR because it was productively used at the time of the sample, or because it was productively used at some earlier period in the history of the language in question.\nIn fact, an affix can have a high TTR even if it was never productively used, for example, because speakers 9.1 Quantifying morphological phenomena at some point borrowed a large number of words containing it; this is the case for a number of Romance affixes in English, occurring in words borrowed from Norman French but never (or very rarely) used to coin new words.\nAn example is the suffix -ence/-ance occurring in many Latin and French loanwords (such as appearance, difference, existence, influence, nuisance, providence, resistance, significance, vigilance, etc.), but only in a handful of words formed in English (e.g. abidance, forbearance, furtherance, hinderance, and riddance).\nIn order to determine the productivity (and thus the current importance) of affixes at a particular point in time, Harald Baayen (cf. e.g.\nOf course, not all hapax legomena are the result of productive rule-application: the words wordform-centeredness and ingenuity that I used in the first sentence of this chapter are both hapax legomena in this book (or would be, if I did not keep mentioning them).\nHowever, wordform-centeredness is a word I coined productively and which is (at the time of writing) not documented anywhere outside of this book; in fact, the sole reason I coined it was in order to use it as an example of a hapax legomenon later).\nIn contrast, ingenuity has been part of the English language for more than four-hundred years (the OED first records it in 1598); it occurs only once in this book for the simple reason that I only needed it once (or pretended to need it, to have another example of a hapax legomenon).\nSo a word may be a hapax legomenon because it is a productive coinage, or because it is infrequently needed (in larger corpora, the category of hapaxes typically also contains misspelled or incorrectly tokenized words which will have to be cleaned up manualy -for example, the token manualy is a hapax legomenon in this book because I just misspelled it intentionally, but the word manually occurs dozens of times in this book).\nBaayen's idea is quite straightforwardly to use the phenomenon of hapax legomenon as an operationalization of the construct \"productive application of a rule\" in the hope that the correlation between the two notions (in a large enough corpus) will be substantial enough for this operationalization to make sense.\n(4) HTR = 𝑛 (hapax legomena) 𝑛 (tokens)\nWe will refer to this measure as the hapax-token ratio (or HTR) by analogy with the term type-token ratio.\nNote, however, that in the literature this measure is referred to as P for \"Productivity\" (following Baayen, who first suggested the measure); I depart from this nomenclature here to avoid confusion with p for \"probability (of error)\".\nLet us apply this measure to our two diminutive affixes.\nThe suffix -icle has just five hapax legomena in the BNC (auricle, denticle, pedicle, pellicle and tunicle).\nThis means that its HTR is 5 /20772 = 0.0002, so 0.02 percent of its tokens are hapax legomena.\nIn contrast, there are 247 hapax legomena for mini-in the BNC (including, for example, mini-earthquake, Statistical evaluation\nAs pointed out in connection with the comparison of the TTRs for mini-in the FLOB and the FROWN corpus, we would like to be able to test differences between two (or more) TTRs (and, of course, also two or more HTRs) for statistical significance.\nTheoretically, this could be done very easily.\nTake the TTR: if we interpret it as the probability of encountering a new type as we move through our samples, we are treating it like a nominal variable Type, with the values new and seen before.\nOne appropriate statistical test for distributions of nominal values under different conditions is the 𝜒 2 test, which we are already more than familiar with.\nFor example, if we wanted to test whether the TTRs of -icle and mini-in the BNC differ significantly, we might construct a table like Table\nThe 𝜒 2 test would tell us that the difference is highly significant with a respectable effect size (𝜒 2 = 4334.67, df = 1, 𝑝 < 0.001, 𝜙 = 0.4392).\nFor HTRs, we could follow a similar procedure: in this case we are dealing with a nominal variable Type with the variables occurs only once and occurs more than once, so we could construct the corresponding table and perform the 𝜒 2 test.\n9.1\nQuantifying morphological phenomena\nHowever, while the logic behind this procedure may seem plausible in theory both for HTRs and for TTRs, in practice, matters are much more complicated.\nThe reason for this is that, as mentioned above, type-token ratios and hapax-token ratios are dependent on sample size.\nIn order to understand why and how this is the case and how to deal with it, let us leave the domain of morphology for a moment and look at the relationship between tokens and types or hapax legomena in texts.\nConsider the opening sentences of Jane Austen's novel Pride and Prejudice (the novel is freely available from Project Gutenberg and in the Supplementary Online Material, file TXQP): (5) It is a truth universally acknowledged, that a 2/-1 single man in possession of a 3 good fortune, must be in 2/-1 want of 2/-1 a 4 wife.\nHowever little known the feelings or views of 3 such a 5 man 2/-1 may be 2/-1 on his first entering a 6 neighbourhood, this truth 2/-1 is 2/-1 so well fixed in 3 the 2/-1 minds of 4 the surrounding families, that 2/-1 he is 3 considered the rightful property of 5 some one or 2/-1 other of 6 their daughters.\nAll words without a subscript are new types and hapax legomena at the point at which they appear in the text; if a word has a subscript, it means that it is a repetition of a previously mentioned word, the subscript is its token frequency at this point in the text.\nThe first repetition of a word is additionally marked by a subscript reading -1, indicating that it ceases to be hapax legomenon at this point, decreasing the overall count of hapaxes by one.\nAs we move through the text word by word, initially all words are new types and hapaxes, so the type-and hapax-counts rise at the same rate as the token counts.\nHowever, it only takes eight token before we reach the first repetition (the word a), so while the token frequency rises to 8, the type count remains 9\nMorphology constant at seven and the hapax count falls to six.\nSix words later, there is another occurrence of a, so type and hapax counts remain, respectively, at 12 and 11 as the token count rises to 14, and so on.\nIn other words, while the number of types and the number of hapaxes generally increase as the number of tokens in a sample increases, they do not increase at a steady rate.\nThe more types have already occurred, the more types there are to be reused (put simply, speakers will encounter fewer and fewer communicative situations that require a new type), which makes it less and less probable that new types (including new hapaxes) will occur.\nFigure\nTherefore, comparing TTRs derived from samples of different sizes will always make the smaller sample look more productive.\nIn other words, we cannot compare such TTRs, let alone evaluate the differences statistically -the result will simply be meaningless.\nThe same is true for HTRs, with the added problem that, under certain circumstances, it will decrease at some point as we keep increasing the sample size: at some point, all possible words will have been used, so unless new words are added to the language, the number of hapaxes will shrink again and finally drop to zero when all existing types have been used at least twice.\nWe will encounter the same problem when we compare the TTR or HTR of particular affixes or other linguistic phenomena, rather than that of a text.\nConsider Figures 9.2a and 9.2b, which show the TTR and the HTR of the verb suffixes -ise/-ize (occurring in words like realize, maximize or liquidize) and -ify (occurring in words like identify, intensify or liquify).\nAs we can see, the TTR and HTR of both affixes behave roughly like that of Jane Austen's vocabulary as a whole as we increase sample size: both of them 9.1\nQuantifying morphological phenomena\nHowever, note that -ify has a token frequency that is less than half of that of -ise/-ize, so the sample is much smaller: as in the example of lexical richness in Pride and Prejudice, this means that the TTR and the HTR of this smaller sample are exaggerated and our comparisons in Tables 9.4 and 9.5 as well as the accompanying statistics are, in fact, completely meaningless.\nThe simplest way of solving the problem of different sample sizes is to create samples of equal size for the purposes of comparison.\nWe simply take the size of the smaller of our two samples and draw a random sample of the same size from the larger of the two samples (if our data sets are large enough, it would be even better to draw random samples for both affixes).\nThis means that we lose some data, but there is nothing we can do about this (note that we can still include the discarded data in a qualitative description of the affix in question).\nThe TTR of -ise/-ize based on the random sub-sample is 78 /356 = 0.2191, that of -ify is still 49 /356 = 0.1376; the difference between the two suffixes is much clearer now, and a 𝜒 2 test shows that it is very significant, although the effect size is weak\n(cf. Table\nIn the case of the HTR, decreasing the sample size is slightly more problematic than in the case of the TTR.\nThe proportion of hapax legomena actually resulting from productive rule application becomes smaller as sample size decreases.\nTake example (\nCase studies Morphemes and stems\nOne general question in (derivational) morphology concerns the category of the stem which an affix may be attached to.\nThis is obviously a descriptive issue that can be investigated on the basis of corpora very straightforwardly simply by identifying all types containing the affix in question and describing their internal structure.\nIn the case of affixes with low productivity, this will typically add little insight over studies based on dictionaries, but for productive affixes, a corpus analysis will yield more detailed and comprehensive results since corpora will contain spontaneously produced or at least recently created items not (yet) found in dictionaries.\nSuch newly created words will often offer particularly clear insights into constraints that an affix places on its stems.\nFinally, corpus-based approaches are without an alternative in diachronic studies and yield particularly interesting results when used to study changes in the quality or degree of productivity, (cf. for example\nIn the study of constraints placed by derivational affixes on the stems that they combine with, the combinability of derivational morphemes (in an absolute sense or in terms of preferences) is of particular interest.\nAgain, corpus linguistics is a uniquely useful tool to investigate this.\nFinally, there are cases where two derivational morphemes are in direct competition because they are functionally roughly equivalent (e.g. -ness and -ity, both of which form abstract nouns from typically adjectival bases, -ise/-ize and -ify, 9 Morphology which form process verbs from nominal and adjectival bases, or -ic and -ical, which form adjectives from typically nominal bases).\nHere, too, corpus linguistics provides useful tools, for example to determine whether the choice between affixes is influenced by syntactic, semantic or phonological properties of stems.\nCase study: Phonological constraints on -ify\nAs part of a larger argument that -ise/-ize and -ify should be considered phonologically conditioned allomorphs, The most obvious constraint is that the syllable directly preceding -ify must carry the main stress of the word.\nThis has a number of consequences, of which we will focus on two: First, monosyllabic stems (as in falsify) are preferred, since they always meet this criterion.\nSecond, if a polysyllabic stem ends in an unstressed syllable, the stress must be shifted to that syllable (as in perSONify from PERson); Plag simply checks his neologisms against the literature, but we will evaluate the claims from the literature quantitatively.\nOur main hypothesis will be that neologisms with -ify do not differ from established types with respect to the fact that the syllable directly preceding the suffix must carry primary stress, with the consequences that (i) they prefer monosyllabic stems, and (ii) if the stem is polysyllabic, they prefer stems that already have the primary stress on the last syllable.\nOur independent variable is therefore Lexical Status with the values established word vs. neologism (which will be operationalized presently).\nOur 9.2 Case studies dependent variables are Syllabicity with the values monosyllabic and polysyllabic, and Stress Shift with the values required vs. not required (both of which should be self-explanatory).\nOur design compares two predefined groups of types with respect to the distribution that particular properties have in these groups; this means that we do not need to calculate TTRs or HTRs, but that we need operational definitions of the values established word and neologism.\nFollowing Plag, let us define neologism as \"coined in the 20th century\", but let us use a large historical dictionary (the Oxford English Dictionary, 3rd edition) and a large corpus (the BNC) in order to identify words matching this definition\n; this will give us the opportunity to evaluate the idea that hapax legomena are a good way of operationalizing productivity.\nExcluding cases with prefixed stems, the OED contains 456 entries or sub-entries for verbs with -ify, 31 of which are first documented in the 20th century.\nOf the latter, 21 do not occur in the BNC at all, and 10 do occur in the BNC, but are not hapaxes (see Table\nBefore we turn to the definition and sampling of established types, let us determine the precision and recall of the operational definition of neologism as \"hapax legomenon in the BNC\", using the formulas introduced in Chapter 4.\nPrecision is defined as the number of true positives (items that were found and that actually are what they are supposed to be) divided by the number of all positives (all items found); 10 of the 30 hapaxes in the BNC are actually neologisms, so the precision is 10 /30 = 0.3333.\nRecall is defined as the number of true positives divided by the number of true positives and false negatives (i.e. all items that should have been found); 10 of the 45 neologisms were actually found by using the hapax definition, so the recall is 10 /45 = 0.2222.\nIn other words, neither precision nor recall of the method are very good, at least for moderately productive affixes like -ify (the method will presumably give better results with highly productive affixes).\nLet us also determine the recall of neologisms from the OED (using the definition \"first documented in the 20th century according to the OED\"):\nthe OED lists 31 of the 45 neologisms, so the recall is 31 /45 = 0.6889; this is much better than the recall of the corpus-based hapax definition, but it also shows that if we combine 9 Morphology commodify, desertify, extensify, geriatrify corpus data and dictionary data, we can increase coverage substantially even for moderately productive affixes.\nLet us now turn to the definition of established types.\nGiven our definition of neologisms, established types would first have to be documented before the 20th century, so we could use the 420 types in the OED that meet this criterion (again, excluding prefixed forms).\nHowever, these 420 types contain many very rare or even obsolete forms, like duplify 'to make double', eaglify 'to make into an eagle' or naucify 'to hold in low esteem'.\nClearly, these are not \"established\" in any meaningful sense, so let us add the requirement that a type must occur in the BNC at least twice to count as established.\nLet us further limit the category to verbs first documented before the 19th century, in order to leave a clear diachronic gap between the established types and the productive types.\nThis leaves the words in Table Case study: Semantic differences between -ic and -ical Affixes, like words, can be related to other affixes by lexical relations like synonymy, antonymy, etc.\nIn the case of (roughly) synonymous affixes, an obvious research question is what determines the choice between them -for example, whether there are more fine-grained semantic differences that are not immediately apparent.\nOne way of approaching this question is to focus on stems that occur with both affixes (such as liqui(d) in liquidize and liquify/liquefy, scarce in scarceness and scarcity or electr-in electric and electrical) and to investigate the semantic contexts in which they occur -for example, by categorizing their collocates, analogous to the way Case studies\nA good example of this approach is found in Kaunisto uses a mixture of dictionaries and existing literature to identify potentially interesting values for the variable Semantic Category; we will restrict ourselves to dictionaries here.\nConsider the definitions from six major dictionaries in (\nSummarizing, we can posit the following four broad values for our variable Semantic Category, with definitions that are hopefully specific enough to serve as an annotation scheme: • devices and appliances working by electricity (light, appliance, etc.)\n• energy in the form of electricity • circuits, broadly defined as entities producing or carrying electricity, including (cord, outlet, plug, but also power plant, etc.)\nThe definitions are too heterogeneous to base a specific hypothesis on them, but we might broadly expect electric to be more typical for the categories device and circuit and electrical for the category industry.\nTable\nThe difference between electric and electrical is significant overall (𝜒 2 = 12.68, df = 3, 𝑝 < 0.01, 𝜙 = 0.2869), suggesting that the two words somehow differ with 9.2 Case studies 9\nMorphology respect to their preferences for these categories.\nSince we are interested in the nature of this difference, it is much more insightful to look at the 𝜒 2 components individually.\nThis gives us a better idea where the overall significant difference comes from.\nIn this case, it comes almost exclusively from the fact that electrical is indeed associated with the research and supply of electricity (industry), although there is a slight preference for electric with nouns referring to devices.\nGenerally, the two words seem to be relatively synonymous, at least in 1960s British English.\nLet us repeat the study with the BROWN corpus.\nTable\nAgain, the overall difference between the two words is significant and the effect is slightly stronger than in the LOB corpus (𝜒 2 = 22.83, df = 3, 𝑝 < 0.001, 𝜙 = 0.3413), suggesting a stronger differentiation between them.\nAgain, the most interesting question is where the effect comes from.\nIn this case, devices are much more frequently referred to as electric and less frequently as electrical than expected, and, as in the LOB corpus, the nouns in the category industry are more frequently referred to as electrical and less frequently as electric than expected (although not significantly so).\nAgain, there is no clear difference with respect to the remaining two categories.\nBroadly speaking, then, one of our expectations is borne out by the British English data and one by the American English data.\nWe would now have to look at larger corpora to see whether this is an actual difference between the two varieties or whether it is an accidental feature of the corpora used here.\nWe might also want to look at more modern corpora -the importance of electricity in our daily lives has changed quite drastically even since the 1960s, so the words may have specialized semantically more clearly in the meantime.\nFinally, we would look more closely at the categories we have used, to see whether a different or a more fine-grained categorization might reveal additional insights\nOf course, this kind of investigation can also be designed as an inductive study of differential collocates (again, like the study of synonyms such as high and tall).\nLet us look at the nominal collocates of electric and electrical in the BNC.\nTable\nThere is an additional pattern that would warrant further investigation: there are collocates for both variants that correspond to what some of the dictionaries we consulted refer to as 'produced by energy': shock, field and fire for electric and signal, energy, impulse for electrical.\nIt is possible that electric more specifically characterizes phenomena that are caused by electricity, while electrical characterizes phenomena that manifest electricity.\nThe case study demonstrates, then, that a differential-collocate analysis is a good alternative to the manual categorization and category-wise comparison of all collocates: it allows us to process very large data sets very quickly and then focus on the semantic properties of those collocates that are shown by the statistical analysis to differentiate between the variants.\nWe must keep in mind, however, that this kind of study does not primarily uncover differences between affixes, but differences between specific word pairs containing these affixes.\nThey are, as pointed out above, essentially lexical studies of near-synonymy.\nOf course, it is possible that by performing such analyses for a large number of word pairs containing a particular affix pair, general semantic differences may emerge, but since we are frequently dealing with highly lexicalized forms, there is no guarantee for this.\nCase studies\nCase study: Phonological differences between -ic and -ical\nIn an interesting but rarely-cited paper, Let us test this hypothesis using the LOB corpus.\nSince this is a written corpus, let us define Length in terms of letters and assume that this is a sufficiently close approximation to phonological length.\nTables 9.15 and 9.16 lists all types with the two suffixes from LOB in decreasing order of length; note that since the point here is to show the influence of length on suffix choice, prefixed stems, compound stems, etc. are included in their full form (the lists are included in a more readable format in the Supplementary Online Materials, file U7BR).\nWe can test the hypothesis based on the mean length of the two samples using a t-test, or by ranking them by length using the U test.\nAs mentioned in Chapter 6, word length (however we measure it) rarely follows a normal distribution, so the U test would probably be the better choice in this case, but let us use the 𝑡-test for the sake of practice (the data are there in full, if you want to calculate a U test).\nThere are 373 stem types occurring with -ic in the LOB corpus, with a mean length of 7.32 and a sample variance of 5.72; there are 153 stem types occurring with -ical, with a mean length of 6.60 and a sample variance of 4.57.\nApplying the formula in (15) from Chapter 6, we get a 𝑡-value of 2.97.\nThere are 314.31 degrees of freedom in our sample (as calculated using the formula in 16), which means that 𝑝 < 0.001.\nIn other words, length (as measured in letters) seems to have an influence on the choice between the two affixes, with longer stems favoring -ic.\nThis case study has demonstrated the use of a relatively simple operationalization to test a hypothesis about phonological length.\nWe have used samples of types rather than samples of tokens, as we wanted to determine the influence of stem length on affix choice -in this context, the crucial question is how many stems of a given length occur with a particular affix variant, but it does not matter how often a particular stem does so.\nHowever, if there was more variation between the two suffixes, the frequency with which a particular stem is used with a particular affix might be interesting, as it would allow us to approach the question by ranking stems in terms of their preference and then correlating this ranking with their length.\n9.2 Case studies Table Case study: Affix combinations\nIt has sometimes been observed that certain derivational affixes show a preference for stems that are already derived by a particular affix.\nFor example, For example, Case Study 9.2.1.3 demonstrates that longer stems generally seem to prefer the variant -ic; however, the mean length of derived stems is necessarily longer than that of non-derived stems, so it is puzzling, at first glance, that stems with the affix -olog-should prefer -ical.\nOf course, -olog-may be an exception, with derived stems in general preferring the shorter -ic; however, we would still need to account for this exceptional behavior.\nCase studies\nBut let us start more humbly by laying the empirical foundations for such discussions and test the observation by First\n, though, let us see what we can find out by looking at the overall distribution of types, using the four-million-word BNC Baby.\nOnce we remove all prefixes and standardize the spelling, there are 846 types for the two suffixes.\nThere is a clear overall preference for -ic (659 types) over -ical (187 types) (incidentally, there are only 54 stems that occur with both suffixes).\nFor stems with -(o)log-, the picture is drastically different: there is an overwhelming preference for -ical (55 types) over -ic As mentioned above, this could be due specifically to the affix -olog-, but it could also be a general preference of derived stems for -ical.\nIn order to determine this, we have to look at derived stems with other affixes.\nThere are a number of other affixes that occur frequently enough to make them potentially interesting, such as -ist-, as in statistic(al) (-ic vs. -ical: 74 /2), -graph-, as in geographic(al) ( 19 /8), or -et-, as in arithmetic(al) ( 32 /9).\nNote that all of them have more types with -ic, which suggests that derived stems in general, possibly due to their length, prefer -ic and that -olog-really is an exception.\nBut there is a methodological issue that we have to address before we can really conclude this.\nNote that we have been talking of a \"preference\" of particular 9 Morphology stems for one or the other suffix, but this is somewhat imprecise: we looked at the total number of stem types with -ic and -ical with and without additional suffixes.\nWhile differences in number are plausibly attributed to preferences, they may also be purely historical leftovers due to the specific history of the two suffixes (which is rather complex, involving borrowing from Latin, Greek and, in the case of -ical, French).\nMore convincing evidence for a productive difference in preferences would come from stems that take both -ic and -ical (such as electric/al, symmetric/al or numeric/al, to take three examples that display a relatively even distribution between the two): for these stems, there is obviously a choice, and we can investigate the influence of additional affixes on that choice.\nThe BNC Baby does not contain enough derived stems that occur with both suffix variants, so let us focus on two specific suffixes and extract the relevant data from the full BNC.\nSince -ist is roughly equal to -olog-in terms of type frequency, let us choose this suffix for comparison.\nTable\nThe different preferences of stems with -ist and -olog-are very obvious even from a purely visual inspection of the table: stems with the former occur at the top of the ranking, stems with the latter occur at the bottom and there is almost no overlap.\nThis is reflected clearly in the median ranks of the two stem types: the median for -ist is 4.5 (𝑁 = 8, rank sum = 38), the median for -olog-is 21.5 (𝑁 = 26, rank sum = 557).\nA Mann-Whitney U test shows that this difference is highly significant (𝑈 = 2, 𝑁 1 = 8, 𝑁 2 = 26, 𝑝 < 0.001).\nNow that we have established that different suffixes may, indeed, display different preferences for other suffixes (or suffix variants), we could begin to answer the question why this might be the case.\nIn this instance, the explanation is likely found in the complicated history of borrowings containing the suffixes in question.\nThe point of this case study was not to provide such an explanation but to show how an empirical basis can be provided using token frequencies derived from linguistic corpora.\nMorphemes and demographic variables\nThere are a few studies investigating the productivity of derivational morphemes across language varieties (e.g., medium or genre), across groups defined by sex, education and/or class, or across varieties.\nThis is an extremely interesting area of research that may offer valuable insights into the very nature of morphological richness and productivity, allowing us, for example, to study potential differences between regular, presumably subconscious applications of derivational rules and the deliberate coining of words.\nDespite this, it is an area that has not been studied too intensively, so there is much that remains to be discovered.\nCase study: Productivity and genre Guz\nThe suffix has a relatively high token frequency: there are 2862 tokens in the fiction section of the BNC, and 7189 tokens in the newspaper section (including all sub-genres of newspaper language, such as reportage, editorial, etc.) (the data are provided in the Supplementary Online Material, file LAF3).\nThis difference is not due to the respective sample sizes: the fiction section in the BNC is much larger than the newspaper section; thus, the difference token frequency would suggest that the suffix is more important in newspaper language than in fiction.\nHowever, as extensively discussed in Section 9.1.1, token frequency cannot be used to base such statements on.\nInstead, we need to look at the type-token ratio and the hapax-token ratio.\nTo get a first impression, consider Figure Both the TTR and the HTR suggest that the suffix is more productive in fiction: the ratios rise faster in fiction than in newspapers and remain consistently higher as we go through the two sub-corpora.\nIt is only when the tokens have been exhausted in the fiction subcorpus but not in the newspaper subcorpus, that the ratios in the latter slowly catch up.\nThis broadly supports our hypothesis, but let us look at the genre differences more closely both qualitatively and quantitatively.\nIn order to compare the two genres in terms of the type-token and hapaxtoken ratios, they need to have the same size.\nThe following discussion is based on the full data from the fiction subcorpus and a subsample of the newspaper corpus that was arrived at by deleting every second, then every third and finally every 192nd example, ensuring that the hits in the sample are spread through the entire newspaper subcorpus.\nLet us begin by looking at the types.\nOverall, there are 96 different types, 48 of which occur in both samples (some examples of types that frequent in both samples are relationship (the most frequent word in the fiction sample), championship (the most frequent word in the news sample), friendship, partnership, lordship, ownership and membership.\nIn addition, there are 36 types that occur only in the prose sample (for example, churchmanship, dreamership, librarianship and swordsmanship) and 12 that occur only in the newspaper sample (for example, associateship, draughtsmanship, trusteeship and sportsmanship).\nThe number of types exclusive to each genre suggests that the suffix is more important in fiction than in newspapers.\nThe TTR of the suffix in newspaper language is 60 /2862 = 0.021, and the HTR is 20 /2862 = 0.007.\nIn contrast, the TTR in fiction is 84 /2862 = 0.0294, and the HTR is 29 /2862 = 0.0101.\nAlthough the suffix, as expected, is generally not very productive, it is more productive in fiction than in newspapers.\nAs Table Let us now turn to the hapax legomena\n.\nThese are so rare in both genres that the difference in TTR is not statistically significant, as Table Case studies\nIt is not straightforwardly clear whether such cases should be treated as hapaxes.\nIf we think of the two samples as subsamples of the same corpus, it is very counterintuitive to do so.\nIt might be more reasonable to count only those words as hapaxes whose frequency in the combined subsamples is still one.\nHowever, the notion \"hapax\" is only an operational definition for neologisms, based on the hope that the number of hapaxes in a corpus (or sub-corpus) is somehow indicative of the number of productive coinages.\nWe saw in Case Study 9.2.1.1 that this is a somewhat vain hope, as the correlation between neologisms and hapaxes is not very impressive.\nStill, if we want to use this operational definition, we have to stick with it and define hapaxes strictly relative to whatever (sub-)corpus we are dealing with.\nIf we extend the criterion for hapax-ship beyond one subsample to the other, why stop there?\nWe might be even stricter and count only those words as hapaxes that are still hapaxes when we take the entire BNC into account.\nAnd if we take the entire BNC into account, we might as well count as hapaxes only those words that occur only once in all accessible archives of the language under investigation.\nThis would mean that the hapaxes in any sample would overwhelmingly cease to be hapaxes -the larger our corpus, the fewer hapaxes there will be.\nTo illustrate this: just two words from the fiction sample retain their status as hapax legomena if we search the Google Books collection: impress-ship, which does not occur at all (if we discount linguistic accounts which mention it, such as This case study has demonstrated the potential of using the TTR and the HTR not as a means of assessing morphological richness and productivity as such, but as a means of assessing genres with respect to their richness and productivity.\nIt has also demonstrated some of the problems of identifying hapax legomena in the context of such cross-variety comparisons.\nAs mentioned initially, there are not too many studies of this kind, but Case study:\nProductivity and speaker sex Morphological productivity has not traditionally been investigated from a sociolinguistic perspective, but a study by The BNC contains substantially more speech and writing by male speakers than by female speakers, which is reflected in differences in the number of affix tokens produced by men and women\n: for -ity, there are 2562 tokens produced by women and 8916 tokens produced by men; for -ness, there are 616 tokens produced by women and 1154 tokens produced by men (note that unlike Säily, I excluded the words business and witness, since they did not seem to me to be synchronically transparent instances of the affix).\nTo get samples of equal size for each affix, random subsamples were drawn from the tokens produced by men.\nBased on these subsamples, the type-token ratios for -ity are 0.0652 for men and 0.0777 for women; as Table Note that Säily investigates spoken and written language separately and she also includes social class in her analysis, so her results differ from the ones presented here; she finds a significantly lower HTR for -ness in lower-class women's speech in the spoken subcorpus, but not in the written one, and a significantly lower HTR for -ity in both subcorpora.\nThis might be due to the different methods used, or to the fact that I excluded business, which is disproportionally fre-9 Morphology quent in male speech and writing in the BNC and would thus reduce the diversity in the male sample substantially.\nHowever, the type-based differences do not have a very impressive effect size in our design and they are unstable across conditions in Säily's, so perhaps they are simply not very substantial.\nLet us turn to the HTR next.\nAs before, we are defining what counts as a hapax legomenon not with reference to the individual subsamples of male and female speech, but with respect to the combined sample.\nTable Case studies\nAlthough the difference in HTR is relatively small, Table As Table\nIn this case, the results correspond to Säily's, who also finds a significant difference in productivity for -ity, but not for -ness.\nThis case study was meant to demonstrate, once again, the method of comparing TTRs and HTRs based on samples of equal size.\nIt was also meant to draw attention to the fact that morphological productivity may be an interesting area of research for variationist sociolinguistics; however, it must be pointed out that it would be premature to conclude that men and women differ in their productive use of particular affixes; as Säily herself points out, men and women are not only represented unevenly in quantitative terms (with a much larger proportion of male language included in the BNC), but also in qualitative terms (the language varieties with which they are represented differ quite strikingly).\nThus, this may actually be another case of different degrees of productivity in different language varieties (which we investigated in the preceding case study).\nText\nAs mentioned repeatedly, linguistic corpora, by their nature, consist of word forms, while other levels of linguistic representation are not represented unless the corresponding annotations are added.\nIn written corpora, there is one level other than the lexical that is (or can be) directly represented: the text.\nWell-constructed linguistic corpora typically consist of (samples from) individual texts, whose meta-information (author, title, original place and context of publication, etc.) are known.\nThere is a substantial body of corpus-linguistic research based on designs that combine the two inherently represented variables Word (Form) and Text; such designs may be concerned with the occurrence of words in individual texts, or, more typically, with the occurrence of words in clusters of texts belonging to the same language variety (defined by topic, genre, function, etc.).\nTexts are, of course, produced by speakers, and depending on how much and what kind of information about these speakers is available, we can also cluster texts according to demographic variables such as dialect, socioeconomic status, gender, age, political or religious affiliation, etc. (as we have done in many of the examples in earlier chapters).\nIn these cases, quantitative corpus linguistics is essentially a variant of sociolinguistics, differing mainly in that the linguistic phenomena it pays most attention to are not necessarily those most central to sociolinguistic research in general.\nKeyword analysis\nIn the investigation of relationships between words (or other units of language structure) and texts (or clusters of texts), researchers frequently use a method referred to as keyword analysis.\nIn other words, the corpus-linguistic identification of keywords is analogous to the identification of differential collocates, except that it analyses the association of a word W to a particular text (or collection of texts) T in comparison to the language as a whole (as represented by the reference corpus, which is typically a large, balanced corpus).\nTable\nIf keyword analysis is applied to a single text, the aim is typically to identify either the topic area or some stylistic property of that text.\nWhen applied to text categories, the aim is typically to identify general lexical and/or grammatical properties of the language variety represented by the text categories.\nAs a first example of the kind of results that keyword analysis yields, consider Table\nAs we can see, the differences are relatively small, as all lists are dominated by frequent function words and punctuation marks.\nTen of these occur on all three lists\nApplying keyword analysis to each text or collection of texts allows us to identify the words that differ most significantly in frequency from the reference corpus, telling us how the text in question differs lexically from the (written) language of its time as a whole.\nTable\nThe keywords now convey a very specific idea of what the text is about: there are two proper names of rivers (the Neosho already seen on the frequency list and the Marais des Cygnes, represented by its constituents Cygnes, Marais and des), and there are a number of words for specific species of fish as well as the words river and channel.\nThe text is clearly about fish in the two rivers.\nThe occurrence of the words station and abundance suggests a research context, which is supported by the occurrence of two dates and opening and closing parentheses (which are often used in scientific texts to introduce references).\nThe text in question is indeed a scientific report on fish populations: Fish Populations, Following a Drought, In the Neosho and Marais des Cygnes Rivers of Kansas (available via Project Gutenberg and in the Supplementary Online Material, file TXQP).\nNote that the occurrence of some tokens (such as the dates and the parentheses) may be characteristic of a language variety rather than an individual text, a point we will return to below.\nNext, consider Table Again, the keywords identified are a mixture of topical markers and markers for the language variety (in this case, the genre) of the text, so even a study of the keywords of single texts provides information about more general linguistic properties of the text in question as well as its specific topic.\nBut keyword analysis reveals its true potential when we apply it to clusters of texts, as in the case studies in the next section.\nCase studies 10.2.1\nLanguage variety\nKeyword analysis has been applied to a wide range of language varieties defined by topic area (e.g. travel writing), genre (e.g. news reportage) or both (e.g. history textbooks) (see the contributions in Bondi & Scott 2010 for recent examples).\nHere, we will look at two case studies of scientific language.\nCase study: Keywords in scientific writing\nThere are a number of keyword-based analyses of academic writing (cf., for example, It is immediately obvious from the preponderance of scientific terminology that we are dealing with Scientific English -there are general scientific terms like fig\nIt may not be surprising that scientific terminology dominates in a corpus of Scientific English, but it demonstrates that keyword analysis works.\nGiven this, we can make some more profound observations on the basis of the list in Table 10.5.\nFor example, we observe that certain kinds of punctuation are typical of academic writing in general (such as the parentheses, which we already suspected based on the analysis of the fish population report in Section 10.1 above).\nEven more interestingly, keyword analysis can reveal function words that are characteristic for a particular language variety and thus give us potential insights into grammatical structures that may be typical for it; for example, is, the and of are among the most significant keywords of Scientific English.\nThe last two are presumably related to the nominal style that is known to characterize academic texts, while the higher-than-normal frequency of is may be due to the prevalence of definitions, statements of equivalence, etc.\nThis (and other observations made on the basis of keyword analysis) would of course have to be followed up by more detailed analyses of the function these words serve -but keyword analysis tells us what words are likely to be interesting to investigate.\nCase studies Case study: [a + __ + of] in Scientific English\nOf course, keyword analysis is not the only way to study lexical characteristics of language varieties.\nIn principle, any design studying the interaction of lexi-10 Text cal items with other units of linguistic structure can also be applied to specific language varieties.\nFor example, If we compare the result in Table However, the two lists -that in Table Table\nThe scientific vocabulary now dominates the collocates of the framework even more clearly than in the simple collocational framework analysis above: the in-10.2 Case studies formal a lot of and other colloquial words are now completely absent.\nThis case study shows the variability that even seemingly simple grammatical patterns may display across language varieties.\nIt is also meant to demonstrate how simple techniques like collocational-framework analysis can be combined with more sophisticated techniques to yield more insightful results.\nComparing speech communities\nAs pointed out at the beginning of this chapter, a keyword analysis of corpora that are defined by demographic variables is essentially a variant of variationist sociolinguistics.\nThe basic method remains the same, the only difference being that the corpora under investigation have to be constructed based on the variables in question, or, more typically, that existing corpora have to be separated into subcorpora accordingly.\nThis is true for inductive keyword analyses as well 10.2\nCase studies\nMany of the examples in the early chapters of this book demonstrate how, in principle, lexical differences between varieties can be investigated -take two sufficiently large corpora representing two different varieties, and study the distribution of a particular word across these two corpora.\nAlternatively, we can study the distribution of all words across the two corpora in the same way as we studied their distribution across texts or language varieties in the preceding section.\nThis was actually done fairly early, long before the invention of keyword analysis, by Comparing two (large) corpora representing two varieties will not, however, straightforwardly result in a list of dialect differences.\nInstead, there are at least five types of differences that such a comparison will uncover.\nNot all of them will be relevant to a particular research design, and some of them are fundamental problems for any research design and must be dealt with before we can proceed.\nTable\nFor someone hoping to uncover dialectal differences between British and American English, these lists are likely to be confusing, to say the least.\nThe hyphen is the strongest American keyword?\nQuotation marks are typical for British English?\nThe word The is typically American?\nClitics like n't, 's and 'm are British, while words containing these clitics, like didn't, it's and I'm are American?\nOf course not -all of these apparent differences between American and British English are actually differences in the way the two corpora were prepared.\nThe tagged version of the BROWN corpus does not contain quotation marks because they have intentionally been stripped from the text.\nThe with an uppercase T does not occur in the tagged LOB corpus, because case is normalized such that only proper names are capitalized.\nAnd clitics are separate tokens in LOB but not in BROWN.\nIn other words, the two corpora have to be made comparable before they can be compared.\nTable\nThis list is much more insightful.\nThere are still some artifacts of corpus construction: the codes F and J are used in BROWN to indicate that letter combinations and formulae have been removed.\nBut the remainder of the keywords is now representative of the kinds of differences a dialectal keyword analysis will typically uncover.\nFirst, there are differences in spelling.\nFor example, labour and behaviour are spelled with ou in Britain, but with o in the USA, the US-American defense is spelled defence in Britain, and the British programme is spelled program in the USA.\nThese differences are dialectal and may be of interest in applied contexts, but they are not likely to be of primary interest to most linguists.\nIn fact, they are often irritating, since of course we would like to know whether words like labo(u)r or behavio(u)r are more typical for British or for American English aside from the spelling differences.\nTo find out, we have to normalize spellings in the corpora before comparing them (which is possible, but labo(u)r-intensive).\nSecond, there are proper nouns that differ in frequency across corpora: for example, geographical names like London, Britain, Commonwealth, and (New) York will differ in frequency because their referents are of different degrees of interest to the speakers of the two varieties.\nThere are also personal names that differ across corpora; for example, the name Macmillan occurs 63 times in the LOB corpus but only once in BROWN; this is because in 1961, Harold Macmillan was the British Prime Minister and thus Brits had more reason to mention the name.\nBut there are also names that differ in frequency because they differ in popularity in the speech communities: for example, Mike is a keyword for BROWN, Michael for LOB.\nThus, proper names may differ in frequency for purely cultural or for linguistic reasons; the same is true of common nouns.\nThird, nouns may differ in frequency not because they are dialectal, but because the things they refer to play a different role in the respective cultures.\nState, for example, is a word found in both varieties, but it is more frequent in US-American English because the USA is organized into 50 states that play an important cultural and political role.\nFourth, nouns may differ in frequency due to dialectal differences (as we saw in many of the examples in previous chapters).\nTake toward and towards, which mean the same thing, but for which the first variant is preferred in US-American and the second in British English.\nOr take round, which is an adjective meaning 'shaped like a circle or a ball' in both varieties, but also an adverb with a range of related meanings that corresponds to American English around.\nThis case study was mainly intended to demonstrate the difficulty of comparing corpora that are not really comparable in terms of the way they have been constructed.\nIt was also meant to demonstrate how large-scale comparisons of varieties of a language can be done and what kind of results they yield.\nFrom a theoretical perspective, these results may seem to be of secondary interest, at least in the domain of lexis, since lexical differences between the major varieties of English are well documented.\nBut from a lexicographical perspective, largescale comparisons of varieties are useful, especially because dialectal differences are constantly evolving.\nCase study: British vs. American culture\nKeyword analysis of language varieties is often done not to uncover dialectal variation, but to identify cultural differences between speech communities.\nSuch studies have two nominal variables: Culture (operationalized as \"corpus containing language produced by members of the culture\") and Area of Life (operationalized as \"semantic field\").\nThey then investigate the importance of different areas of life for the cultures involved (where the importance of an area is operationalized as \"having a large number of words from the corresponding semantic field among the differential keywords\").\nThe earliest study of this kind is The authors inductively identify words pointing to cultural contrasts by discarding all words whose distribution across the two corpora is not significant, all proper names, and all words whose significant differences in distribution are due to dialectal variation (including spelling variation).\nNext, they look at concordances of the remaining words to determine first, which senses are most frequent and thus most relevant for the observed differences, and second, whether the words are actually distributed across the respective corpus, discarding those 10 Text whose overall frequency is simply due to their frequent occurrence in a single file (since those words would not tell us anything about cultural differences).\nFinally, they sort the words into semantic fields such as sport, travel and transport, business, mass media, military, etc., discussing the quantitative and qualitative differences for each semantic field.\nFor example, they note that there are obvious differences between the types of sports whose vocabulary differentiates between the two corpora (baseball is associated with the BROWN corpus, cricket and rugby with the LOB corpus), reflecting the importance of these sports in the two cultures, but also that general sports vocabulary (athletic, ball, playing, victory) is more often associated with the BROWN corpus, suggesting a greater overall importance of sports in 1961 US-American culture.\nExcept for one case, they do not present the results systematically.\nThey list lexical items they found to differentiate between the corpora, but it is unclear whether these lists are exhaustive or merely illustrative (the only drawback of this otherwise methodologically excellent study).\nThe one case where they do present a table is the semantic field military.\nTheir results are shown in Table\nThere are two things to note about this list.\nFirst, as (1) a. These remarkable ships and weapons, ranging the oceans, will be capable of accurate fire on targets virtually anywhere on earth.\n(BROWN G35) b. A trap for throwing these miniature clays fastens to the barrel so that the shooter can throw his own targets.\n(BROWN E10)\nSecond, the list is not exhaustive, listing only words which show a significant difference across the two varieties.\nFor example, the obvious items soldier and soldiers are missing because they are roughly equally frequent in the two varieties.\nHowever, if we want to make strong claims about the role of a particular domain of life (i.e., semantic field) in a culture, we need to take into consideration not just the words that show significant differences but also the ones that do not.\nIf there are many of the latter, this would weaken the results.\n10.2 Case studies two corpora\n.\nUnlike Leech and Fallon in the study described above, it seems that they did not create a complete list of differential keywords and then categorized them into semantic fields, but instead focused on words for kinship relations, spiritual entities and witchcraft straight away.\nThis procedure yields seemingly convincing word lists like that in Table Text\nWith respect to the latter, note that it is also questionable whether one can simply combine a British and an American corpus to represent \"Western\" culture.\nFirst, it assumes that the two cultures individually belong to such a larger culture and can jointly represent it.\nSecond, it assumes that these two cultures do not accord any specific importance to whatever domain we are looking at.\nHowever, especially if we choose our keywords selectively, we could easily show that Authority has a central place in British or American culture.\nTable\nThis case study demonstrates some of the potential pitfalls of cultural keyword analysis.\nThis is not to suggest that Co-occurrence of lexical items and demographic categories The potential overlap between keyword analysis and sociolinguistics becomes most obvious when using individual demographic variables such as sex, age, education, income, etc. as individual variables.\nNote that such variables may be nominal (sex), or ordinal (age, income, education); however, even potentially ordinal variables are treated as nominal in keyword-based studies, since keyword analysis cannot straightforwardly deal with ordinal data (although it could, in principle, be adapted to do so).\nCase study: A deductive approach to sex differences A thorough study of lexical differences between male and female speech is\nOne area in which this is unproblematic is color: Schmid finds that all basic color terms The semantic domain personal reference is slightly more difficult -it is large, lexically diverse and has no clear boundaries.\nSchmid operationalizes it in terms of the pronouns I, you, he, she, we, they and the relatively generic human nouns boy, girl, man, men, people, person, persons, woman, women (it is unclear why the sex-neutral child(ren) and the plurals boys and girls are missing).\nThis is not a bad selection, but it is clear that there are many other ways of referring to persons -proper names, kinship terms, professions, to name just a few.\nThere may be good reasons for excluding these, but doing so means that we are studying not the semantic field of personal reference as a whole, but a particular aspect of it.\nTable\nWithin the limitations just mentioned, it seems that a good case can be made that women's speech is characterized by a higher proportion of personal reference terms (at least if the corpus is well constructed, a point I will return to in the next subsection).\nThe words selected to represent this domain form a welldelineated set -stylistically neutral English nouns referring to people with no additional semantic content other than sex (the pronouns are even a closed set).\nThe only potential caveat is just that the words are stylistically neutral and that the results may thus reflect a tendency of women to use a more standard variety of the language.\nThus, we might want to look at synonyms for man, woman, boy, girl and child.\nTable Case studies\nMany other fields that Schmid investigates make it much more difficult to come up with a plausibly representative sample of words.\nFor example, for the domain health and body, Schmid looks at breast, hair, headache, legs, sore throat, doctor, sick, ill, leg, eyes, finger, fingers, eye, body, hands, and hand and finds that with the exception of hand they are all more frequently used by women.\nThe selection seems small and rather eclectic, however, so let us enlarge the set with the words ache, aching, flu, health, healthy, influenza, medicine, nurse, pain and unwell from the domain health and arms,\nNote that one difficulty with sociolinguistic research focusing on lexical items is that topical differences in the corpora may distort the picture.\nFor example, among the female keywords we find words like kitchen, baby, biscuits, husband, bedroom, and cooking which could be used to construct a stereotype of women's language as being home-and family-oriented.\nIn contrast, among the male keywords we find words like minus, plus, percent, equals, squared, decimal as well as many number words, which could be used to construct a stereotype of male language as being concerned with abstract domains like mathematics.\nHowever, these differences very obviously depend on the topics of the conversations included in the corpus.\nIt is not inconceivable, for example, that male linguists constructing a spoken corpus will record their male colleagues in a university setting and their female spouses in a home setting.\nThus, we must take care to distinguish stable, topic-independent differences from those that are due to the content of the corpora investigated.\nThis should be no surprise, of course, since keyword analysis was originally invented to uncover precisely such differences in content.\nIdeology\nJust as we can choose texts to stand for demographic variables, we can choose them to stand for the world views or ideologies of the speakers who produced them.\nNote that in this case, the texts serve as an operational definition of the corresponding ideology, an operationalization that must be plausibly justified.\nCase study: Political ideologies\nAs an example, consider Table\nObviously, the names of each party are overrepresented in the respective manifestos as compared to that of the other party.\nMore interesting is the fact that would is a keyword for the Liberal Democrats; this is because their manifesto 10.2 Case studies 10\nText mentions hypothetical events more frequently, which Rayson takes to mean that they did not expect to win the election.\nGoing beyond Rayson's discussion of individual words, note that the Labour manifesto does not have any words relating to specific policies among the ten strongest keywords, while the Liberal Democrats have green and environmental, pointing to their strong environmental focus, as well as powers, which, when we look at the actual manifesto, turns out to be due to the fact that they are very concerned with the distribution of decision-making powers.\nWhy\nmight this be the case?\nWe could hypothesize that since the Labour Party was already in power in 2001, they might have felt less of a need than the Liberal Democrats to mention specific policies that they were planning to implement.\nSupport for this hypothesis comes from the fact that the Liberal Democrats not only use the word would more frequently than Labour, but also the word will.\nIn order to test this hypothesis, we would have to look at a Labour election manifesto during an election in which they were not in power: the prediction would be that in such a situation, we would find words relating to specific policies.\nLet us take the 2017 election as a test case.\nThere are two ways in which we could now proceed: We could compare the Labour 2017 manifesto to the 2001 manifesto, or we could simply repeat Rayson's analysis and compare the 2017 manifestos of Labor and the Liberal Democrats.\nTo be safe, let us do both (again, the 2017 manifestos, converted into comparable form, are found in the Supplementary Online Material, file TXQP).\nTable\nThe results of both comparisons bear out the prediction: most of the significant keywords in the 2017 manifesto relate to specific policies.\nThe comparison with the Liberal Democrat manifesto highlights core Labour policies, with words like workers, unions, women and workplace.\nThe comparison with 2001 partially highlights the same areas, suggesting a return to such core policies between the 2001 \"New Labour\" era of Tony Blair and the 2017 \"radical left\" era of Jeremy Corbyn.\nThe comparison also highlights the topical dominance of the so-called Brexit (a plan for the UK to leave the European Union): this is reflected in the word Brexit itself, but likely also in words like ensure, protect and protections, and businesses, which refer to the economic consequences of the so-called Brexit.\nOf course, the fact that our prediction is borne out does not mean that the hypothesis about 10.2\nCase studies\nThis case study has demonstrated that keyword analysis can be used to investigate ideological differences through linguistic differences.\nIn such investigations, of course, identifying keywords is only the first step, to be followed by a closer analysis of how these keywords are used in context (cf.\nOne issue that needs consideration is whether in the context of a specific research design it is more appropriate to compare two texts potentially representing different ideologies directly to each other, as Rayson does, or whether it is more appropriate to compare each of the two texts to a large reference corpus, as the usual procedure in keyword analysis would be.\nIn the first case, the focus will necessarily be on differences, as similarities are removed from the analysis by virtue of the fact that they will not be statistically significant -we could call 10 Text this procedure differential keyword analysis.\nIn the second case, both similarities and differences could emerge; however, so would any vocabulary that is associated with the domain of politics in general.\nWhich strategy is more appropriate depends on the aims of our study.\nCase study: The importance of men and women\nJust as text may stand for something other than a text, words may stand for something other than words in a given research design.\nPerhaps most obviously, they may stand for their referents (or classes of referents).\nIf we are careful with our operational definitions, then, we may actually use corpus-linguistic methods to investigate not (only) the role of words in texts, but the role of their referents in a particular community.\nIn perhaps the first study attempting this, First, Kjellmer notes that overall, men are referred to much more frequently than women: There are 18 116 male pronouns in the LOB corpus compared to only 8366 female ones (Kjellmer's figures differ very slightly from the ones given here and below, perhaps because he used an earlier version of the corpus).\nThis difference between male and female pronouns is significant: using the singlevariable version of the 𝜒 2 test introduced in Chapter 6, and assuming that the population in 1961 consisted of 50 percent men and 50 percent women, we get the expected frequencies shown in Table Kjellmer's main question is whether, given this overall imbalance, there are differences in the individual text categories, and as Table Even taking into consideration the general overrepresentation of men in the corpus, they are overrepresented strongly in reportage and editorials, religious writing and belles-lettres/biographies -all \"factual\" language varieties, suggesting that actual, existing men are simply thought of as more worthy topics of discussion than actual, existing women.\nWomen, in contrast, are overrepresented in popular lore, general fiction, adventure and western, and romance and love stories (overrepresented, that is, compared to their general underrepresentation; in absolute numbers, they are mentioned less frequently in every single category except romance and love stories).\nIn other words, fictive women are slightly less strongly discriminated against in terms of their worthiness for discussion than are real women.\nOther researchers have taken up and expanded Kjellmer's method of using the distribution of male and female pronouns (and other gendered words) in corpora to assess the role of women in society (see, e.g. Time periods 10.2.5.1 Case study:\nVerbs\nin the going-to future\nJust as we can treat speech communities, demographic groups or ideologies as subcorpora which we can compare using keyword analysis or textually differential collexeme analysis, we can treat time periods as such subcorpora.\nThis allows us to track changes in vocabulary or in lexico-grammatical associations.\nThis procedure was first proposed by Let us take the English going-to future as an example and study potential changes in the verbs that it occurs with during the 18th and 19th centuries, i.e. in the centuries when, as shown in Case Study 8.2.5.3 in Chapter 8, it first grammaticalized and then rose drastically in terms of discourse frequency.\nFor Present-Day English, This case study is meant to demonstrate the use of collostructional analysis and keyword analysis, specifically, textual differential collexeme analysis, as a 10 Text method for diachronic linguistics.\nBoth approaches -separate analyses of successive periods or direct (pairwise or multinomial) comparisons of successive periods -can uncover changes in the association between grammatical constructions and lexical items, and thereby in the semantics of constructions.\nThe separate analysis of successive periods seems conceptually more straightforward (cf. Stefanowitsch 2006a for criticism of the direct comparison of historical periods), but the direct comparison can be useful in that it automatically discards similarities between periods and puts differences in sharp focus.\nCase study: Culture across time\nAs has become clear, a comparison of speech communities often results in a comparison of cultures, but of course culture can also be studied on the basis of a corpus without such a comparison.\nReferents that are important in a culture are more likely to be talked and written about than those that are not; thus, in a sufficiently large and representative corpus, the frequency of a linguistic item may be taken to represent the importance of its referent in the culture.\nThis is the basic logic behind a research tradition referred to as \"culturomics\", a word that is intended to mean something like \"rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities\"\nThe use of the Google Books archive may be criticized because it is not a balanced corpus, but the authors point out that first, it is the largest corpus available and second, books constitute cultural products and thus may not be such a bad choice for studying culture after all.\nThese are reasonable arguments, but if possible, it seems a good idea to complement any analysis done with Google Books with an analysis of a more rigorously constructed balanced corpus.\nAs a simple example, consider the search for the word God in the English part of the Google Books archive, covering the 19th and 20th centuries.\nI used the 2012 version of the corpus, so the result differs very slightly from theirs.\nI also repeated the analysis using the Corpus of Historical American English (COHA), which spans more or less the same period.\nClearly, the word God has decreased in frequency -dramatically so in the Google Books archive, slightly less dramatically so in COHA.\nThe question is what conclusions to draw from this.\nThe authors present it as an example of the \"history of religion\", concluding from their result somewhat flippantly that \"'God' is not dead but needs a new publicist\".\nThis flippancy, incidentally, signals an unwillingness to engage with their own results in any depth that is not entirely untypical of researchers in culturomics.\nBroadly speaking the result certainly suggests a waning dominance of religion on topic selection in book publishing (Google Books), and slightly less so in published texts in general (COHA).\nThis is not surprising to anyone who has paid attention for the last 200 years; more generally, it is not surprising that the rise and fall in importance of particular topics is reflected in the frequency of the vocabulary used to talk and write about these topics, but the point of this case study was mainly to demonstrate that the method works.\nWhile it is not implausible to analyze culture in general on the basis of a literary corpus, any analysis that involves the area of publishing itself will be particularly convincing.\nOne such example is the use of frequencies to identify periods of censorship in\nThe authors plausibly take this drastic drop in frequency as evidence of political censorship -Chagall's works, like those of other Jewish artists, were declared to be \"degenerate\" and confiscated from museums, and it makes sense that his name would not be mentioned in books written in Nazi Germany.\nHowever, the question is, again, what conclusions to draw from such an analysis.\nSpecifically, we know how to interpret the drop in frequency of the name Marc Chagall during the Nazi era in Germany because we know that Marc Chagall's works were banned.\nBut if we did not know this, we would not know how to interpret the change in frequency, since words, especially names, may rise or fall in frequency for all kinds of reasons.\nConsider the following figure, which shows the development of the frequency of the name Karl Marx in the German and English Google Books archive (extracted from the bigram files downloaded from the Google Books site, see Supplementary Online Material, file CUBF).\nNote the different frequency scalesthe name is generally much more frequent in German than in English, but what interests us are changes in frequency.\nAgain, we see a rise in frequency in the 1920s, and then a visible decrease during the Nazi era from 1933 to 1945.\nAgain, this can plausibly be seen as evidence Metaphor\nThe ease with which corpora are accessed via word forms is an advantage as long as it is our aim to investigate words, for example with respect to their relationship to other words, to their internal structure or to their distribution across grammatical structures and across texts and language varieties.\nAs we saw in Chapter 8, the difficulty of accessing corpora at levels of linguistic representation other than the word form is problematic where our aim is to investigate grammar in its own right, but since grammatical structures tend to be associated with particular words and/or morphemes, these difficulties can be overcome to some extent.\nWhen it comes to investigating phenomena that are not lexical in nature, the word-based nature of corpora is clearly a disadvantage and it may seem as though there is no alternative to a careful manual search and/or a sophisticated annotation (manual, semi-manual or based on advanced natural-language technology).\nHowever, corpus linguists have actually uncovered a number of relationships between words and linguistic phenomena beyond lexicon and grammar without making use of such annotations.\nIn the final chapter of this book, we will discuss a number of case studies of one such phenomenon: metaphor.\nStudying metaphor in corpora Metaphor is traditionally defined as the transfer of a word from one referent (variously called vehicle, figure or source) to another (the tenor, ground or target) (cf. e.g. Aristotle, Poetics, XXI).\nIf metaphor were indeed located at the word level, it should be straightforwardly amenable to corpus-linguistic analysis.\nUnfortunately, things are slightly more complicated.\nFirst, the transfer does not typically concern individual words but entire semantic fields (or even conceptual domains, according to some theories).\nSecond, as discussed in some detail in Chapter 4, there is nothing in the word itself that distinguishes its literal and metaphorical uses.\nOne way around this problem is manual annotation, and there are very detailed and sophisticated proposals for annotation procedures (most notably the Pragglejaz Metaphor Identification Procedure, cf., for example,\nHowever, as stressed in various places throughout this book, the manual annotation of corpora severely limits the amount of data that can be included in a research design; this does not invalidate manual annotation, but it makes alternatives highly desirable.\nTwo broad alternatives have been proposed in corpus linguistics.\nSince these were discussed in some detail in Chapter 4, we will only repeat them briefly here before illustrating them in more detail in the case studies.\nCase studies\nThe first approach to extracting metaphors from corpora starts from a source domain, searching for individual words or sets of words (synonym sets, semantic fields, discourse domains) and then identifying the metaphorical uses and the respective targets and underlying metaphors manually.\nThis approach is extensively demonstrated, for example, in Source domains\nAmong other things, the corpus-based study of (small set of) source domain words may provide insights into the systematicity of metaphor (cf. esp. Deignan 1999b).\nIn cognitive linguistics, it is claimed that metaphor is fundamentally a mapping from one conceptual domain to another, and that metaphorical expressions are essentially a reflex of such mappings.\nThis suggests a high degree of isomorphism between literal and metaphorical language: words should essentially display the same systemic and usage-based behavior when they are used as the source domain of a metaphor as when they are used in their literal sense unless 11.2 Case studies there is a specific reason in the semantics of the target domain that precludes this\nLet us replicate Deignans study using the BNC Baby.\nTo keep other factors equal, let us focus on attributive uses of adjectives that modify target domain nouns (as in cold facts), or nouns that are themselves used metaphorically (as in \"The project went into cold storage\" meaning work on it ceased).\nDeignan focuses on the base forms of these adjectives, let us do the same.\nShe also excludes \"highly fixed collocations and idioms\" because their potential metaphorical origin may no longer be transparent -let us not follow her here, as we can always identify and discuss such cases after we have extracted and tabulated our data.\nDeignan does not explicitly present an annotation scheme, but she presents dictionary-like definitions of her categories and extensive examples of her categorization decisions that, taken together, serve the same function.\nHer categories differ in number (between four and ten) and semantic granularity across the four words, let us design a stricter annotation scheme with a minimal number of categories.\nLet us assume that survey of the dictionaries already used in Case Study 9.2.1.2 yields the following major metaphor categories: 1. activity, with the metaphors high activity is heat and low activity is coldness, as in cold/hot war, hot pursuit, hot topic, etc.).\nThis sense is not recognized by the dictionaries, except insofar as it is implicit in the definitions of cold war, hot pursuit, cold trail, etc.\nIt is understood here to include a sense of hot described in dictionaries as \"currently popular\" or \"of immediate interest\" (e.g. hot topic).\n2. affection, with the metaphors affection is heat and indifference is coldness, as in cold stare, warm welcome, etc.\nThis sense is recognized by all dictionaries, but we will interpret it to include sense connected with sexual attraction and (un)responsiveness, e.g. hot date.\n11 Metaphor 3. temperament, with the metaphors emotional behavior is heat and rational behavior is coldness, as in cool head, cold facts, hot temper, etc.\nMost dictionaries recognize this sense as distinct from the previous oneboth are concerned with emotion or its absence, but in case of the affection, the distinction is one between affectionate feelings and their absense, in the case of temperament the distinction is one between behavior based on any emotion and behavior unaffected by emotion.\n4. synesthesia, a category covering uses described in dictionaries as \"conveying or producing the impression of being hot, cold, etc. \" in some sensory domain other than temperature, i.e. warm color, cold light, cool voice, etc. 5.\nevaluation, with the (potential) metaphor positive things have a temperature, as in a really cool movie, a cool person, a hot new idea, etc.\nThis may not be a metaphor at all, as both uses are very idiomatic; in fact, hot in this sense could be included under activity or affection, and cool in this sense is presumably derived from temperament.\nTable\nThe category activity is instantiated only for the words cold and hot and its absence for the other two words is significant.\nWe can imagine (and, in a sufficiently large data set, find) uses for cool and warm that would fall into this category.\nFor example, Frederick Pohl's 1981 novel The Cool War describes a geopolitical situation in which political allies sabotage each other's economies, and it is occasionally used to refer to real-life situations as well.\nBut this seems to be a deliberate analogy rather than a systematic use, leaving us with an unexpected gap in the middle of the linguistic scale between hot and cold.\nThe category affection is found with three of the four words, but its absence for the word cool is statistically significant, as is its clear overrepresentation with warm.\nThis lack of systematicity is even more unexpected than the one observed with activity: for the latter, we could argue that it reflects a binary distinction that uses only the extremes of the scale, for example because there is not enough of a potential conceptual difference between a cold war and a cool war.\nWith 11.2 Case studies\nWith temperament, we find a partially complementary situation: again, three of the four words occur with this metaphor, including, again, the extreme points.\nHowever, in this case it is cool that is significantly overrepresented and warm that is significantly absent.\nA possible explanation would be that there is a potential for confusion between the metaphors affection is temperature and temperament is temperature, and so speakers divide up the continuum from cold to hot between them.\nHowever, this does not explain why cold is frequently used in both metaphorical senses.\nThe gaps in the last category, evaluation, are less confusing.\nAs mentioned above, this is probably not a single coherent category and we would not expect uses to be equally disributed across the four words.\nThis case study demonstrates the use of corpus data to evaluate claims about conceptual structure.\nSpecifically, it shows how a central claim of conceptual metaphor theory can be investigated (see the much more detailed discussion in Case study: Word forms in metaphorical mappings\nAnother area in which we might expect a large degree of isomorphism between literal and metaphorical uses of a word is the quantitative and qualitative distribution of word forms.\nIn a highly intriguing study, Her study is generally deductive in that she starts with an expectation (if not quite a full-fledged hypothesis) that there are frequently differences between the singular and the plural forms of a metaphorically used word with respect to connotation.\nA cursory look at a few relatively randomly selected examples appears to corroborate this expression.\nMore precisely, it seems that the singular form flame has positive connotations more frequently than expected (cf. 1), while the plural form flames has negative connotations more frequently than expected (cf. 2): (1) a. Deignan studies this potential difference systematically based on a sample of more than 1500 hits for flame/s in the Bank of English (a proprietary, non-accessible corpus owned by HarperCollins), from which she manually extracts all 153 metaphorical uses.\nThese are then categorized according to their connotation.\nDeignan's design thus has two nominal variables: Word Form of Flame (with the variables singular and plural) and Connotation of Metaphor (with the values positive and negative.\nShe does not provide an annotation scheme for categorizing the metaphorical expressions, but she provides a set of examples that are intuitively quite plausible.\nTable\nThis explanation itself is of course a hypothesis about the literal use of singular and plural flame that must be tested separately.\nDeignan does not provide such a test, so let us do it here.\nLet us select a sample of 20 hits each for literal uses the singular and plural of flame(s) from the BNC (as mentioned above, Deignan's corpus is not accessible, so we must hope that the BNC is roughly comparable).\nCase studies\nIt is difficult to determine which hits should be categorized as positive and which as negative.\nLet us assume that any unwanted and/or destructive fire should be characterized as negative, and, on this basis, categorize lines\nThus, Deignan's explanation appears to be generally correct, providing evidence for a substantial degree of isomorphism between literal and figurative uses of (at least some) words.\nAn analysis of more such cases could show whether this isomorphism between literal and metaphorical uses is a general principle (as the conceptual theory of metaphor as Metaphor\nThis case study demonstrates first, how to approach the study of metaphor starting from source-domain words, and, second, that such an approach may be applied not just descriptively, but in the context of answering fundamental questions about the nature of metaphor.\nCase study: The impact of metaphorical expressions\nA slightly different example of a source-domain oriented study is found in Stefanowitsch (3) a.\nThe studies are deductive in that they aim to test the hypothesis that metaphorical language serves a cognitive function and that for each pair of patterns investigated, the metaphorical variant should be used with nouns referring to more complex entities.\nThe construct complexity is operationalized in the form of axioms derived from gestalt psychology, such as the following:\nConcepts representing entities that have a simple shape and/or have a clear boundary are less complex than those representing entities with complex shapes or fuzzy boundaries (because they are more easily delineable).\nThis follows from the gestalt principles of closure and simplicity Case studies\nFor each pair of expressions, the differential collexemes are identified and the resulting lists are compared against these axiomatic assumptions.\nLet us illustrate this using the pattern the dawn/beginning of NP.\nA case insensitive query for the string dawn or beginning, followed by of, followed by up to three words that are not a noun, followed by a noun yields the results shown in Table\nIn a short discussion of this study, Since search engine frequency data are notoriously unreliable, let us replicate this observation in a large corpus, the 400+ million word Corpus of Contemporary American English (COCA).\nThe names of decades (such as 1960s or sixties) occur too infrequently with dawn of in this corpus to say anything useful about them, but the names of centuries are frequent enough for a differential collexeme analysis.\nTable\nThere are clear differences between the centuries associated with dawn and those associated with beginning: the literal expression is associated with the past (nineteenth, seventeenth (just below significance)), while the metaphorical expression, as already observed by Liberman, is associated with the twenty-first century, i.e., the future (the expressions a new, our new and the incoming also support this).\nI would argue that this does point to a difference in boundedness and duration.\nWhile all centuries are objectively speaking, of the same length and have the same clear boundaries, it seems reasonable to assume that the past feels more bounded than the future because it is actually over, and we can imagine it in its entirety.\nIn contrast, none of the speakers in the COCA will live to see the end of the 21st century, making it conceptually less bounded to them.\nIf this is true, then we should be able to observe the same effect in the past: When the twentieth century was still the future, it, too, should have been associated with the metaphorical dawn of.\nLet us test this hypothesis using the Corpus of Historical American English, which includes language from the early nineteenth to the very early twenty-first century -in a large part of the corpus, the twentieth century was thus entirely or partly in the future.\nTable Case studies\nEven if we agree with this conclusion in general, however, it does not preclude a more literary, rhetorical function for metaphor in addition: while some of the expression pairs investigated in It is very obvious that the metaphorical expression the dawn of is significantly overrepresented in the text category fiction and underrepresented in the text categories academic and spoken, corroborating the intuition about the literaryness of the expression.\nWithin this text category, of course, it may well have the cognitive function attributed to it in\nThis case study demonstrates use of the differential collexeme analysis (and thus of collocational methods in general) that goes beyond associations between words and other elements of structure and instead uses words and grammatical patterns as ways of investigating semantic associations.\nDirect comparisons of literal and metaphorical language are rare in the research literature, so this remains a potentially interesting field of research.\nThe study also demonstrates that the distribution of particular metaphorical expressions across varieties, which can easily be determined in corpora that contain the relevant metadata, may shed light on the function of those expressions (and of metaphor in general).\nTarget domains\nAs discussed in Chapter 4, there are two types of metaphorical utterances: those that could be interpreted literally in their entirety (like the example I am burned up from A metaphorical pattern is a multi-word expression from a given source domain (SD) into which one or more specific lexical item from a given target domain (TD) have been inserted Case studies\nA metaphorical pattern analysis of a given target domain (like 'anger') thus proceeds by selecting one or more words that refer to (or are inherently connected with) this domain (for example, the word anger, or the set irritation, annoyance, anger, rage, fury, etc.) and retrieve all instances of this word or set of words from a corpus.\nThe next step consists in identifying all cases where the search term(s) occur in a multi-word expression referring to some domain other than emotions.\nFinally, the source domains of these expressions are identified, 11 Metaphor giving us the metaphor instantiated by each metaphorical pattern.\nThe patterns can then be grouped into larger sets corresponding to metaphors like \"emotions are substances\".\n11.2.2.1 Case study: Happiness across cultures\nLet us attempt to do this, focusing on the two metaphors just mentioned but discussing others in passing.\nIn order to introduce the method of metaphorical pattern analysis, let us limit the study to small samples of language, which will allow us to study the relevant concordances in detail.\nThis will make it less probable that we will find statistically significant differences, so let us treat the following as an exploratory pilot study.\nGiven how frequently we have compared British and American English in this book, these two varieties may seem an obvious place to start, but the two cultures may be too similar, and the word happiness happens to be too infrequent in the BROWN corpus anyway.\nLet us compare British English (the LOB corpus) and Indian English (the Kolhapur corpus constructed along the same categories) instead. .\nthough he never expected to attain the [happiness] he yearned for in a daughter-in-law and West again .\nBarry had brought her more [happiness] than she had ever known was possible , ut there 'll be sons for you -aye , and [happiness] , too -when Helen 's gone from your si y known before that there was no hope of [happiness] in the future for her and Gavin .\nif he is own love for her , his desire for her The transfer metaphor is also instantiated a number of times in the concordance, namely as [NP STIM bring NP EMOT ] (lines 16 and 42), and [NP STIM provide NP EMOT ] (line 20).\nAdditional clear cases of metaphorical patterns are [glow of NP EMOT ] (lines 28 and 53) and [warm NP EMOT ] (line 47), which instantiate the metaphor happiness is warmth, and [NP EMOT drain out of NP EXP 's face] (line 40), which instantiates happiness is a liquid filling the experiencer.\nIn other cases, it depends on our judgment (which we have to defend within a given research design) whether a hit constitutes a metaphorical pattern.\nFor example, do we want to analyze [NP STIM 's NP EMOT ] (lines 23 and 43) and [PRON.POSS. STIM NP EMOT ] (lines 37, 39, 45, 50) as happiness is a possessed object, or do we consider the possessive construction to be too abstract semantically to be analyzed as metaphorical?\nSimilarly, do we analyze [NP STIM engender NP EMOT ] (line 2) as an instance of happiness is an organism, based on the etymology of engender, which comes from Latin generare beget and was still used for organisms in Middle English (cf. Chaucer's ...swich licour, / Of which vertu engendred is the flour)?\nYou might want to think about these and other cases in the concordance, to get a sense of the kind of annotation scheme you would need to make such decisions on a principled, replicable basis.\nFor now, let us turn to Indian English.\nTable\nAgain, the search metaphor is instantiated several times in the concordance: we find [search for NP EMOT ] (lines 2 and 4) and [NP EXP seek NP EMOT ] (lines 3 and 5).\nThey all seem to be from the same text, so similar considerations apply\nAgain, there are other clear cases of metaphor, such as [NP EXP burst with NP EMOT ] (line 30), [NP EMOT flood NP EXP 's heart] (line 31), and [NP EXP be filled with NP EMOT ] (line 32) (again, they seem to be from the same text).\nComparing the metaphors we set out to investigate, we see that the pursuit and search metaphors are fairly evenly distributed across the two varieties, with no significant difference even on the horizon.\nThe transfer metaphor, in contrast, shows clear differences, and as Table Case studies Case study: Intensity of emotions\nWhether we start from the source domain or from the target domain, the extraction of metaphorical patterns from large corpora typically requires time-consuming manual annotation.\nHowever, if we are interested in specific metaphors, we can speed up the extraction significantly, by looking for utterances containing vocabulary from the source and target domains we are interested in.\nFor example, We can take this basic idea and apply it in a linguistically slightly more conservative way to target-item oriented versions of metaphorical pattern analysis.\nInstead of searching for co-occurrence in a span, let us construct a set of structured queries that would find metaphorical patterns instantiating a given metaphor.\nAs mentioned in Case Study 11.2.2.1, the word happiness and its German translation equivalent Glück may differ in the intensity of the emotion they refer to.\nIn Let us investigate these metaphors with respect to frequent basic emotion terms in English, to see whether there are general differences between emotions with respect to metaphorical intensity.\n(5) a.\nThe paper also lists a number of metaphorical patterns that describe an increasing pressure, e.g. [NP EMOT build inside NP EXP ] or an overflowing, e.g. [NP EXP brim over with NP EMOT ], but let us focus on those patterns that describe a sudden failure to contain the substance.\nThese are listed in ( Let us apply both sets of queries to the basic emotion nouns anger, desire, disgust, fear, happiness, pride, sadness, shame.\nTable\nThere are two emotion nouns whose frequency in these metaphorical patterns deviates from the expected frequency: desire is described as a substance in a container less frequently than expected, and sadness is more frequently than expected.\nIt is an interesting question why this should be the case, but it is not plausibly related to the intensity of the respective emotions.\nTable\nTwo emotion nouns occur with the bursting metaphors noticeably more frequently than expected, namely anger and pride (the latter marginally significantly); three occur noticeably less frequently, desire, fear and shame.\nAgain, this does not seem to be related to the intensity of the emotion -fear or desire can be just as intense as anger or pride.\nInstead, it seems to be related to the likelihood that the emotion will be outwardly visible, for example, by resulting in a particular kind of behavior toward others.\nAgain, desire is an exception, it may be that it does not participate in container metaphors at all.\nThis case study demonstrates how central metaphors for a given target domain can be identified by searching for combinations of words describing (aspects of) the source and the target domain in question.\nIt also shows that these metaphors can be associated to different degrees with different words within a given target domain (cf. e.g. 11 Metaphor\nIf we are interested in questions relating to a particular source domain, as in Section 11.2.1, or to a particular target domain, as in Section 11.2.2, our first task is to define a representative set of lexical items to query.\nThis set may be dictated by the hypothesis we are planning to test, or we may, in more exploratory studies, assemble it on the basis of thesauri or words and patterns identified in previous studies.\nIf, on the other hand, we are interested in source domains associated with a particular target domain, matters are more complicated.\nWe can start by selecting a set of target-domain items and then identify all source domains in the metaphorical patterns of these items, but while this will tell us something about these items, it does not tell us much about the target domain as a whole.\nOr we can identify the source domains manually, which restricts the amount of data we can reasonably process.\nTo demonstrate this method, let us define a subcorpus for the domain economy in the BNC Baby.\nThis is easiest done by using the meta-information supplied by the corpus makers, which includes the category \"commerce\" as a subcategory of \"newspaper\" (cf. Table\nAs expected, most of the strongest keywords for the subcorpus are directly related to the domain of economics.\nAmong the Top 15 (shown in the first part of the table), only two are not directly related to this domain: the proper name Middlesbrough and the word rise.\nThe keyness of the former is due to the fact that one of the files in the BNC Baby commercial subcorpus is from the Northern Echo, a regional newspaper covering County Durham and Teesside -Middlesbrough is the largest town in this region and is thus mentioned frequently, but it is not generally an important town, so it is hardly mentioned outside of this text.\nThe keyness of the latter is more interesting, as it is the kind of word we are looking for: its literal meaning, 'motion from a lower to a higher position', would 11.2 Case studies\nThis suggests that 'vertical motion' is a central source domain in the domain of economics, which we can now study by querying the respective keywords as well as other words from this domain (which we could get from a thesaurus).\nAs an example, consider the concordance of the lemma rise in Figure\nThe results of such a keyword analysis can now be used as a basis for all kinds of studies.\nFor example, we may simply be interested in describing frequent metaphorical patterns in the data (say, in the context of teaching English for Special Purposes); some very noticeable examples are [rise N in NP] (lines 2, 7, 16) and [see NP rise V ] (lines 8, 13), or [hold a rise N ] (line 18).\nOr we may be interested in the kind of research question discussed in Case Study 11.2.1.1, i.e. in whether literal synonyms and antonyms of rise are mapped isomorphically onto the domain of economics (the fact that jump, surge and soar as well as fall, slump and plunge are among the top 200 keywords certainly suggests they are.\nOr we may be interested in the kind of research question discussed in Case Study 11.\nThis case study demonstrates that central metaphors for a given target domain can be identified by applying a keyword analysis to a specialized corpus of texts from that domain.\nThe case study does not discuss a particular research question, but obviously, the method is useful in the context of many different research designs.\nOf course, it requires specialized corpora for the target domain under investigation.\nSuch corpora are not available (and in some cases not imaginable) for all target domains, so the method works better for some target domains (such as economics) than for others (like emotions).\nCase study: Metaphoricity signals\nAlthough metaphorical expressions are pervasive in all language varieties and speakers do not generally seem to draw special attention to their occurrence, 11 Metaphor there is a wide range of devices that mark non-literal language more or less explicitly (as in metaphorically/figuratively speaking, picture NP as NP, so to speak/ say): (9) a. ...the princess held a gun to Charles's head, figuratively speaking...\n(BNC CBF) b. He pictures eternity as a filthy Russian bathhouse...\n(BNC A18) c. ...the only way they can deal with crime is to fight fire, so to speak, with fire.\n(BNC ABJ)\nMore restricted case studies are needed to determine whether the idea of metaphoricity signals is, in principle, plausible.\nLet us look at what is intuitively the clearest case of such a signal on Wallington et al. 's list: the sentence adverbials metaphorically speaking and figuratively speaking.\nAs a control, let us use the roughly equally frequent sentence adverbial technically speaking, which does not signal metaphoricity but which can, of course, co-occur with (conventionalized) metaphors and which can thus serve as a baseline.\nThere are 22 cases of technically speaking in the BNC:\n(10) a. Do you mind if, technically speaking, I resign rather than you sack me?\n(BNC A0F) b.\nTechnically speaking as long as nobody was hurt, no injuries, no damage to the other vehicle, this is not an accident.\n(BNC A5Y)\nc. 11 Metaphor\nTaking a generous view, four of these are part of a clause that arguably contains a metaphor:\n(10f) uses hold as part of the phrase hold liable, instantiating a metaphor like \"believing something about someone is holding them\" (cf. also hold s.o. responsible/accountable, hold in high esteem); (10h) uses the verb evolve metaphorically to refer to a non-evolutionary development and then uses the spatial expressions towards and opposite direction metaphorically to describe the quality of the development; (10r) uses provide as part of the phrase provide employment, which instantiates a metaphor like \"causing someone to be in a state is transferring an object to them\" (cf. also provide s.o. with an opportunity/insight/ power...); (10t) contains the spatial preposition off as part of the phrase off duty, which could be said to instantiate the metaphor \"a situation is a location\".\nNote that all four expressions involve highly conventionalized metaphors, that would hardly be noticed as such by speakers.\nThere are 7 hits for the sentence adverbial metaphorically speaking in the BNC:\n(BNC KM7)\nIn clear contrast to technically speaking, six of these seven hits occur in clauses that contain a metaphor:\nbloody the nose of sb in (11a) means 'be successful in court against sb', instantiating the metaphor legal fight is physical fight; take 11.2 Case studies it on the chin in (11c) means 'endure being criticized', instantiating the metaphor argument is physical fight; make a killing in (11d) means 'be financially successful', instantiating the metaphor commercial activity is a hunt; a frozen moment in time in (11e) means 'documentation of a particular state', instantiating the metaphor time is a flowing body of water\n; put the boot in in (11f) means 'treat sb cruelly', instantiating the metaphor life (or sports) is physical fight; burst into song in (11g) means \"take one's turn speaking\", instantiating the metaphor speaking is singing.\nThe only exception is (11b); this is a metalinguistic use, indicating that someone did not understand that an utterance was meant metaphorically, rather than marking an utterance as metaphorical.\nThere are 13 hits for figuratively speaking in the BNC:\n(12) a.\nThe darts, the lumps of poison and the raw materials from which it is extracted all provide a challenge for others with a taste (figuratively speaking) for excitement.\n(BNC AC9)\nb.\nAlternatively, you could select spiky, upright plants like agaves or yuccas to transport you across the world, figuratively speaking, to the great deserts of North America.\n(BNC ACX)\nc.\nPalladium, statue of the goddess Pallas (Minerva) at Troy on which the city's safety was said to depend, hence, figuratively speaking, the Bar seen as a bulwark of society.\n(BNC B0Y)\nd.\nFiguratively speaking, who would not give their right arm to find such a love?\n(BNC B21)\ne.\n[I]t is surprising to me that this process was ever permitted on this site at all (being figuratively speaking within arms length of the dwellings).\n(BNC B2D) f.\nFiguratively speaking, we also make the law of value serve our aims.\n(BNC BMA) g.\nThis schlocky international movie, photographed in eye-straining colour, cashing in (figuratively speaking) on the craze for James Bond pictures [.\nHere, we would expect to find not just metaphors but also other kinds of nonliteral language -which we do in almost all cases.\nThe one exception is (12i), where the context (even enlarged beyond what is shown here) does not contain anything that could be a metaphor (it might be a metalinguistic use, indicating that the person called Tracey has been speaking figuratively).\nAll other cases are clearly figurative: a taste for excitement in (12a) means 'an experience', instantiating the metaphor experience is taste; transport sb. across the world in (12b) means 'make sb think of a distant location', instantiating the metaphor imaginary distance is physical distance, bulwark of society in (12c) means 'defender of society', instantiating the metaphor defense is a wall, give one's right arm to do sth in (12d) means 'want sth very much', instantiating the metonymy body part for personal value; be within arm's length in (12e) means 'be in close proximity', instantiating the metonymy arm's length for short distance; make sth serve one's aims in (12f) means 'put sth to use in achieving sth', instantiating the metaphor to be used is to serve; cash in in (12g) means 'be successful', instantiating the metaphor life is commercial transaction; hold gun to sb's head in (12h) means 'coerce sb to act', instantiating the metaphor power is physical force; (12j) is from a speech by the Soviet head of state Nikita Khrushchev in which he uses artillery to refer metonymically about nuclear missiles; you told me in (12k) means 'your co-employee told me', instantiating the metonymy employee for company; the superego is soluble in alcohol in (12l) means 'self-control disappears when drunk ', instantiating the metaphor character is a physical substance; balance the books in (12m) means 'make sure debits and credits match', instantiating the metaphor abstract entities are physical entities.\nWe can now compare the literal and metaphorical contexts in which the expressions technically speaking and metaphorically/figuraltively speaking occur.\nIf 11.2 Case studies the latter are a metaphoricity signal, they should occur significantly more frequently in metaphorical contexts than the former.\nTable\nThis case study found a clear effect where the authors of the study it is based on did not.\nThis demonstrates the need to formulate specific predictions concerning the behavior of specific linguistic items in such a way that they can be tested systematically and the results be evaluated statistically.\nThe study also shows that the area of metaphoricity signals is worthy of further investigation.\nCase study: Metaphor and ideology\nRegardless of whether metaphor is a rhetorical device (as has traditionally been assumed) or a cognitive device (as seems to be the majority view today), it is clear 11 Metaphor that it can serve an ideological function, allowing authors suggest a particular perspective on a given topic.\nThus, an analysis of the metaphors used in texts manifesting a particular ideology should allow us to uncover those perspectives.\nFor example, Charteris-Black (2005) investigates a corpus of \"right-wing communication and media reporting\" on immigration, containing speeches, political manifestos and articles from the conservative newspapers Daily Mail and Daily Telegraph.\nHe finds, among other things, that the metaphor \"immigration is a flood\" is used heavily, arguing that this allows the right to portray immigration as a disaster that must be contained, citing examples like a flood of refugees, the tide of immigration, and the trickle of applicants has become a flood.\nCharteris-Black's findings are intriguing, but since he does not compare the findings from his corpus of right-wing materials to a neutral or a corresponding left-wing corpus, it remains an open question whether the use of these metaphors indicates a specifically right-wing perspective on immigration.\nLet us therefore replicate his analysis more systematically.\nThe BNC contains 1 232 966 words from the Daily Telegraph (all files whose names begin with AH, AJ and AK), which will serve as our right-wing corpus, and 918 159 words from the Guardian (all files whose names begin with A8, A9 or AA, except file AAY), which will serve as our corresponding left-wing (or at least left-leaning) corpus.\nSince Charteris-Black's examples all involve reference to target-domain items such as refugee and immigration, a metaphorical pattern analysis (cf. Section 11.2.2 above) suggests itself.\nFigure\nIn terms of absolute frequencies, there is no great difference between the two subcorpora\nThis case study demonstrates that even general metaphors such as \"immigration is a mass of water\" may be associated with particular political ideologies.\nThere is a large research literature on the role of metaphor in political discourse (see, for example, (13) a. The ham sandwich is waiting for his check b. Nixon bombed Hanoi.\nIn (13a), the metonym ham sandwich stands for the target expression 'the person who ordered the ham sandwich', in (13b) the metonym Nixon stands for the target expression 'the air-force pilots controlled by Nixon' (at least at first glance).\nCase studies\nThus, metonymy differs from metaphor in that it does not mix vocabulary from two domains, which has consequences for a transfer of the methods introduced for the study of metaphor in Section 11.1.\nThe source-domain oriented approach can be transferred relatively straightforwardly -we can query an item (or set of items) that we suspect may be used as metonyms then identify the actual metonymic uses.\nThe main difficulty with this approach is choosing promising items for investigation.\nFor example, the word sandwich occurs almost 900 times in the BNC, but unless I have overlooked one, it is not used as a metonym even once.\nA straightforward analogue to the target-domain oriented approach (i.e., metaphorical pattern analysis) is more difficult to devise, as metonymies do not combine vocabulary from different semantic domains.\nOne possibility would be to search for verbs that we know or suspect to be used with metonymic subjects and/or objects.\nFor example, a Google search for ⟨\"is waiting for (his|her| their|the) check\"⟩ turns up about 20 unique hits; most of these have people as subjects and none of them have meals as subjects, but there are three cases that have table as subject, as in ( Querying the BNC for ⟨[pos=\".*NN.*\"] [lemma=\"bomb\" & pos=\".*VB.*\"]⟩ yields 31 hits referring to the dropping of bombs.\nOf these, only a single one has the ultimate decision maker as a subject (cf. 15a).\nSomewhat more frequent in subject position are countries or inhabitants of countries (5 cases) (cf. 15b, c).\nEven more frequently, the organization responsible for carrying out the bombing -e.g. an air force, or part of an air force -is chosen as the subject (9 cases) (cf. 15d, e).\nThe most frequent case (14 hits) mentions the aircraft carrying the bombs in subject position, often accompanied by an adjective referring to the country whose military operates the planes (cf 15f) or some other responsible group (cf. 11 Metaphor 15g).\nFinally, there are two cases where the bombs themselves occupy the subject position (cf. 15h).\n*\"] ⟩ -have a similar distribution, again, there is only one hit with a human controller in subject position.\nAll hits (whether with pronouns, common nouns or proper names), interestingly, have metonymic subjects -i.e., not a single example has the bomber pilot in the subject position.\nThis is unexpected, since literal uses should be more frequent than figurative uses (it leads More systematic study of such metonymies by target domain could uncover more such facts as well as contributing to a general picture of how important particular metonymies are in a particular language.\nThis case study sketches a potential target-oriented approach to the corpusbased study of metonymy, along with some general questions that we might investigate using it (most obviously, the question of how central a given metonymy is in the language under investigation).\nAgain, metonymy is a vastly under-researched area in corpus linguistics, so much work remains to be done.\nEpilogue\nIn this book, I have focused on corpus linguistics as a methodology, more precisely, as an application of a general observational scientific procedure to large samples of linguistic usage.\nI have refrained from placing this method in a particular theoretical framework for two reasons.\nThe first reason is that I am not convinced that linguistics should be focusing quite as much on theoretical frameworks, but rather on linguistic description based on data.\nEdward Sapir famously said that \"unfortunately, or luckily, no language is tyrannically consistent.\nAll grammars leak\"\nThe second reason is that I believe that corpus linguistics has a place in any theoretical linguistic framework, as long as that framework has some commitment to modeling linguistic reality.\nObviously, the precise place, or rather, the distance from the data analyzed using this method and the consequences of this analysis for the model depend on the kind of linguistic reality that is being modeled.\nIf it is language use, as it usually is in historically or sociolinguistically oriented studies, the distance is relatively short, requiring the researcher to discover the systematicity behind the usage patterns observed in the data.\nIf it is the mental representation of language, the length of the distance depends on your assumptions about those representations.\nTraditionally, those representations have been argued to be something fundamentally different from linguistic usage.\nIt has been claimed that they are an ephemeral \"competence\" based on a \"universal\" grammar.\nThere is disagreement as to the nature of this universal grammar -some claim that is a \"mental organ\"\nHowever, more recent models do not draw as strict a line between usage and mental representations.\nThe Usage-Based Model\nIn these models, the corpus becomes more than just a research tool, it becomes an integral part of a model of linguistic competence (cf. Stefanowitsch 2011).\nThis view is most radically expressed in the notion of \"lexical priming\" developed in\nThe notion of priming as here outlined assumes that the mind has a mental concordance of every word it has encountered, a concordance that has been richly glossed for social, physical, discoursal, generic and interpersonal context.\nThis mental concordance is accessible and can be processed in much the same way that a computer concordance is, so that all kinds of patterns, including collocational patterns, are available for use.\nIt simultaneously serves as a part, at least, of our knowledge base.\nIn less radical usage-based models of language, such as Langacker's, the corpus is not a model of linguistic competence -the latter is seen as a consequence of linguistic input perceived and organized by human minds with a particular structure (such as the capacity for figure-ground categorization).\nThe corpus is, however, a reasonable model (or at least an operationalization) of this linguistic input.\nMany of the properties of language that guide the storage of units and the abstraction of schemas over these stored units can be derived from corpora -frequencies, associations between units of linguistic structure, distributions of these units across grammatical and textual contexts, the internal variability of these units, etc. (cf.\nThis view is explicitly taken in language acquisition research conducted within the Usage-Based Model (e.g. Finally, in usage-based models as well as in models of language in general, corpora can be treated as models (or operationalizations) of the typical linguistic output of the members of a speech community, i.e. the language produced based on their internalized linguistic knowledge.\nThis is the least controversial view, and the one that I have essentially adopted throughout this book.\nEven under this view, corpus data remain one of the best sources of linguistic data we haveone that can only keep growing, providing us with ever deeper insights into the leaky, intricate, ever-changing signature activity of our species.\nI hope this book has inspired you and I hope it will help you produce research that inspires all of us.\nFurther reading\nAlthough it may seem somewhat dated, one of the best discussions of what exactly \"language\" is or can be is Study notes to Further reading\nA readable exposition of Popper's ideas about falsification is his essay \"Science as falsification\", included in the collection Conjectures and Refutations A discussion of the role of operationalization in the context of corpus-based semantics is found in Study notes to Chapter 4 Resources 13 Study notes\nFurther reading No matter what corpora and concordancing software you work with, you will need regular expressions at some point.\nInformation is easy to find online, I recommend the Wikipedia Page as a starting point (Wikipedia contributors 2018).\nAn excellent introduction to issues involved in annotating corpora is found in Geoffrey Leech's contribution \"Adding linguistic annotation\" in Study notes to Chapter 5 (see Study notes to Chapter 6) Study notes to Chapter 6\nResources 1.\nA comprehensive and well-maintained statistical software package is R, available for download free of charge from 2.\nEspecially if you are using Linux or Windows, I also recommend you download R Studio (also free of charge), which provides an advanced user interface to R,\nFurther reading\nAnyone serious about using statistics in their research should start with a basic introduction to statistics, and then proceed to an introduction of more advanced methods, preferably one that introduces a statistical software package at the same time.\nFor the first step, I recommend 2.\nThe 𝑛-gram data from the Google Books archive is available for download free of charge at\nFurther reading\nThis chapter has focused on very simple aspects of variation across text types and a very simple notion of \"text type\".\nAs seen in some of the case studies in this chapter, text is frequently a proxy for demographic properties of the speakers who have produced it, making corpus linguistics a variant of sociolinguistics, see further Study notes to Chapter 11 Further reading Critical values for the Mann-Whitney-Text There are three tables, one for 𝑝 < 0.05, one for 𝑝 < 0.01 and one for 𝑝 < 0.001 (all for two-tailed tests).\n• Starting with the first table, perform the following steps.\n-Find the smaller one of your sample sizes in the rows labelled m and the larger of your sample sizes in the columns labelled n.\n-Find the cell at the intersection of the row and the column.\n-If your U value is smaller than or equal to the value in this cell, your result is significant at the level given above the table.\n• Repeat the three steps with the second table.\nIf your U value is larger than the value in the appropriate cell, stop and report a significance level of 0.05.\nIf it is smaller or equal to the value in the appropriate cell, go to 3.\n• Repeat the three steps with the third\n",
        "entities": [
            [
                142,
                157,
                "TERMINO"
            ],
            [
                227,
                236,
                "TERMINO"
            ],
            [
                771,
                786,
                "TERMINO"
            ],
            [
                951,
                966,
                "TERMINO"
            ],
            [
                988,
                1001,
                "TERMINO"
            ],
            [
                1548,
                1563,
                "TERMINO"
            ],
            [
                1678,
                1695,
                "TERMINO"
            ],
            [
                1814,
                1829,
                "TERMINO"
            ],
            [
                2191,
                2206,
                "TERMINO"
            ],
            [
                2335,
                2350,
                "TERMINO"
            ],
            [
                2958,
                2973,
                "TERMINO"
            ],
            [
                3620,
                3628,
                "TERMINO"
            ],
            [
                3883,
                3891,
                "TERMINO"
            ],
            [
                3993,
                4008,
                "TERMINO"
            ],
            [
                5702,
                5717,
                "TERMINO"
            ],
            [
                6428,
                6443,
                "TERMINO"
            ],
            [
                8310,
                8325,
                "TERMINO"
            ],
            [
                8568,
                8583,
                "TERMINO"
            ],
            [
                10872,
                10892,
                "TERMINO"
            ],
            [
                14118,
                14134,
                "TERMINO"
            ],
            [
                15223,
                15238,
                "TERMINO"
            ],
            [
                15684,
                15699,
                "TERMINO"
            ],
            [
                16343,
                16358,
                "TERMINO"
            ],
            [
                21036,
                21048,
                "TERMINO"
            ],
            [
                22676,
                22684,
                "TERMINO"
            ],
            [
                25778,
                25794,
                "TERMINO"
            ],
            [
                26024,
                26039,
                "TERMINO"
            ],
            [
                30024,
                30036,
                "TERMINO"
            ],
            [
                31570,
                31585,
                "TERMINO"
            ],
            [
                31945,
                31956,
                "TERMINO"
            ],
            [
                32711,
                32731,
                "TERMINO"
            ],
            [
                33064,
                33084,
                "TERMINO"
            ],
            [
                34786,
                34801,
                "TERMINO"
            ],
            [
                34803,
                34823,
                "TERMINO"
            ],
            [
                34825,
                34843,
                "TERMINO"
            ],
            [
                35694,
                35704,
                "TERMINO"
            ],
            [
                36182,
                36197,
                "TERMINO"
            ],
            [
                36373,
                36388,
                "TERMINO"
            ],
            [
                38349,
                38364,
                "TERMINO"
            ],
            [
                38514,
                38529,
                "TERMINO"
            ],
            [
                38782,
                38797,
                "TERMINO"
            ],
            [
                38852,
                38867,
                "TERMINO"
            ],
            [
                38896,
                38911,
                "TERMINO"
            ],
            [
                38979,
                38996,
                "TERMINO"
            ],
            [
                39030,
                39042,
                "TERMINO"
            ],
            [
                39125,
                39140,
                "TERMINO"
            ],
            [
                39318,
                39333,
                "TERMINO"
            ],
            [
                39759,
                39777,
                "TERMINO"
            ],
            [
                40256,
                40271,
                "TERMINO"
            ],
            [
                40346,
                40358,
                "TERMINO"
            ],
            [
                40409,
                40421,
                "TERMINO"
            ],
            [
                40529,
                40545,
                "TERMINO"
            ],
            [
                41793,
                41808,
                "TERMINO"
            ],
            [
                42370,
                42385,
                "TERMINO"
            ],
            [
                43597,
                43612,
                "TERMINO"
            ],
            [
                45016,
                45028,
                "TERMINO"
            ],
            [
                45689,
                45701,
                "TERMINO"
            ],
            [
                47063,
                47078,
                "TERMINO"
            ],
            [
                47280,
                47294,
                "TERMINO"
            ],
            [
                48125,
                48140,
                "TERMINO"
            ],
            [
                48757,
                48772,
                "TERMINO"
            ],
            [
                48855,
                48870,
                "TERMINO"
            ],
            [
                48894,
                48909,
                "TERMINO"
            ],
            [
                49136,
                49154,
                "TERMINO"
            ],
            [
                49628,
                49644,
                "TERMINO"
            ],
            [
                49883,
                49899,
                "TERMINO"
            ],
            [
                50139,
                50155,
                "TERMINO"
            ],
            [
                50726,
                50742,
                "TERMINO"
            ],
            [
                51369,
                51381,
                "TERMINO"
            ],
            [
                51602,
                51611,
                "TERMINO"
            ],
            [
                51919,
                51935,
                "TERMINO"
            ],
            [
                52039,
                52055,
                "TERMINO"
            ],
            [
                52662,
                52678,
                "TERMINO"
            ],
            [
                54381,
                54390,
                "TERMINO"
            ],
            [
                55023,
                55037,
                "TERMINO"
            ],
            [
                56060,
                56073,
                "TERMINO"
            ],
            [
                58383,
                58396,
                "TERMINO"
            ],
            [
                59452,
                59467,
                "TERMINO"
            ],
            [
                60217,
                60230,
                "TERMINO"
            ],
            [
                60388,
                60400,
                "TERMINO"
            ],
            [
                60550,
                60562,
                "TERMINO"
            ],
            [
                60593,
                60605,
                "TERMINO"
            ],
            [
                61744,
                61759,
                "TERMINO"
            ],
            [
                61969,
                61985,
                "TERMINO"
            ],
            [
                62233,
                62245,
                "TERMINO"
            ],
            [
                62411,
                62424,
                "TERMINO"
            ],
            [
                62523,
                62539,
                "TERMINO"
            ],
            [
                62815,
                62830,
                "TERMINO"
            ],
            [
                62856,
                62867,
                "TERMINO"
            ],
            [
                63239,
                63250,
                "TERMINO"
            ],
            [
                63339,
                63355,
                "TERMINO"
            ],
            [
                63832,
                63847,
                "TERMINO"
            ],
            [
                64462,
                64473,
                "TERMINO"
            ],
            [
                65493,
                65504,
                "TERMINO"
            ],
            [
                65734,
                65749,
                "TERMINO"
            ],
            [
                67932,
                67942,
                "TERMINO"
            ],
            [
                69097,
                69107,
                "TERMINO"
            ],
            [
                69732,
                69742,
                "TERMINO"
            ],
            [
                69835,
                69845,
                "TERMINO"
            ],
            [
                71695,
                71702,
                "TERMINO"
            ],
            [
                71787,
                71801,
                "TERMINO"
            ],
            [
                72504,
                72515,
                "TERMINO"
            ],
            [
                72561,
                72577,
                "TERMINO"
            ],
            [
                73004,
                73013,
                "TERMINO"
            ],
            [
                73374,
                73389,
                "TERMINO"
            ],
            [
                73496,
                73511,
                "TERMINO"
            ],
            [
                73593,
                73608,
                "TERMINO"
            ],
            [
                73759,
                73774,
                "TERMINO"
            ],
            [
                74653,
                74661,
                "TERMINO"
            ],
            [
                75010,
                75025,
                "TERMINO"
            ],
            [
                75688,
                75703,
                "TERMINO"
            ],
            [
                76452,
                76467,
                "TERMINO"
            ],
            [
                77876,
                77887,
                "TERMINO"
            ],
            [
                79232,
                79247,
                "TERMINO"
            ],
            [
                81098,
                81113,
                "TERMINO"
            ],
            [
                82153,
                82168,
                "TERMINO"
            ],
            [
                82231,
                82247,
                "TERMINO"
            ],
            [
                82253,
                82268,
                "TERMINO"
            ],
            [
                82392,
                82401,
                "TERMINO"
            ],
            [
                82587,
                82603,
                "TERMINO"
            ],
            [
                82794,
                82808,
                "TERMINO"
            ],
            [
                82918,
                82927,
                "TERMINO"
            ],
            [
                83052,
                83067,
                "TERMINO"
            ],
            [
                83167,
                83181,
                "TERMINO"
            ],
            [
                83202,
                83218,
                "TERMINO"
            ],
            [
                83267,
                83290,
                "TERMINO"
            ],
            [
                83643,
                83658,
                "TERMINO"
            ],
            [
                83813,
                83822,
                "TERMINO"
            ],
            [
                84083,
                84097,
                "TERMINO"
            ],
            [
                84152,
                84161,
                "TERMINO"
            ],
            [
                84202,
                84218,
                "TERMINO"
            ],
            [
                84475,
                84490,
                "TERMINO"
            ],
            [
                84893,
                84908,
                "TERMINO"
            ],
            [
                85054,
                85068,
                "TERMINO"
            ],
            [
                85602,
                85616,
                "TERMINO"
            ],
            [
                85746,
                85761,
                "TERMINO"
            ],
            [
                85805,
                85820,
                "TERMINO"
            ],
            [
                86027,
                86042,
                "TERMINO"
            ],
            [
                86473,
                86487,
                "TERMINO"
            ],
            [
                86784,
                86799,
                "TERMINO"
            ],
            [
                87002,
                87017,
                "TERMINO"
            ],
            [
                87170,
                87185,
                "TERMINO"
            ],
            [
                87311,
                87338,
                "TERMINO"
            ],
            [
                87432,
                87459,
                "TERMINO"
            ],
            [
                87464,
                87479,
                "TERMINO"
            ],
            [
                87568,
                87583,
                "TERMINO"
            ],
            [
                87889,
                87904,
                "TERMINO"
            ],
            [
                88324,
                88339,
                "TERMINO"
            ],
            [
                88471,
                88486,
                "TERMINO"
            ],
            [
                90639,
                90654,
                "TERMINO"
            ],
            [
                90686,
                90701,
                "TERMINO"
            ],
            [
                90967,
                90982,
                "TERMINO"
            ],
            [
                91030,
                91045,
                "TERMINO"
            ],
            [
                91953,
                91968,
                "TERMINO"
            ],
            [
                92062,
                92074,
                "TERMINO"
            ],
            [
                93349,
                93364,
                "TERMINO"
            ],
            [
                94451,
                94469,
                "TERMINO"
            ],
            [
                95220,
                95235,
                "TERMINO"
            ],
            [
                95550,
                95565,
                "TERMINO"
            ],
            [
                95613,
                95630,
                "TERMINO"
            ],
            [
                97010,
                97018,
                "TERMINO"
            ],
            [
                97244,
                97258,
                "TERMINO"
            ],
            [
                97597,
                97612,
                "TERMINO"
            ],
            [
                98020,
                98032,
                "TERMINO"
            ],
            [
                98101,
                98116,
                "TERMINO"
            ],
            [
                98190,
                98205,
                "TERMINO"
            ],
            [
                100391,
                100406,
                "TERMINO"
            ],
            [
                100436,
                100451,
                "TERMINO"
            ],
            [
                109111,
                109126,
                "TERMINO"
            ],
            [
                109729,
                109744,
                "TERMINO"
            ],
            [
                110231,
                110242,
                "TERMINO"
            ],
            [
                110292,
                110306,
                "TERMINO"
            ],
            [
                111167,
                111182,
                "TERMINO"
            ],
            [
                118806,
                118816,
                "TERMINO"
            ],
            [
                121807,
                121822,
                "TERMINO"
            ],
            [
                123204,
                123219,
                "TERMINO"
            ],
            [
                125551,
                125563,
                "TERMINO"
            ],
            [
                134581,
                134596,
                "TERMINO"
            ],
            [
                134600,
                134615,
                "TERMINO"
            ],
            [
                135229,
                135243,
                "TERMINO"
            ],
            [
                135292,
                135302,
                "TERMINO"
            ],
            [
                135345,
                135358,
                "TERMINO"
            ],
            [
                138790,
                138805,
                "TERMINO"
            ],
            [
                139680,
                139694,
                "TERMINO"
            ],
            [
                139784,
                139794,
                "TERMINO"
            ],
            [
                140365,
                140380,
                "TERMINO"
            ],
            [
                140517,
                140532,
                "TERMINO"
            ],
            [
                140907,
                140914,
                "TERMINO"
            ],
            [
                141347,
                141357,
                "TERMINO"
            ],
            [
                141381,
                141391,
                "TERMINO"
            ],
            [
                141698,
                141711,
                "TERMINO"
            ],
            [
                142007,
                142020,
                "TERMINO"
            ],
            [
                142146,
                142156,
                "TERMINO"
            ],
            [
                142877,
                142884,
                "TERMINO"
            ],
            [
                142993,
                143007,
                "TERMINO"
            ],
            [
                143292,
                143302,
                "TERMINO"
            ],
            [
                143448,
                143463,
                "TERMINO"
            ],
            [
                143539,
                143546,
                "TERMINO"
            ],
            [
                145041,
                145048,
                "TERMINO"
            ],
            [
                145170,
                145177,
                "TERMINO"
            ],
            [
                145568,
                145576,
                "TERMINO"
            ],
            [
                146634,
                146651,
                "TERMINO"
            ],
            [
                146861,
                146868,
                "TERMINO"
            ],
            [
                148744,
                148753,
                "TERMINO"
            ],
            [
                149049,
                149058,
                "TERMINO"
            ],
            [
                150536,
                150547,
                "TERMINO"
            ],
            [
                151290,
                151299,
                "TERMINO"
            ],
            [
                151892,
                151903,
                "TERMINO"
            ],
            [
                152861,
                152872,
                "TERMINO"
            ],
            [
                153585,
                153601,
                "TERMINO"
            ],
            [
                154347,
                154362,
                "TERMINO"
            ],
            [
                154601,
                154611,
                "TERMINO"
            ],
            [
                154736,
                154750,
                "TERMINO"
            ],
            [
                156027,
                156038,
                "TERMINO"
            ],
            [
                162598,
                162613,
                "TERMINO"
            ],
            [
                162645,
                162656,
                "TERMINO"
            ],
            [
                163081,
                163092,
                "TERMINO"
            ],
            [
                163239,
                163254,
                "TERMINO"
            ],
            [
                163375,
                163385,
                "TERMINO"
            ],
            [
                163399,
                163409,
                "TERMINO"
            ],
            [
                164235,
                164250,
                "TERMINO"
            ],
            [
                167676,
                167686,
                "TERMINO"
            ],
            [
                167764,
                167776,
                "TERMINO"
            ],
            [
                172511,
                172526,
                "TERMINO"
            ],
            [
                172739,
                172754,
                "TERMINO"
            ],
            [
                173022,
                173032,
                "TERMINO"
            ],
            [
                176355,
                176370,
                "TERMINO"
            ],
            [
                178245,
                178260,
                "TERMINO"
            ],
            [
                178938,
                178947,
                "TERMINO"
            ],
            [
                179068,
                179084,
                "TERMINO"
            ],
            [
                179116,
                179125,
                "TERMINO"
            ],
            [
                179381,
                179393,
                "TERMINO"
            ],
            [
                179520,
                179530,
                "TERMINO"
            ],
            [
                179641,
                179651,
                "TERMINO"
            ],
            [
                180025,
                180034,
                "TERMINO"
            ],
            [
                180480,
                180490,
                "TERMINO"
            ],
            [
                180534,
                180544,
                "TERMINO"
            ],
            [
                181133,
                181143,
                "TERMINO"
            ],
            [
                182418,
                182433,
                "TERMINO"
            ],
            [
                182637,
                182652,
                "TERMINO"
            ],
            [
                182690,
                182706,
                "TERMINO"
            ],
            [
                185114,
                185121,
                "TERMINO"
            ],
            [
                186098,
                186107,
                "TERMINO"
            ],
            [
                186969,
                186976,
                "TERMINO"
            ],
            [
                187290,
                187298,
                "TERMINO"
            ],
            [
                187395,
                187403,
                "TERMINO"
            ],
            [
                188002,
                188016,
                "TERMINO"
            ],
            [
                188075,
                188084,
                "TERMINO"
            ],
            [
                188217,
                188229,
                "TERMINO"
            ],
            [
                188280,
                188292,
                "TERMINO"
            ],
            [
                189575,
                189584,
                "TERMINO"
            ],
            [
                189796,
                189805,
                "TERMINO"
            ],
            [
                189825,
                189833,
                "TERMINO"
            ],
            [
                190692,
                190709,
                "TERMINO"
            ],
            [
                191254,
                191261,
                "TERMINO"
            ],
            [
                191830,
                191845,
                "TERMINO"
            ],
            [
                192238,
                192250,
                "TERMINO"
            ],
            [
                193019,
                193027,
                "TERMINO"
            ],
            [
                194106,
                194121,
                "TERMINO"
            ],
            [
                194382,
                194403,
                "TERMINO"
            ],
            [
                197804,
                197812,
                "TERMINO"
            ],
            [
                204570,
                204584,
                "TERMINO"
            ],
            [
                204701,
                204717,
                "TERMINO"
            ],
            [
                205237,
                205249,
                "TERMINO"
            ],
            [
                206013,
                206029,
                "TERMINO"
            ],
            [
                207424,
                207434,
                "TERMINO"
            ],
            [
                207838,
                207855,
                "TERMINO"
            ],
            [
                207938,
                207955,
                "TERMINO"
            ],
            [
                208077,
                208094,
                "TERMINO"
            ],
            [
                209116,
                209126,
                "TERMINO"
            ],
            [
                211843,
                211860,
                "TERMINO"
            ],
            [
                212194,
                212211,
                "TERMINO"
            ],
            [
                212844,
                212861,
                "TERMINO"
            ],
            [
                212863,
                212880,
                "TERMINO"
            ],
            [
                212911,
                212928,
                "TERMINO"
            ],
            [
                213132,
                213149,
                "TERMINO"
            ],
            [
                213227,
                213244,
                "TERMINO"
            ],
            [
                213528,
                213545,
                "TERMINO"
            ],
            [
                214009,
                214024,
                "TERMINO"
            ],
            [
                214154,
                214171,
                "TERMINO"
            ],
            [
                214485,
                214502,
                "TERMINO"
            ],
            [
                214612,
                214629,
                "TERMINO"
            ],
            [
                214828,
                214845,
                "TERMINO"
            ],
            [
                215009,
                215026,
                "TERMINO"
            ],
            [
                215059,
                215076,
                "TERMINO"
            ],
            [
                216821,
                216838,
                "TERMINO"
            ],
            [
                216871,
                216885,
                "TERMINO"
            ],
            [
                216941,
                216955,
                "TERMINO"
            ],
            [
                218197,
                218214,
                "TERMINO"
            ],
            [
                219368,
                219385,
                "TERMINO"
            ],
            [
                219531,
                219548,
                "TERMINO"
            ],
            [
                219932,
                219949,
                "TERMINO"
            ],
            [
                219956,
                219973,
                "TERMINO"
            ],
            [
                220639,
                220656,
                "TERMINO"
            ],
            [
                220782,
                220799,
                "TERMINO"
            ],
            [
                221256,
                221273,
                "TERMINO"
            ],
            [
                221583,
                221600,
                "TERMINO"
            ],
            [
                221737,
                221754,
                "TERMINO"
            ],
            [
                221892,
                221903,
                "TERMINO"
            ],
            [
                222387,
                222395,
                "TERMINO"
            ],
            [
                222589,
                222599,
                "TERMINO"
            ],
            [
                223338,
                223355,
                "TERMINO"
            ],
            [
                223601,
                223609,
                "TERMINO"
            ],
            [
                223677,
                223694,
                "TERMINO"
            ],
            [
                223899,
                223916,
                "TERMINO"
            ],
            [
                223960,
                223977,
                "TERMINO"
            ],
            [
                224172,
                224189,
                "TERMINO"
            ],
            [
                224413,
                224421,
                "TERMINO"
            ],
            [
                224885,
                224902,
                "TERMINO"
            ],
            [
                225063,
                225080,
                "TERMINO"
            ],
            [
                225117,
                225125,
                "TERMINO"
            ],
            [
                226818,
                226835,
                "TERMINO"
            ],
            [
                227536,
                227553,
                "TERMINO"
            ],
            [
                227912,
                227929,
                "TERMINO"
            ],
            [
                230851,
                230866,
                "TERMINO"
            ],
            [
                231179,
                231196,
                "TERMINO"
            ],
            [
                232558,
                232573,
                "TERMINO"
            ],
            [
                233689,
                233704,
                "TERMINO"
            ],
            [
                233717,
                233732,
                "TERMINO"
            ],
            [
                234783,
                234798,
                "TERMINO"
            ],
            [
                235260,
                235277,
                "TERMINO"
            ],
            [
                235361,
                235376,
                "TERMINO"
            ],
            [
                235525,
                235540,
                "TERMINO"
            ],
            [
                235874,
                235889,
                "TERMINO"
            ],
            [
                236485,
                236499,
                "TERMINO"
            ],
            [
                238601,
                238612,
                "TERMINO"
            ],
            [
                238805,
                238821,
                "TERMINO"
            ],
            [
                238951,
                238960,
                "TERMINO"
            ],
            [
                239371,
                239387,
                "TERMINO"
            ],
            [
                239597,
                239612,
                "TERMINO"
            ],
            [
                241027,
                241039,
                "TERMINO"
            ],
            [
                241621,
                241636,
                "TERMINO"
            ],
            [
                247191,
                247207,
                "TERMINO"
            ],
            [
                247652,
                247668,
                "TERMINO"
            ],
            [
                253265,
                253280,
                "TERMINO"
            ],
            [
                253460,
                253475,
                "TERMINO"
            ],
            [
                256120,
                256135,
                "TERMINO"
            ],
            [
                256366,
                256381,
                "TERMINO"
            ],
            [
                257822,
                257837,
                "TERMINO"
            ],
            [
                258499,
                258514,
                "TERMINO"
            ],
            [
                259790,
                259807,
                "TERMINO"
            ],
            [
                259856,
                259873,
                "TERMINO"
            ],
            [
                260838,
                260845,
                "TERMINO"
            ],
            [
                261385,
                261393,
                "TERMINO"
            ],
            [
                261791,
                261806,
                "TERMINO"
            ],
            [
                262467,
                262481,
                "TERMINO"
            ],
            [
                262694,
                262708,
                "TERMINO"
            ],
            [
                263963,
                263977,
                "TERMINO"
            ],
            [
                271565,
                271573,
                "TERMINO"
            ],
            [
                271822,
                271839,
                "TERMINO"
            ],
            [
                273096,
                273104,
                "TERMINO"
            ],
            [
                273323,
                273331,
                "TERMINO"
            ],
            [
                273375,
                273383,
                "TERMINO"
            ],
            [
                274645,
                274659,
                "TERMINO"
            ],
            [
                276878,
                276886,
                "TERMINO"
            ],
            [
                277616,
                277627,
                "TERMINO"
            ],
            [
                280011,
                280025,
                "TERMINO"
            ],
            [
                281788,
                281803,
                "TERMINO"
            ],
            [
                296311,
                296322,
                "TERMINO"
            ],
            [
                296493,
                296501,
                "TERMINO"
            ],
            [
                296590,
                296598,
                "TERMINO"
            ],
            [
                296802,
                296810,
                "TERMINO"
            ],
            [
                297060,
                297068,
                "TERMINO"
            ],
            [
                297555,
                297570,
                "TERMINO"
            ],
            [
                299219,
                299234,
                "TERMINO"
            ],
            [
                299433,
                299448,
                "TERMINO"
            ],
            [
                300079,
                300093,
                "TERMINO"
            ],
            [
                300220,
                300234,
                "TERMINO"
            ],
            [
                300523,
                300537,
                "TERMINO"
            ],
            [
                300736,
                300750,
                "TERMINO"
            ],
            [
                301081,
                301096,
                "TERMINO"
            ],
            [
                301192,
                301207,
                "TERMINO"
            ],
            [
                302076,
                302092,
                "TERMINO"
            ],
            [
                302653,
                302668,
                "TERMINO"
            ],
            [
                302946,
                302969,
                "TERMINO"
            ],
            [
                303794,
                303809,
                "TERMINO"
            ],
            [
                304140,
                304155,
                "TERMINO"
            ],
            [
                307634,
                307651,
                "TERMINO"
            ],
            [
                308155,
                308172,
                "TERMINO"
            ],
            [
                308430,
                308447,
                "TERMINO"
            ],
            [
                309505,
                309520,
                "TERMINO"
            ],
            [
                310519,
                310542,
                "TERMINO"
            ],
            [
                310704,
                310719,
                "TERMINO"
            ],
            [
                310784,
                310795,
                "TERMINO"
            ],
            [
                311062,
                311077,
                "TERMINO"
            ],
            [
                312748,
                312765,
                "TERMINO"
            ],
            [
                315440,
                315454,
                "TERMINO"
            ],
            [
                316448,
                316462,
                "TERMINO"
            ],
            [
                316539,
                316553,
                "TERMINO"
            ],
            [
                317231,
                317248,
                "TERMINO"
            ],
            [
                318566,
                318574,
                "TERMINO"
            ],
            [
                320403,
                320417,
                "TERMINO"
            ],
            [
                322382,
                322396,
                "TERMINO"
            ],
            [
                322526,
                322540,
                "TERMINO"
            ],
            [
                324226,
                324237,
                "TERMINO"
            ],
            [
                324271,
                324282,
                "TERMINO"
            ],
            [
                324659,
                324682,
                "TERMINO"
            ],
            [
                324753,
                324776,
                "TERMINO"
            ],
            [
                325343,
                325358,
                "TERMINO"
            ],
            [
                328238,
                328246,
                "TERMINO"
            ],
            [
                328327,
                328342,
                "TERMINO"
            ],
            [
                329352,
                329367,
                "TERMINO"
            ],
            [
                329469,
                329484,
                "TERMINO"
            ],
            [
                330203,
                330220,
                "TERMINO"
            ],
            [
                330394,
                330409,
                "TERMINO"
            ],
            [
                330663,
                330678,
                "TERMINO"
            ],
            [
                331425,
                331436,
                "TERMINO"
            ],
            [
                331487,
                331498,
                "TERMINO"
            ],
            [
                331824,
                331847,
                "TERMINO"
            ],
            [
                332151,
                332167,
                "TERMINO"
            ],
            [
                335433,
                335448,
                "TERMINO"
            ],
            [
                336893,
                336909,
                "TERMINO"
            ],
            [
                337017,
                337032,
                "TERMINO"
            ],
            [
                337278,
                337293,
                "TERMINO"
            ],
            [
                337433,
                337448,
                "TERMINO"
            ],
            [
                337457,
                337474,
                "TERMINO"
            ],
            [
                338300,
                338311,
                "TERMINO"
            ],
            [
                338495,
                338510,
                "TERMINO"
            ],
            [
                338558,
                338569,
                "TERMINO"
            ],
            [
                341847,
                341858,
                "TERMINO"
            ],
            [
                342576,
                342591,
                "TERMINO"
            ],
            [
                343559,
                343570,
                "TERMINO"
            ],
            [
                344241,
                344256,
                "TERMINO"
            ],
            [
                344728,
                344743,
                "TERMINO"
            ],
            [
                345193,
                345208,
                "TERMINO"
            ],
            [
                345645,
                345660,
                "TERMINO"
            ],
            [
                345929,
                345944,
                "TERMINO"
            ],
            [
                346338,
                346355,
                "TERMINO"
            ],
            [
                346683,
                346699,
                "TERMINO"
            ],
            [
                347536,
                347553,
                "TERMINO"
            ],
            [
                349234,
                349249,
                "TERMINO"
            ],
            [
                350201,
                350216,
                "TERMINO"
            ],
            [
                350849,
                350864,
                "TERMINO"
            ],
            [
                351136,
                351151,
                "TERMINO"
            ],
            [
                351376,
                351385,
                "TERMINO"
            ],
            [
                351598,
                351613,
                "TERMINO"
            ],
            [
                351911,
                351926,
                "TERMINO"
            ],
            [
                352251,
                352266,
                "TERMINO"
            ],
            [
                352536,
                352554,
                "TERMINO"
            ],
            [
                352564,
                352573,
                "TERMINO"
            ],
            [
                352827,
                352845,
                "TERMINO"
            ],
            [
                353416,
                353434,
                "TERMINO"
            ],
            [
                354020,
                354038,
                "TERMINO"
            ],
            [
                354209,
                354221,
                "TERMINO"
            ],
            [
                354247,
                354263,
                "TERMINO"
            ],
            [
                355142,
                355157,
                "TERMINO"
            ],
            [
                355829,
                355844,
                "TERMINO"
            ],
            [
                355847,
                355862,
                "TERMINO"
            ],
            [
                358368,
                358383,
                "TERMINO"
            ],
            [
                358455,
                358470,
                "TERMINO"
            ],
            [
                360569,
                360584,
                "TERMINO"
            ],
            [
                363854,
                363869,
                "TERMINO"
            ],
            [
                364735,
                364743,
                "TERMINO"
            ],
            [
                367365,
                367373,
                "TERMINO"
            ],
            [
                367496,
                367507,
                "TERMINO"
            ],
            [
                367810,
                367821,
                "TERMINO"
            ],
            [
                367890,
                367901,
                "TERMINO"
            ],
            [
                367942,
                367961,
                "TERMINO"
            ],
            [
                368431,
                368441,
                "TERMINO"
            ],
            [
                369083,
                369102,
                "TERMINO"
            ],
            [
                369315,
                369329,
                "TERMINO"
            ],
            [
                369340,
                369351,
                "TERMINO"
            ],
            [
                369737,
                369756,
                "TERMINO"
            ],
            [
                370182,
                370193,
                "TERMINO"
            ],
            [
                370366,
                370385,
                "TERMINO"
            ],
            [
                371939,
                371958,
                "TERMINO"
            ],
            [
                372931,
                372950,
                "TERMINO"
            ],
            [
                374494,
                374504,
                "TERMINO"
            ],
            [
                374784,
                374800,
                "TERMINO"
            ],
            [
                374957,
                374976,
                "TERMINO"
            ],
            [
                374997,
                375016,
                "TERMINO"
            ],
            [
                375034,
                375042,
                "TERMINO"
            ],
            [
                377044,
                377063,
                "TERMINO"
            ],
            [
                378851,
                378870,
                "TERMINO"
            ],
            [
                379042,
                379061,
                "TERMINO"
            ],
            [
                379106,
                379113,
                "TERMINO"
            ],
            [
                379236,
                379255,
                "TERMINO"
            ],
            [
                380087,
                380106,
                "TERMINO"
            ],
            [
                383203,
                383222,
                "TERMINO"
            ],
            [
                385316,
                385324,
                "TERMINO"
            ],
            [
                385589,
                385597,
                "TERMINO"
            ],
            [
                385988,
                385996,
                "TERMINO"
            ],
            [
                386061,
                386069,
                "TERMINO"
            ],
            [
                387354,
                387369,
                "TERMINO"
            ],
            [
                387396,
                387411,
                "TERMINO"
            ],
            [
                392584,
                392599,
                "TERMINO"
            ],
            [
                393032,
                393046,
                "TERMINO"
            ],
            [
                397130,
                397145,
                "TERMINO"
            ],
            [
                400848,
                400863,
                "TERMINO"
            ],
            [
                402151,
                402174,
                "TERMINO"
            ],
            [
                402238,
                402252,
                "TERMINO"
            ],
            [
                405055,
                405064,
                "TERMINO"
            ],
            [
                409146,
                409161,
                "TERMINO"
            ],
            [
                409500,
                409515,
                "TERMINO"
            ],
            [
                410372,
                410387,
                "TERMINO"
            ],
            [
                410887,
                410902,
                "TERMINO"
            ],
            [
                411324,
                411343,
                "TERMINO"
            ],
            [
                412705,
                412724,
                "TERMINO"
            ],
            [
                415029,
                415048,
                "TERMINO"
            ],
            [
                415435,
                415450,
                "TERMINO"
            ],
            [
                418052,
                418071,
                "TERMINO"
            ],
            [
                418171,
                418186,
                "TERMINO"
            ],
            [
                418359,
                418378,
                "TERMINO"
            ],
            [
                419855,
                419870,
                "TERMINO"
            ],
            [
                420584,
                420607,
                "TERMINO"
            ],
            [
                421867,
                421882,
                "TERMINO"
            ],
            [
                422763,
                422778,
                "TERMINO"
            ],
            [
                423361,
                423371,
                "TERMINO"
            ],
            [
                424380,
                424397,
                "TERMINO"
            ],
            [
                425198,
                425215,
                "TERMINO"
            ],
            [
                426437,
                426454,
                "TERMINO"
            ],
            [
                426754,
                426771,
                "TERMINO"
            ],
            [
                428928,
                428945,
                "TERMINO"
            ],
            [
                429867,
                429884,
                "TERMINO"
            ],
            [
                430877,
                430892,
                "TERMINO"
            ],
            [
                430961,
                430978,
                "TERMINO"
            ],
            [
                431010,
                431027,
                "TERMINO"
            ],
            [
                433805,
                433819,
                "TERMINO"
            ],
            [
                433864,
                433879,
                "TERMINO"
            ],
            [
                434228,
                434242,
                "TERMINO"
            ],
            [
                434710,
                434724,
                "TERMINO"
            ],
            [
                434843,
                434858,
                "TERMINO"
            ],
            [
                437046,
                437055,
                "TERMINO"
            ],
            [
                437183,
                437197,
                "TERMINO"
            ],
            [
                440708,
                440727,
                "TERMINO"
            ],
            [
                446114,
                446130,
                "TERMINO"
            ],
            [
                448204,
                448222,
                "TERMINO"
            ],
            [
                448952,
                448966,
                "TERMINO"
            ],
            [
                449659,
                449674,
                "TERMINO"
            ],
            [
                450449,
                450463,
                "TERMINO"
            ],
            [
                450861,
                450875,
                "TERMINO"
            ],
            [
                451244,
                451252,
                "TERMINO"
            ],
            [
                454731,
                454746,
                "TERMINO"
            ],
            [
                455068,
                455084,
                "TERMINO"
            ],
            [
                456439,
                456453,
                "TERMINO"
            ],
            [
                456676,
                456690,
                "TERMINO"
            ],
            [
                457076,
                457086,
                "TERMINO"
            ],
            [
                457497,
                457507,
                "TERMINO"
            ],
            [
                458427,
                458442,
                "TERMINO"
            ],
            [
                464745,
                464753,
                "TERMINO"
            ],
            [
                465469,
                465484,
                "TERMINO"
            ],
            [
                466081,
                466099,
                "TERMINO"
            ],
            [
                466362,
                466385,
                "TERMINO"
            ],
            [
                466637,
                466655,
                "TERMINO"
            ],
            [
                467740,
                467748,
                "TERMINO"
            ],
            [
                467841,
                467849,
                "TERMINO"
            ],
            [
                469320,
                469338,
                "TERMINO"
            ],
            [
                469968,
                469978,
                "TERMINO"
            ],
            [
                470561,
                470576,
                "TERMINO"
            ],
            [
                472587,
                472597,
                "TERMINO"
            ],
            [
                472907,
                472922,
                "TERMINO"
            ],
            [
                475808,
                475818,
                "TERMINO"
            ],
            [
                476500,
                476515,
                "TERMINO"
            ],
            [
                478720,
                478730,
                "TERMINO"
            ],
            [
                479760,
                479774,
                "TERMINO"
            ],
            [
                483600,
                483611,
                "TERMINO"
            ],
            [
                484198,
                484209,
                "TERMINO"
            ],
            [
                486135,
                486150,
                "TERMINO"
            ],
            [
                486890,
                486901,
                "TERMINO"
            ],
            [
                487616,
                487627,
                "TERMINO"
            ],
            [
                488113,
                488124,
                "TERMINO"
            ],
            [
                488336,
                488344,
                "TERMINO"
            ],
            [
                488851,
                488862,
                "TERMINO"
            ],
            [
                488921,
                488932,
                "TERMINO"
            ],
            [
                489095,
                489106,
                "TERMINO"
            ],
            [
                489652,
                489667,
                "TERMINO"
            ],
            [
                490394,
                490409,
                "TERMINO"
            ],
            [
                490857,
                490872,
                "TERMINO"
            ],
            [
                496853,
                496870,
                "TERMINO"
            ],
            [
                498285,
                498295,
                "TERMINO"
            ],
            [
                498598,
                498608,
                "TERMINO"
            ],
            [
                500469,
                500477,
                "TERMINO"
            ],
            [
                500966,
                500980,
                "TERMINO"
            ],
            [
                501315,
                501325,
                "TERMINO"
            ],
            [
                501997,
                502008,
                "TERMINO"
            ],
            [
                502302,
                502312,
                "TERMINO"
            ],
            [
                508883,
                508894,
                "TERMINO"
            ],
            [
                515043,
                515054,
                "TERMINO"
            ],
            [
                516538,
                516547,
                "TERMINO"
            ],
            [
                517159,
                517177,
                "TERMINO"
            ],
            [
                517229,
                517247,
                "TERMINO"
            ],
            [
                517292,
                517308,
                "TERMINO"
            ],
            [
                517732,
                517747,
                "TERMINO"
            ],
            [
                517952,
                517968,
                "TERMINO"
            ],
            [
                518145,
                518161,
                "TERMINO"
            ],
            [
                518368,
                518386,
                "TERMINO"
            ],
            [
                518455,
                518471,
                "TERMINO"
            ],
            [
                518528,
                518544,
                "TERMINO"
            ],
            [
                518789,
                518805,
                "TERMINO"
            ],
            [
                518889,
                518905,
                "TERMINO"
            ],
            [
                519021,
                519034,
                "TERMINO"
            ],
            [
                519040,
                519056,
                "TERMINO"
            ],
            [
                519106,
                519122,
                "TERMINO"
            ],
            [
                519139,
                519157,
                "TERMINO"
            ],
            [
                519244,
                519260,
                "TERMINO"
            ],
            [
                519515,
                519529,
                "TERMINO"
            ],
            [
                519630,
                519644,
                "TERMINO"
            ],
            [
                520356,
                520372,
                "TERMINO"
            ],
            [
                520543,
                520559,
                "TERMINO"
            ],
            [
                520768,
                520784,
                "TERMINO"
            ],
            [
                520915,
                520931,
                "TERMINO"
            ],
            [
                520932,
                520948,
                "TERMINO"
            ],
            [
                521654,
                521670,
                "TERMINO"
            ],
            [
                522029,
                522045,
                "TERMINO"
            ],
            [
                522057,
                522070,
                "TERMINO"
            ],
            [
                522113,
                522129,
                "TERMINO"
            ],
            [
                522588,
                522604,
                "TERMINO"
            ],
            [
                522710,
                522726,
                "TERMINO"
            ],
            [
                522864,
                522880,
                "TERMINO"
            ],
            [
                523843,
                523859,
                "TERMINO"
            ],
            [
                524842,
                524858,
                "TERMINO"
            ],
            [
                525138,
                525153,
                "TERMINO"
            ],
            [
                525205,
                525220,
                "TERMINO"
            ],
            [
                526010,
                526020,
                "TERMINO"
            ],
            [
                526536,
                526552,
                "TERMINO"
            ],
            [
                527690,
                527700,
                "TERMINO"
            ],
            [
                529665,
                529681,
                "TERMINO"
            ],
            [
                530170,
                530184,
                "TERMINO"
            ],
            [
                531441,
                531451,
                "TERMINO"
            ],
            [
                533353,
                533362,
                "TERMINO"
            ],
            [
                534005,
                534021,
                "TERMINO"
            ],
            [
                534139,
                534155,
                "TERMINO"
            ],
            [
                534492,
                534508,
                "TERMINO"
            ],
            [
                538029,
                538045,
                "TERMINO"
            ],
            [
                541348,
                541364,
                "TERMINO"
            ],
            [
                541686,
                541701,
                "TERMINO"
            ],
            [
                541910,
                541926,
                "TERMINO"
            ],
            [
                541954,
                541970,
                "TERMINO"
            ],
            [
                542213,
                542229,
                "TERMINO"
            ],
            [
                542618,
                542633,
                "TERMINO"
            ],
            [
                543109,
                543119,
                "TERMINO"
            ],
            [
                544723,
                544734,
                "TERMINO"
            ],
            [
                544901,
                544917,
                "TERMINO"
            ],
            [
                544977,
                544988,
                "TERMINO"
            ],
            [
                545546,
                545562,
                "TERMINO"
            ],
            [
                550499,
                550508,
                "TERMINO"
            ],
            [
                550874,
                550883,
                "TERMINO"
            ],
            [
                551396,
                551415,
                "TERMINO"
            ],
            [
                551427,
                551442,
                "TERMINO"
            ],
            [
                551653,
                551667,
                "TERMINO"
            ],
            [
                551978,
                551988,
                "TERMINO"
            ],
            [
                552772,
                552787,
                "TERMINO"
            ],
            [
                552922,
                552937,
                "TERMINO"
            ],
            [
                553275,
                553291,
                "TERMINO"
            ],
            [
                554875,
                554892,
                "TERMINO"
            ],
            [
                555187,
                555204,
                "TERMINO"
            ],
            [
                557327,
                557335,
                "TERMINO"
            ],
            [
                559324,
                559333,
                "TERMINO"
            ],
            [
                559528,
                559537,
                "TERMINO"
            ],
            [
                560508,
                560517,
                "TERMINO"
            ],
            [
                560660,
                560677,
                "TERMINO"
            ],
            [
                563339,
                563352,
                "TERMINO"
            ],
            [
                563466,
                563477,
                "TERMINO"
            ],
            [
                565398,
                565411,
                "TERMINO"
            ],
            [
                565567,
                565580,
                "TERMINO"
            ],
            [
                568269,
                568279,
                "TERMINO"
            ],
            [
                568823,
                568837,
                "TERMINO"
            ],
            [
                569329,
                569344,
                "TERMINO"
            ],
            [
                570078,
                570095,
                "TERMINO"
            ],
            [
                576190,
                576206,
                "TERMINO"
            ],
            [
                577080,
                577096,
                "TERMINO"
            ],
            [
                577281,
                577296,
                "TERMINO"
            ],
            [
                590401,
                590416,
                "TERMINO"
            ],
            [
                590495,
                590510,
                "TERMINO"
            ],
            [
                591082,
                591097,
                "TERMINO"
            ],
            [
                591452,
                591464,
                "TERMINO"
            ],
            [
                593762,
                593782,
                "TERMINO"
            ],
            [
                594024,
                594040,
                "TERMINO"
            ],
            [
                595559,
                595575,
                "TERMINO"
            ],
            [
                596017,
                596033,
                "TERMINO"
            ],
            [
                596261,
                596270,
                "TERMINO"
            ],
            [
                596301,
                596310,
                "TERMINO"
            ],
            [
                596465,
                596480,
                "TERMINO"
            ],
            [
                596812,
                596823,
                "TERMINO"
            ],
            [
                596871,
                596882,
                "TERMINO"
            ]
        ]
    },
    {
        "text": "Furthermore, corpus linguistics has played a crucial role in the advancement of natural language processing (NLP) technologies.\nBy training computer algorithms on extensive corpora, researchers have been able to develop language models that possess the ability to understand and generate text that resembles human language.\nThis has led to significant progress in areas such as machine translation, information retrieval, and other NLP applications.\nTo summarize, the text-corpus method is an immensely powerful tool in linguistic research that enables researchers to gain valuable insights into the rules and patterns of a language by analyzing a vast collection of texts.\nIts applications range from studying language variation and change to the development of NLP technologies.\nHowever, it is important to note that it is not directly related to the specific phrase \"run into the sand.\"\nOne notable milestone in the field of corpus linguistics was the publication of \"Computational Analysis of Present-Day American English\" in 1967 by Kučera and Francis.\nThis work was based on an analysis of the Brown Corpus, which was a carefully selected compilation of approximately one million American English words from various sources.\nKučera and Francis utilized computational analyses along with elements from linguistics, language teaching, psychology, statistics, and sociology to create a comprehensive and diverse body of work.\nAnother significant publication was Quirk's \"Towards a description of English Usage\" in 1960, which introduced the Survey of English Usage.\n",
        "entities": [
            [
                13,
                28,
                "TERMINO"
            ],
            [
                80,
                107,
                "TERMINO"
            ],
            [
                399,
                420,
                "TERMINO"
            ],
            [
                473,
                486,
                "TERMINO"
            ],
            [
                653,
                671,
                "TERMINO"
            ],
            [
                711,
                729,
                "TERMINO"
            ],
            [
                919,
                934,
                "TERMINO"
            ],
            [
                1320,
                1337,
                "TERMINO"
            ]
        ]
    }
]